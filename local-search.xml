<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>向GITHUB添加SSH-KEY</title>
    <link href="/2021/07/06/%E5%90%91GITHUB%E6%B7%BB%E5%8A%A0SSH-KEY/"/>
    <url>/2021/07/06/%E5%90%91GITHUB%E6%B7%BB%E5%8A%A0SSH-KEY/</url>
    
    <content type="html"><![CDATA[<h3 id="1-打开命令行窗口"><a href="#1-打开命令行窗口" class="headerlink" title="1.打开命令行窗口"></a>1.打开命令行窗口</h3><h3 id="2-检查是否已有SSH"><a href="#2-检查是否已有SSH" class="headerlink" title="2.检查是否已有SSH"></a>2.检查是否已有SSH</h3><p>如果没有，会返回如下信息，继续第三步创建SSH<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ cd ~/.ssh<br><span class="hljs-string">bash:</span> <span class="hljs-string">cd:</span> <span class="hljs-regexp">/c/</span>Users<span class="hljs-regexp">/Him/</span>.<span class="hljs-string">ssh:</span> No such file or directory<br></code></pre></td></tr></table></figure></p><p>如果本地已经有创建SSH，会返回如下信息，表示本地已经有创建过SSH了，跳过第三步，直接看第四步<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">$ cd ~/.ssh<br>$ ls<br>id_rsa  id_rsa.pub  known_hosts<br></code></pre></td></tr></table></figure></p><h3 id="3-创建SSH-key"><a href="#3-创建SSH-key" class="headerlink" title="3.创建SSH key"></a>3.创建SSH key</h3><h5 id="最后一个参数替换成你自己的GitHub注册邮箱"><a href="#最后一个参数替换成你自己的GitHub注册邮箱" class="headerlink" title="最后一个参数替换成你自己的GitHub注册邮箱"></a>最后一个参数替换成你自己的GitHub注册邮箱</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">$ ssh-keygen -t rsa -C &quot;your_email@example.com&quot;<br>Generating public/private rsa key pair.<br></code></pre></td></tr></table></figure><p>接下来会提示你输入生成的key存放的路径，不设置直接回车的话会默认创建在C:/Users/你的用户账号/.ssh文件夹下<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">Enter <span class="hljs-built_in">file</span> <span class="hljs-keyword">in</span> which <span class="hljs-built_in">to</span> save <span class="hljs-keyword">the</span> key (/c/Users/Him/.ssh/id_rsa):<br>Created <span class="hljs-built_in">directory</span> <span class="hljs-string">'/c/Users/userpath/.ssh'</span>.<br></code></pre></td></tr></table></figure></p><p>再接下来会提示你输入密码，这个密码是用来每次提交的时候输入确认，可以不设置，直接回车两次<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-string">Enter</span> <span class="hljs-string">passphrase</span> <span class="hljs-string">(empty</span> <span class="hljs-string">for</span> <span class="hljs-literal">no</span> <span class="hljs-string">passphrase):</span><br><span class="hljs-string">Enter</span> <span class="hljs-string">same</span> <span class="hljs-string">passphrase</span> <span class="hljs-attr">again:</span><br></code></pre></td></tr></table></figure></p><p>最后成功后会看到类似如下的输出，表示成功生成SSH key了，可以到C:/Users/你的用户账号/.ssh文件夹下看下<br><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs undefined">Your identification has been saved in /c/Users/Him/.ssh/id_rsa.<br>Your public key has been saved in /c/Users/Him/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>SHA256:RwvBINgH8CEt2KniltmykeyDsOseUYcwMzehFeyT86s 1225723686<span class="hljs-meta">@qq.com</span><br>The key's randomart image is:<br>+---[RSA 2048]----+<br>|<span class="hljs-string"> o+%OO+o.        </span>|<br>|<span class="hljs-string">..=+%*+ ..       </span>|<br>|<span class="hljs-string"> ..+o+o.. .      </span>|<br>|<span class="hljs-string">o.  o=.  o .     </span>|<br>|<span class="hljs-string">o oolalala S o      </span>|<br>|<span class="hljs-string"> +.+.. . .       </span>|<br>|<span class="hljs-string">. .o    .        </span>|<br>|<span class="hljs-string">  . .  .         </span>|<br>|<span class="hljs-string">   . E.          </span>|<br>+----[SHA256]-----+<br></code></pre></td></tr></table></figure></p><h3 id="4-添加SSH-key到GitHub"><a href="#4-添加SSH-key到GitHub" class="headerlink" title="4.添加SSH key到GitHub"></a>4.添加SSH key到GitHub</h3><p>首先复制.ssh文件夹下id_rsa.pub文件的内容，可以直接用文本编辑器打开复制，也可以用如下命令行复制<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-variable">$ </span>clip &lt; ~<span class="hljs-regexp">/.ssh/id</span>_rsa.pub<br></code></pre></td></tr></table></figure></p><p>然后进入<a href="https://github.com/settings/keys设置，如果没有登录要先登录" target="_blank" rel="noopener">https://github.com/settings/keys设置，如果没有登录要先登录</a></p><p>或者登录后依次点击右上角Settings，然后再点击SSH and GPG keys</p><p>然后点击New SSH key按钮，然后输入Title和我们刚才复制的Key，Title的话表示这个key来自哪里，比如说可以叫“家里的笔记本”</p><p>最后点击Add SSH key按钮保存</p><h3 id="5-测试SSH连接"><a href="#5-测试SSH连接" class="headerlink" title="5.测试SSH连接"></a>5.测试SSH连接</h3><p>输入如下命令<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-variable">$ </span>ssh -T git<span class="hljs-variable">@github</span>.com<br></code></pre></td></tr></table></figure></p><p>会得到如下输出，询问是否确认连接，输入yes回车确认<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">The authenticity of host &apos;github.com (13.229.188.59)&apos; can&apos;t be established.<br>RSA key fingerprint is SHA256:nThbg6kXUpJWGl7mykeyCspRomTxdCARLviKw6E5SY8.<br>Are you sure you want to continue connecting (yes/no)? yes<br>Click and drag to move<br></code></pre></td></tr></table></figure></p><p>最后连接成功会看到如下输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">Warning: Permanently added &apos;github.com,13.229.188.59&apos; (RSA) to the list of known hosts.<br>Hi ghxiaoxiao! You&apos;ve successfully authenticated, but GitHub does not provide shell access.<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>kubenetes安装metrics-server_unable_to_handle_the_request</title>
    <link href="/2021/07/01/kubenetes%E5%AE%89%E8%A3%85metrics-server-unable-to-handle-the-request/"/>
    <url>/2021/07/01/kubenetes%E5%AE%89%E8%A3%85metrics-server-unable-to-handle-the-request/</url>
    
    <content type="html"><![CDATA[<p>今天在安装kubernetes metrics-server时，报错</p><p>Error from server (ServiceUnavailable): the server is currently unable to handle the request (get nodes.metrics.k8s.io)</p><p>执行如下<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@localhost ~]# kubectl top node<br><span class="hljs-builtin-name">Error</span> <span class="hljs-keyword">from</span><span class="hljs-built_in"> server </span>(ServiceUnavailable): the<span class="hljs-built_in"> server </span>is currently unable <span class="hljs-keyword">to</span> handle the request (<span class="hljs-builtin-name">get</span> nodes.metrics.k8s.io)<br></code></pre></td></tr></table></figure></p><p>解决方案：<br>在/etc/kubernetes/manifests/kube-apiserver.yaml 文件command字段下增加<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attr">--enable-aggregator-routing</span>=<span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure></p><p>然后重启kubelet便能解决<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">systemctl  restart  kubelet<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>自动更新Linux内核脚本</title>
    <link href="/2021/06/25/%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0Linux%E5%86%85%E6%A0%B8%E8%84%9A%E6%9C%AC/"/>
    <url>/2021/06/25/%E8%87%AA%E5%8A%A8%E6%9B%B4%E6%96%B0Linux%E5%86%85%E6%A0%B8%E8%84%9A%E6%9C%AC/</url>
    
    <content type="html"><![CDATA[<h3 id="经常玩kubernetes-的都知道如果想要你的集群支持IPVS-就必须把内核升级到4-18以上，所以就有了下面的脚本"><a href="#经常玩kubernetes-的都知道如果想要你的集群支持IPVS-就必须把内核升级到4-18以上，所以就有了下面的脚本" class="headerlink" title="经常玩kubernetes 的都知道如果想要你的集群支持IPVS 就必须把内核升级到4.18以上，所以就有了下面的脚本"></a>经常玩kubernetes 的都知道如果想要你的集群支持IPVS 就必须把内核升级到4.18以上，所以就有了下面的脚本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">#!/bin/bash<br>yum -y upgrade &amp;&amp;<br>rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org &amp;&amp;<br># rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm;<br>rpm -Uvh https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm;<br>yum --disablerepo=\* --enablerepo=elrepo-kernel repolist &amp;&amp;<br>yum --disablerepo=\* --enablerepo=elrepo-kernel install  kernel-ml.x86_64  -y &amp;&amp;<br>yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64  -y &amp;&amp;<br>yum --disablerepo=\* --enablerepo=elrepo-kernel install kernel-ml-tools kernel-ml-devel kernel-ml-headers -y &amp;&amp;<br>sudo awk -F\&apos; &apos;$1==&quot;menuentry &quot; &#123;print i++ &quot; : &quot; $2&#125;&apos; /etc/grub2.cfg;<br>sudo grub2-set-default 0 &amp;&amp; <br>sudo grub2-mkconfig -o /boot/grub2/grub.cfg<br></code></pre></td></tr></table></figure><h4 id="此脚本只在centos7-验证过"><a href="#此脚本只在centos7-验证过" class="headerlink" title="此脚本只在centos7 验证过"></a>此脚本只在centos7 验证过</h4>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Centos7卸载旧内核清理</title>
    <link href="/2021/01/16/Centos7%E5%8D%B8%E8%BD%BD%E6%97%A7%E5%86%85%E6%A0%B8%E6%B8%85%E7%90%86/"/>
    <url>/2021/01/16/Centos7%E5%8D%B8%E8%BD%BD%E6%97%A7%E5%86%85%E6%A0%B8%E6%B8%85%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h3 id="查看当前操作系统的所有内核版本"><a href="#查看当前操作系统的所有内核版本" class="headerlink" title="查看当前操作系统的所有内核版本"></a>查看当前操作系统的所有内核版本</h3><h5 id="Linux在升级新的内核后，旧有的系统内核并不会删除或停用，而当系统再次启动时，默认的还是使用之前的旧有内核，比如说就像下面这种情况："><a href="#Linux在升级新的内核后，旧有的系统内核并不会删除或停用，而当系统再次启动时，默认的还是使用之前的旧有内核，比如说就像下面这种情况：" class="headerlink" title="Linux在升级新的内核后，旧有的系统内核并不会删除或停用，而当系统再次启动时，默认的还是使用之前的旧有内核，比如说就像下面这种情况："></a>Linux在升级新的内核后，旧有的系统内核并不会删除或停用，而当系统再次启动时，默认的还是使用之前的旧有内核，比如说就像下面这种情况：</h5><ol><li><p>在删除旧有内核版本前，我们先重启电脑，将电脑选择为最新版内核版本启动，通过下面的方式查看当前操作系统所使用的内核版本。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-selector-attr">[root@11111 ~]</span># <span class="hljs-selector-tag">uname</span> <span class="hljs-selector-tag">-r</span><br>5<span class="hljs-selector-class">.2</span><span class="hljs-selector-class">.2-1</span><span class="hljs-selector-class">.el7</span><span class="hljs-selector-class">.elrepo</span><span class="hljs-selector-class">.x86_64</span><br></code></pre></td></tr></table></figure></li><li><p>查看当前系统所有的内核版本</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@<span class="hljs-number">11111</span> ~]# rpm -qa | grep kernel<br>kernel<span class="hljs-number">-3.10</span><span class="hljs-number">.0</span><span class="hljs-number">-957.1</span><span class="hljs-number">.3</span>.el7.x86_64<br>kernel-tools-libs<span class="hljs-number">-3.10</span><span class="hljs-number">.0</span><span class="hljs-number">-1160.11</span><span class="hljs-number">.1</span>.el7.x86_64<br>kernel-debug-devel<span class="hljs-number">-3.10</span><span class="hljs-number">.0</span><span class="hljs-number">-1160.11</span><span class="hljs-number">.1</span>.el7.x86_64<br>kernel-headers<span class="hljs-number">-3.10</span><span class="hljs-number">.0</span><span class="hljs-number">-1160.11</span><span class="hljs-number">.1</span>.el7.x86_64<br>kernel-ml<span class="hljs-number">-5.10</span><span class="hljs-number">.7</span><span class="hljs-number">-1.</span>el7.elrepo.x86_64<br>kernel<span class="hljs-number">-3.10</span><span class="hljs-number">.0</span><span class="hljs-number">-1160.11</span><span class="hljs-number">.1</span>.el7.x86_64<br>kernel-tools<span class="hljs-number">-3.10</span><span class="hljs-number">.0</span><span class="hljs-number">-1160.11</span><span class="hljs-number">.1</span>.el7.x86_64<br></code></pre></td></tr></table></figure></li><li><p>删除旧有内核版本</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-selector-tag">yum</span> <span class="hljs-selector-tag">remove</span> <span class="hljs-selector-tag">kernel-3</span><span class="hljs-selector-class">.10</span><span class="hljs-selector-class">.0-514</span><span class="hljs-selector-class">.26</span><span class="hljs-selector-class">.2</span><span class="hljs-selector-class">.el7</span><span class="hljs-selector-class">.x86_64</span><br></code></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>centos7  kernel</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MacOS下刻录系统盘</title>
    <link href="/2021/01/15/MacOS%E4%B8%8B%E5%88%BB%E5%BD%95%E7%B3%BB%E7%BB%9F%E7%9B%98/"/>
    <url>/2021/01/15/MacOS%E4%B8%8B%E5%88%BB%E5%BD%95%E7%B3%BB%E7%BB%9F%E7%9B%98/</url>
    
    <content type="html"><![CDATA[<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>找到U盘位置<br>先将U盘插入电脑，打开终端，用以下命令找到U盘位置<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">diskutil <span class="hljs-built_in">list</span><br></code></pre></td></tr></table></figure></p><p>U盘位置为/dev/disk2</p><p> 取消挂载U盘, 采用以下命令取消挂载U盘<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">diskutil unmountDisk <span class="hljs-regexp">/dev/</span>disk2<br></code></pre></td></tr></table></figure></p><p>刻录<br>采用以下命刻录(换成你自己的实际目录)，耗时比较长，请耐心等待<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">sudo dd <span class="hljs-attribute">if</span>=~/Downloads/ubuntu-20.10-desktop-amd64.iso <span class="hljs-attribute">of</span>=/dev/disk2 <span class="hljs-attribute">bs</span>=1m;<br></code></pre></td></tr></table></figure></p><p>最后采用以下命令“弹出”U盘<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">diskutil eject <span class="hljs-regexp">/dev/</span>disk2<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
    <tags>
      
      <tag>Mac  ISO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>限制国家地区访问Nginx</title>
    <link href="/2021/01/03/%E9%99%90%E5%88%B6%E5%9B%BD%E5%AE%B6%E5%9C%B0%E5%8C%BA%E8%AE%BF%E9%97%AENginx/"/>
    <url>/2021/01/03/%E9%99%90%E5%88%B6%E5%9B%BD%E5%AE%B6%E5%9C%B0%E5%8C%BA%E8%AE%BF%E9%97%AENginx/</url>
    
    <content type="html"><![CDATA[<h4 id="简介我们经常发现有一些莫名其妙的ip访问我们的网站，并且这些ip用户一看就不是我们的目标客户，所以有时候我们需要屏蔽某些国家或者地区的ip。"><a href="#简介我们经常发现有一些莫名其妙的ip访问我们的网站，并且这些ip用户一看就不是我们的目标客户，所以有时候我们需要屏蔽某些国家或者地区的ip。" class="headerlink" title="简介我们经常发现有一些莫名其妙的ip访问我们的网站，并且这些ip用户一看就不是我们的目标客户，所以有时候我们需要屏蔽某些国家或者地区的ip。"></a>简介我们经常发现有一些莫名其妙的ip访问我们的网站，并且这些ip用户一看就不是我们的目标客户，所以有时候我们需要屏蔽某些国家或者地区的ip。</h4><h4 id="本篇文章环境：系统centos7，nginx版本"><a href="#本篇文章环境：系统centos7，nginx版本" class="headerlink" title="本篇文章环境：系统centos7，nginx版本"></a>本篇文章环境：系统centos7，nginx版本</h4><p>我们经常发现有一些莫名其妙的ip访问我们的网站，并且这些ip用户一看就不是我们的目标客户，所以有时候我们需要屏蔽某些国家或者地区的ip。</p><p>1、安装geoip库 安装完成后，geip数据在/usr/share/GeoIP/目录下<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum -y <span class="hljs-keyword">install</span> geoip-devel<br></code></pre></td></tr></table></figure></p><p>2、重新编译nginx，添加http_geoip_module模块 检查原nginx配置参数<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs undefined">/usr/<span class="hljs-keyword">local</span>/nginx/sbin/nginx  -V<br><br><br>nginx <span class="hljs-built_in">version</span>: nginx/<span class="hljs-number">1.18</span><span class="hljs-number">.0</span><br>built <span class="hljs-keyword">by</span> gcc <span class="hljs-number">4.8</span><span class="hljs-number">.5</span> <span class="hljs-number">20150623</span> (Red Hat <span class="hljs-number">4.8</span><span class="hljs-number">.5</span><span class="hljs-number">-44</span>) (GCC)<br>built <span class="hljs-keyword">with</span> OpenSSL <span class="hljs-number">1.0</span><span class="hljs-number">.2</span>k-fips  <span class="hljs-number">26</span> Jan <span class="hljs-number">2017</span><br>TLS SNI support enabled<br>configure arguments: <span class="hljs-comment">--prefix=/usr/local/nginx --with-http_ssl_module --with-http_v2_module</span><br></code></pre></td></tr></table></figure></p><p>nginx源码目录下加上–with-http_geoip_module=dynamic参数重新配置<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-keyword">cd</span> <span class="hljs-string">/usr/local/src/nginx-1.18.0</span><br><span class="hljs-string">./configure</span> <span class="hljs-params">--prefix=/usr/local/nginx</span> <span class="hljs-params">--with-http_ssl_module</span> <span class="hljs-params">--with-http_v2_module</span> <span class="hljs-params">--with-http_geoip_module=dynamic</span><br></code></pre></td></tr></table></figure></p><p>make，注意不要make install，加上make install会覆盖现有nginx目录<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">make</span><br></code></pre></td></tr></table></figure></p><p>3、拷贝新编译的nginx和ngx_http_geoip_module.so到nginx安装目录下</p><h5 id="备份，防止出错"><a href="#备份，防止出错" class="headerlink" title="备份，防止出错"></a>备份，防止出错</h5><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs undefined">mv <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/nginx/</span>sbin<span class="hljs-regexp">/nginx /</span>usr<span class="hljs-regexp">/local/</span>nginx<span class="hljs-regexp">/sbin/</span>nginx.bak<br><br><br>cp <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/src/</span>nginx<span class="hljs-number">-1.18</span><span class="hljs-number">.0</span><span class="hljs-regexp">/objs/</span>nginx <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/nginx/</span>sbin/<br><br>mkdir <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/nginx/</span>modules<br><br>cp <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/src/</span>nginx<span class="hljs-number">-1.18</span><span class="hljs-number">.0</span><span class="hljs-regexp">/objs/</span>ngx_http_geoip_module.so <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/nginx/</span>modules/<br></code></pre></td></tr></table></figure><p>4、配置规则<br>nginx.conf配置文件添加lngx_http_geoip_module.so模块<br><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">load_module modules/ngx_http_geoip_module.so<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure></p><p>nginx.conf添加规则<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment"># 访问地理位置限制规则</span><br>   geoip_country /usr/share/GeoIP/GeoIP.dat;<br><br>    # geoip_city    /usr/share/GeoIP/GeoLiteCity.dat;<br>    # 下面一行根据实际情况编写<br>    map <span class="hljs-variable">$geoip_country_code</span> <span class="hljs-variable">$allowed_country</span> &#123;<br>       <span class="hljs-built_in"> default </span><span class="hljs-literal">no</span>;<br>        CN <span class="hljs-literal">yes</span>;<br>    &#125;<br></code></pre></td></tr></table></figure></p><p>站点配置文件server块中添加访问限制<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-section">server</span> &#123;<br><span class="hljs-attribute">listen</span> <span class="hljs-number">80</span>;  <br><span class="hljs-attribute">server_name</span> xx.xx.xx;<br><br>    <span class="hljs-comment">#添加判断</span><br>    <span class="hljs-attribute">if</span> (<span class="hljs-variable">$allowed_country</span> = <span class="hljs-literal">no</span>) &#123;<br>        <span class="hljs-attribute">return</span> <span class="hljs-number">403</span>;<br>    &#125;<br><br>&#125;<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
    <tags>
      
      <tag>Nginx IP geoip</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Centos7开启GOOGLE-BBR加速配置</title>
    <link href="/2021/01/03/Centos7%E5%BC%80%E5%90%AFGOOGLE-BBR%E5%8A%A0%E9%80%9F%E9%85%8D%E7%BD%AE/"/>
    <url>/2021/01/03/Centos7%E5%BC%80%E5%90%AFGOOGLE-BBR%E5%8A%A0%E9%80%9F%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h3 id="简介BBR-是-Google-提出的一种新型拥塞控制算法，可以使-Linux-服务器显著地提高吞吐量和减少-TCP-连接的延迟。"><a href="#简介BBR-是-Google-提出的一种新型拥塞控制算法，可以使-Linux-服务器显著地提高吞吐量和减少-TCP-连接的延迟。" class="headerlink" title="简介BBR 是 Google 提出的一种新型拥塞控制算法，可以使 Linux 服务器显著地提高吞吐量和减少 TCP 连接的延迟。"></a>简介BBR 是 Google 提出的一种新型拥塞控制算法，可以使 Linux 服务器显著地提高吞吐量和减少 TCP 连接的延迟。</h3><h4 id="BBR-是-Google-提出的一种新型拥塞控制算法，可以使-Linux-服务器显著地提高吞吐量和减少-TCP-连接的延迟。"><a href="#BBR-是-Google-提出的一种新型拥塞控制算法，可以使-Linux-服务器显著地提高吞吐量和减少-TCP-连接的延迟。" class="headerlink" title="BBR 是 Google 提出的一种新型拥塞控制算法，可以使 Linux 服务器显著地提高吞吐量和减少 TCP 连接的延迟。"></a>BBR 是 Google 提出的一种新型拥塞控制算法，可以使 Linux 服务器显著地提高吞吐量和减少 TCP 连接的延迟。</h4><p>虚拟主机开启bbr加速，效果还是非常明显的。</p><p>本文记录CentOs7下手动开启bbr加速。</p><p>1、查看当前内核<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">uname -r</span><br></code></pre></td></tr></table></figure></p><p>2、导入ELRepo公钥<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">rpm --<span class="hljs-keyword">import</span> <span class="hljs-string">https:</span><span class="hljs-comment">//www.elrepo.org/RPM-GPG-KEY-elrepo.org</span><br></code></pre></td></tr></table></figure></p><p>3、安装ELRepo 现在官网提供（centos6，centos7，centos8源）我这里是centos7.x版本所以选择了7<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum install <span class="hljs-string">https:</span><span class="hljs-comment">//www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm -y</span><br></code></pre></td></tr></table></figure></p><p>4、查看ELRepo提供的内核版本<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum <span class="hljs-params">--disablerepo=</span><span class="hljs-string">"*"</span> <span class="hljs-params">--enablerepo=</span><span class="hljs-string">"elrepo-kernel"</span> list available<br></code></pre></td></tr></table></figure></p><p>5、安装kernel-ml内核，此步安装时间可能比较长，耐心等待并非卡住了</p><h4 id="kernel-lt：表示longterm，即长期支持的内核；当前为4-4-。"><a href="#kernel-lt：表示longterm，即长期支持的内核；当前为4-4-。" class="headerlink" title="kernel-lt：表示longterm，即长期支持的内核；当前为4.4.*。"></a>kernel-lt：表示longterm，即长期支持的内核；当前为4.4.*。</h4><h4 id="kernel-ml：表示mainline，即当前主线的内核；当前为5-2"><a href="#kernel-ml：表示mainline，即当前主线的内核；当前为5-2" class="headerlink" title="kernel-ml：表示mainline，即当前主线的内核；当前为5.2.*"></a>kernel-ml：表示mainline，即当前主线的内核；当前为5.2.*</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum <span class="hljs-attribute">--disablerepo</span>=<span class="hljs-string">'*'</span> <span class="hljs-attribute">--enablerepo</span>=elrepo-kernel install kernel-ml -y<br></code></pre></td></tr></table></figure><p>6、设置默认启动为新内核<br><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">grub2-<span class="hljs-keyword">set</span>-<span class="hljs-keyword">default</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure></p><p>7、启用BBR<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">echo <span class="hljs-string">'net.core.default_qdisc=fq'</span> <span class="hljs-meta">&gt;&gt; </span>/etc/sysctl.conf<br>echo <span class="hljs-string">'net.ipv4.tcp_congestion_control=bbr'</span> <span class="hljs-meta">&gt;&gt; </span>/etc/sysctl.conf<br></code></pre></td></tr></table></figure></p><p>8、重启系统<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">reboot</span><br></code></pre></td></tr></table></figure></p><p>9、检查BBR是否成功<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">sysctl -n net.ipv4.tcp_congestion_control<br>lsmod <span class="hljs-string">| grep bbr</span><br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
    <tags>
      
      <tag>Centos  BBR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubernetes常用对象</title>
    <link href="/2020/11/30/kubernetes%E5%B8%B8%E7%94%A8%E5%AF%B9%E8%B1%A1/"/>
    <url>/2020/11/30/kubernetes%E5%B8%B8%E7%94%A8%E5%AF%B9%E8%B1%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Master"><a href="#1-Master" class="headerlink" title="1. Master"></a>1. Master</h1><p>集群的控制节点，负责整个集群的管理和控制，kubernetes的所有的命令基本都是发给Master，由它来负责具体的执行过程。</p><h2 id="1-1-Master的组件"><a href="#1-1-Master的组件" class="headerlink" title="1.1. Master的组件"></a>1.1. Master的组件</h2><ul><li>kube-apiserver：资源增删改查的入口</li><li>kube-controller-manager：资源对象的大总管</li><li>kube-scheduler：负责资源调度（Pod调度）</li><li>etcd Server:kubernetes的所有的资源对象的数据保存在etcd中。</li></ul><h1 id="2-Node"><a href="#2-Node" class="headerlink" title="2. Node"></a>2. Node</h1><p>Node是集群的工作负载节点，默认情况kubelet会向Master注册自己，一旦Node被纳入集群管理范围，kubelet会定时向Master汇报自身的情报，包括操作系统，Docker版本，机器资源情况等。</p><p>如果Node超过指定时间不上报信息，会被Master判断为“失联”，标记为Not Ready，随后Master会触发Pod转移。</p><h2 id="2-1-Node的组件"><a href="#2-1-Node的组件" class="headerlink" title="2.1. Node的组件"></a>2.1. Node的组件</h2><ul><li>kubelet:Pod的管家，与Master通信</li><li>kube-proxy：实现kubernetes Service的通信与负载均衡机制的重要组件</li><li>Docker：容器的创建和管理</li></ul><h2 id="2-2-Node相关命令"><a href="#2-2-Node相关命令" class="headerlink" title="2.2. Node相关命令"></a>2.2. Node相关命令</h2><p>kubectl get nodes</p><p>kuebctl describe node {node_name}</p><h2 id="2-3-describe命令的Node信息"><a href="#2-3-describe命令的Node信息" class="headerlink" title="2.3. describe命令的Node信息"></a>2.3. describe命令的Node信息</h2><ul><li>Node基本信息：名称、标签、创建时间等</li><li>Node当前的状态，Node启动后会进行自检工作，磁盘是否满，内存是否不足，若都正常则切换为Ready状态。</li><li>Node的主机地址与主机名</li><li>Node上的资源总量：CPU,内存，最大可调度Pod数量等</li><li>Node可分配资源量：当前Node可用于分配的资源量</li><li>主机系统信息：主机唯一标识符UUID，Linux kernel版本号，操作系统，kubernetes版本，kubelet与kube-proxy版本</li><li>当前正在运行的Pod列表及概要信息</li><li>已分配的资源使用概要，例如资源申请的最低、最大允许使用量占系统总量的百分比</li><li>Node相关的Event信息。</li></ul><h1 id="3-Pod"><a href="#3-Pod" class="headerlink" title="3. Pod"></a>3. Pod</h1><p>Pod是Kubernetes中操作的基本单元。每个Pod中有个根容器(Pause容器)，Pause容器的状态代表整个容器组的状态，其他业务容器共享Pause的IP，即Pod IP，共享Pause挂载的Volume，这样简化了同个Pod中不同容器之间的网络问题和文件共享问题。</p><p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1510578930/article/kubernetes/concept/pod.png" srcset="/img/loading.gif" lazyload alt="pod"></p><ol><li>Kubernetes集群中，同宿主机的或不同宿主机的Pod之间要求能够TCP/IP直接通信，因此采用虚拟二层网络技术来实现，例如Flannel，Openvswitch(OVS)等，这样在同个集群中，不同的宿主机的Pod IP为不同IP段的IP，集群中的所有Pod IP都是唯一的，不同Pod之间可以直接通信。</li><li>Pod有两种类型：普通Pod和静态Pod。静态Pod即不通过K8S调度和创建，直接在某个具体的Node机器上通过具体的文件来启动。普通Pod则是由K8S创建、调度，同时数据存放在ETCD中。</li><li>Pod IP和具体的容器端口（ContainnerPort）组成一个具体的通信地址，即Endpoint。一个Pod中可以存在多个容器，可以有多个端口，Pod IP一样，即有多个Endpoint。</li><li>Pod Volume是定义在Pod之上，被各个容器挂载到自己的文件系统中，可以用分布式文件系统实现后端存储功能。</li><li>Pod中的Event事件可以用来排查问题，可以通过kubectl describe pod xxx 来查看对应的事件。</li><li>每个Pod可以对其能使用的服务器上的计算资源设置限额，一般为CPU和Memory。K8S中一般将千分之一个的CPU配置作为最小单位，用m表示，是一个绝对值，即100m对于一个Core的机器还是48个Core的机器都是一样的大小。Memory配额也是个绝对值，单位为内存字节数。</li><li>资源配额的两个参数</li></ol><ul><li>Requests:该资源的最小申请量，系统必须满足要求。</li><li>Limits:该资源最大允许使用量，当超过该量，K8S会kill并重启Pod。</li></ul><p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1510578930/article/kubernetes/concept/pod2.png" srcset="/img/loading.gif" lazyload alt="pod2"></p><h1 id="4-Label"><a href="#4-Label" class="headerlink" title="4. Label"></a>4. Label</h1><ol><li>Label是一个键值对，可以附加在任何对象上，比如Node,Pod,Service,RC等。Label和资源对象是多对多的关系，即一个Label可以被添加到多个对象上，一个对象也可以定义多个Label。</li><li>Label的作用主要用来实现精细的、多维度的资源分组管理，以便进行资源分配，调度，配置，部署等工作。</li><li>Label通俗理解就是“标签”，通过标签来过滤筛选指定的对象，进行具体的操作。k8s通过Label Selector(标签选择器)来筛选指定Label的资源对象，类似SQL语句中的条件查询（WHERE语句）。</li><li>Label Selector有基于等式和基于集合的两种表达方式，可以多个条件进行组合使用。</li></ol><ul><li>基于等式：name=redis-slave（匹配name=redis-slave的资源对象）;env!=product(匹配所有不具有标签env=product的资源对象)</li><li>基于集合：name in (redis-slave,redis-master);name not in (php-frontend)（匹配所有不具有标签name=php-frontend的资源对象）</li></ul><p><strong>使用场景</strong></p><ol><li>kube-controller进程通过资源对象RC上定义的Label Selector来筛选要监控的Pod副本数，从而实现副本数始终保持预期数目。</li><li>kube-proxy进程通过Service的Label Selector来选择对应Pod，自动建立每个Service到对应Pod的请求转发路由表，从而实现Service的智能负载均衡机制。</li><li>kube-scheduler实现Pod定向调度：对Node定义特定的Label，并且在Pod定义文件中使用NodeSelector标签调度策略。</li></ol><h1 id="5-Replication-Controller-RC"><a href="#5-Replication-Controller-RC" class="headerlink" title="5. Replication Controller(RC)"></a>5. Replication Controller(RC)</h1><p>RC是k8s系统中的核心概念，定义了一个期望的场景。</p><p>主要包括：</p><ul><li>Pod期望的副本数（replicas）</li><li>用于筛选目标Pod的Label Selector</li><li>用于创建Pod的模板（template）</li></ul><p>RC特性说明：</p><ol><li>Pod的缩放可以通过以下命令实现：kubectl scale rc redis-slave –replicas=3</li><li>删除RC并不会删除该RC创建的Pod，可以将副本数设置为0，即可删除对应Pod。或者通过kubectl stop /delete命令来一次性删除RC和其创建的Pod。</li><li>改变RC中Pod模板的镜像版本可以实现滚动升级（Rolling Update）。具体操作见<a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/</a></li><li>Kubernetes1.2以上版本将RC升级为Replica Set，它与当前RC的唯一区别在于Replica Set支持基于集合的Label Selector(Set-based selector)，而旧版本RC只支持基于等式的Label Selector(equality-based selector)。</li><li>Kubernetes1.2以上版本通过Deployment来维护Replica Set而不是单独使用Replica Set。即控制流为：Delpoyment→Replica Set→Pod。即新版本的Deployment+Replica Set替代了RC的作用。</li></ol><h1 id="6-Deployment"><a href="#6-Deployment" class="headerlink" title="6. Deployment"></a>6. Deployment</h1><p>Deployment是kubernetes 1.2引入的概念，用来解决Pod的编排问题。Deployment可以理解为RC的升级版（RC+Reolicat Set）。特点在于可以随时知道Pod的部署进度，即对Pod的创建、调度、绑定节点、启动容器完整过程的进度展示。</p><p><strong>使用场景</strong></p><ol><li>创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。</li><li>检查Deployment的状态来确认部署动作是否完成（Pod副本的数量是否达到预期值）。</li><li>更新Deployment以创建新的Pod(例如镜像升级的场景)。</li><li>如果当前Deployment不稳定，回退到上一个Deployment版本。</li><li>挂起或恢复一个Deployment。</li></ol><p>可以通过kubectl describe deployment来查看Deployment控制的Pod的水平拓展过程。</p><h1 id="7-Horizontal-Pod-Autoscaler-HPA"><a href="#7-Horizontal-Pod-Autoscaler-HPA" class="headerlink" title="7. Horizontal Pod Autoscaler(HPA)"></a>7. Horizontal Pod Autoscaler(HPA)</h1><p>Horizontal Pod Autoscaler(HPA)即Pod横向自动扩容，与RC一样也属于k8s的资源对象。</p><p>HPA原理：通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否针对性调整Pod的副本数。</p><p>Pod负载度量指标：</p><ul><li>CPUUtilizationPercentage：Pod所有副本自身的CPU利用率的平均值。即当前Pod的CPU使用量除以Pod Request的值。</li><li>应用自定义的度量指标，比如服务每秒内响应的请求数（TPS/QPS）。</li></ul><h1 id="8-Service-服务"><a href="#8-Service-服务" class="headerlink" title="8. Service(服务)"></a>8. Service(服务)</h1><h2 id="8-1-Service概述"><a href="#8-1-Service概述" class="headerlink" title="8.1. Service概述"></a>8.1. Service概述</h2><p><img src="https://res.cloudinary.com/dqxtn0ick/image/upload/v1510578930/article/kubernetes/concept/service.png" srcset="/img/loading.gif" lazyload alt="service"></p><p>Service定义了一个服务的访问入口地址，前端应用通过这个入口地址访问其背后的一组由Pod副本组成的集群实例，Service与其后端的Pod副本集群之间是通过Label Selector来实现“无缝对接”。RC保证Service的Pod副本实例数目保持预期水平。</p><h2 id="8-2-kubernetes的服务发现机制"><a href="#8-2-kubernetes的服务发现机制" class="headerlink" title="8.2. kubernetes的服务发现机制"></a>8.2. kubernetes的服务发现机制</h2><p>主要通过kube-dns这个组件来进行DNS方式的服务发现。</p><h2 id="8-3-外部系统访问Service的问题"><a href="#8-3-外部系统访问Service的问题" class="headerlink" title="8.3. 外部系统访问Service的问题"></a>8.3. 外部系统访问Service的问题</h2><table><thead><tr><th>IP类型</th><th>说明</th></tr></thead><tbody><tr><td>Node IP</td><td>Node节点的IP地址</td></tr><tr><td>Pod IP</td><td>Pod的IP地址</td></tr><tr><td>Cluster IP</td><td>Service的IP地址</td></tr></tbody></table><h2 id="8-3-1-Node-IP"><a href="#8-3-1-Node-IP" class="headerlink" title="8.3.1. Node IP"></a>8.3.1. Node IP</h2><p>NodeIP是集群中每个节点的物理网卡IP地址，是真实存在的物理网络，kubernetes集群之外的节点访问kubernetes内的某个节点或TCP/IP服务的时候，需要通过NodeIP进行通信。</p><h2 id="8-3-2-Pod-IP"><a href="#8-3-2-Pod-IP" class="headerlink" title="8.3.2. Pod IP"></a>8.3.2. Pod IP</h2><p>Pod IP是每个Pod的IP地址，是Docker Engine根据docker0网桥的IP段地址进行分配的，是一个虚拟二层网络，集群中一个Pod的容器访问另一个Pod中的容器，是通过Pod IP进行通信的，而真实的TCP/IP流量是通过Node IP所在的网卡流出的。</p><h2 id="8-3-3-Cluster-IP"><a href="#8-3-3-Cluster-IP" class="headerlink" title="8.3.3. Cluster IP"></a>8.3.3. Cluster IP</h2><ol><li>Service的Cluster IP是一个虚拟IP，只作用于Service这个对象，由kubernetes管理和分配IP地址（来源于Cluster IP地址池）。</li><li>Cluster IP无法被ping通，因为没有一个实体网络对象来响应。</li><li>Cluster IP结合Service Port组成的具体通信端口才具备TCP/IP通信基础，属于kubernetes集群内，集群外访问该IP和端口需要额外处理。</li><li>k8s集群内Node IP 、Pod IP、Cluster IP之间的通信采取k8s自己的特殊的路由规则，与传统IP路由不同。</li></ol><h2 id="8-3-4-外部访问Kubernetes集群"><a href="#8-3-4-外部访问Kubernetes集群" class="headerlink" title="8.3.4. 外部访问Kubernetes集群"></a>8.3.4. 外部访问Kubernetes集群</h2><p>通过宿主机与容器端口映射的方式进行访问，例如：Service定位文件如下：</p><p>可以通过任意Node的IP 加端口访问该服务。也可以通过Nginx或HAProxy来设置负载均衡。</p><h1 id="9-Volume-存储卷"><a href="#9-Volume-存储卷" class="headerlink" title="9. Volume(存储卷)"></a>9. Volume(存储卷)</h1><h2 id="9-1-Volume的功能"><a href="#9-1-Volume的功能" class="headerlink" title="9.1. Volume的功能"></a>9.1. Volume的功能</h2><ol><li>Volume是Pod中能够被多个容器访问的共享目录，可以让容器的数据写到宿主机上或者写文件到网络存储中</li><li>可以实现容器配置文件集中化定义与管理，通过ConfigMap资源对象来实现。</li></ol><h2 id="9-2-Volume的特点"><a href="#9-2-Volume的特点" class="headerlink" title="9.2. Volume的特点"></a>9.2. Volume的特点</h2><p>k8s中的Volume与Docker的Volume相似，但不完全相同。</p><ol><li>k8s上Volume定义在Pod上，然后被一个Pod中的多个容器挂载到具体的文件目录下。</li><li>k8s的Volume与Pod生命周期相关而不是容器是生命周期，即容器挂掉，数据不会丢失但是Pod挂掉，数据则会丢失。</li><li>k8s中的Volume支持多种类型的Volume：Ceph、GlusterFS等分布式系统。</li></ol><h2 id="9-3-Volume的使用方式"><a href="#9-3-Volume的使用方式" class="headerlink" title="9.3. Volume的使用方式"></a>9.3. Volume的使用方式</h2><p>先在Pod上声明一个Volume，然后容器引用该Volume并Mount到容器的某个目录。</p><h2 id="9-4-Volume类型"><a href="#9-4-Volume类型" class="headerlink" title="9.4. Volume类型"></a>9.4. Volume类型</h2><h2 id="9-4-1-emptyDir"><a href="#9-4-1-emptyDir" class="headerlink" title="9.4.1. emptyDir"></a>9.4.1. emptyDir</h2><p>emptyDir Volume是在Pod分配到Node时创建的，初始内容为空，无须指定宿主机上对应的目录文件，由K8S自动分配一个目录，当Pod被删除时，对应的emptyDir数据也会永久删除。</p><p><strong>作用</strong>：</p><ol><li>临时空间，例如程序的临时文件，无须永久保留</li><li>长时间任务的中间过程CheckPoint的临时保存目录</li><li>一个容器需要从另一个容器中获取数据的目录（即多容器共享目录）</li></ol><p><strong>说明</strong>：</p><p>目前用户无法设置emptyVolume的使用介质，如果kubelet的配置使用硬盘则emptyDir将创建在该硬盘上。</p><h2 id="9-4-2-hostPath"><a href="#9-4-2-hostPath" class="headerlink" title="9.4.2. hostPath"></a>9.4.2. hostPath</h2><p>hostPath是在Pod上挂载宿主机上的文件或目录。</p><p><strong>作用</strong>：</p><ol><li>容器应用日志需要持久化时，可以使用宿主机的高速文件系统进行存储</li><li>需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部应用可以直接访问Docker的文件系统。</li></ol><p><strong>注意点：</strong></p><ol><li>在不同的Node上具有相同配置的Pod可能会因为宿主机上的目录或文件不同导致对Volume上目录或文件的访问结果不一致。</li><li>如果使用了资源配额管理，则kubernetes无法将hostPath在宿主机上使用的资源纳入管理。</li></ol><h2 id="9-4-3-gcePersistentDisk"><a href="#9-4-3-gcePersistentDisk" class="headerlink" title="9.4.3. gcePersistentDisk"></a>9.4.3. gcePersistentDisk</h2><p>表示使用谷歌公有云提供的永久磁盘（Persistent Disk ,PD）存放Volume的数据，它与EmptyDir不同，PD上的内容会被永久保存。当Pod被删除时，PD只是被卸载时，但不会被删除。需要先创建一个永久磁盘，才能使用gcePersistentDisk。</p><p>使用gcePersistentDisk的限制条件：</p><ul><li>Node(运行kubelet的节点)需要是GCE虚拟机。</li><li>虚拟机需要与PD存在于相同的GCE项目中和Zone中。</li></ul><h1 id="10-Persistent-Volume"><a href="#10-Persistent-Volume" class="headerlink" title="10. Persistent Volume"></a>10. Persistent Volume</h1><p>Volume定义在Pod上，属于“计算资源”的一部分，而Persistent Volume和Persistent Volume Claim是网络存储，简称PV和PVC，可以理解为k8s集群中某个网络存储中对应的一块存储。</p><ul><li>PV是网络存储，不属于任何Node，但可以在每个Node上访问。</li><li>PV不是定义在Pod上，而是独立于Pod之外定义。</li><li>PV常见类型：GCE Persistent Disks、NFS、RBD等。</li></ul><p>PV是有状态的对象，状态类型如下：</p><ul><li>Available:空闲状态</li><li>Bound:已经绑定到某个PVC上</li><li>Released:对应的PVC已经删除，但资源还没有回收</li><li>Failed:PV自动回收失败</li></ul><h1 id="11-Namespace"><a href="#11-Namespace" class="headerlink" title="11. Namespace"></a>11. Namespace</h1><p>Namespace即命名空间，主要用于多租户的资源隔离，通过将资源对象分配到不同的Namespace上，便于不同的分组在共享资源的同时可以被分别管理。</p><p>k8s集群启动后会默认创建一个“default”的Namespace。可以通过kubectl get namespaecs查看。</p><p>可以通过kubectl config use-context <code>namespace</code>配置当前k8s客户端的环境，通过kubectl get pods获取当前namespace的Pod。或者通过kubectl get pods –namespace=<code>NAMESPACE</code>来获取指定namespace的Pod。</p><p><strong>Namespace yaml文件的定义</strong></p><h1 id="12-Annotation-注解"><a href="#12-Annotation-注解" class="headerlink" title="12. Annotation(注解)"></a>12. Annotation(注解)</h1><p>Annotation与Label类似，也使用key/value的形式进行定义，Label定义元数据（Metadata）,Annotation定义“附加”信息。</p><p>通常Annotation记录信息如下：</p><ul><li>build信息，release信息，Docker镜像信息等。</li><li>日志库、监控库等。</li></ul><p>参考《Kubernetes权威指南》</p>]]></content>
    
    
    
    <tags>
      
      <tag>-Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>理解Kubernetes对象</title>
    <link href="/2020/11/30/%E7%90%86%E8%A7%A3Kubernetes%E5%AF%B9%E8%B1%A1/"/>
    <url>/2020/11/30/%E7%90%86%E8%A7%A3Kubernetes%E5%AF%B9%E8%B1%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="1-kubernetes对象概述"><a href="#1-kubernetes对象概述" class="headerlink" title="1. kubernetes对象概述"></a>1. kubernetes对象概述</h1><p>kubernetes中的对象是一些持久化的实体，可以理解为是对<code>集群状态的描述或期望</code>。</p><p>包括：</p><ul><li>集群中哪些node上运行了哪些容器化应用</li><li>应用的资源是否满足使用</li><li>应用的执行策略，例如重启策略、更新策略、容错策略等。</li></ul><p><strong>kubernetes的对象是一种意图（期望）的记录，kubernetes会始终保持预期创建的对象存在和集群运行在预期的状态下</strong>。</p><p>操作kubernetes对象（增删改查）需要通过<a href="https://kubernetes.io/docs/reference/" target="_blank" rel="noopener">kubernetes API</a>，一般有以下几种方式：</p><ul><li><code>kubectl</code>命令工具</li><li><code>Client library</code>的方式，例如 <a href="https://github.com/kubernetes/client-go" target="_blank" rel="noopener">client-go</a></li></ul><h1 id="2-Spec-and-Status"><a href="#2-Spec-and-Status" class="headerlink" title="2. Spec and Status"></a>2. Spec and Status</h1><p>每个kubernetes对象的结构描述都包含<code>spec</code>和<code>status</code>两个部分。</p><ul><li><code>spec</code>：该内容由用户提供，描述用户期望的对象特征及集群状态。</li><li><code>status</code>：该内容由kubernetes集群提供和更新，描述kubernetes对象的实时状态。</li></ul><p>任何时候，kubernetes都会控制集群的实时状态<code>status</code>与用户的预期状态<code>spec</code>一致。</p><p>例如：当你定义<code>Deployment</code>的描述文件，指定集群中运行3个实例，那么kubernetes会始终保持集群中运行3个实例，如果任何实例挂掉，kubernetes会自动重建新的实例来保持集群中始终运行用户预期的3个实例。</p><h1 id="3-对象描述文件"><a href="#3-对象描述文件" class="headerlink" title="3. 对象描述文件"></a>3. 对象描述文件</h1><p>当你要创建一个kubernetes对象的时候，需要提供该对象的描述信息<code>spec</code>，来描述你的对象在kubernetes中的预期状态。</p><p>一般使用kubernetes API来创建kubernetes对象，其中<code>spec</code>信息可以以<code>JSON</code>的形式存放在<code>request body</code>中，也可以以<code>.yaml</code>文件的形式通过<code>kubectl</code>工具创建。</p><p>例如，以下为<code>Deployment</code>对象对应的<code>yaml</code>文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1beta2</span> <span class="hljs-comment"># for versions before 1.8.0 use apps/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">nginx-deployment</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  replicas:</span> <span class="hljs-number">3</span><br><span class="hljs-attr">  selector:</span><br><span class="hljs-attr">    matchLabels:</span><br><span class="hljs-attr">      app:</span> <span class="hljs-string">nginx</span><br><span class="hljs-attr">  template:</span><br><span class="hljs-attr">    metadata:</span><br><span class="hljs-attr">      labels:</span><br><span class="hljs-attr">        app:</span> <span class="hljs-string">nginx</span><br><span class="hljs-attr">    spec:</span><br><span class="hljs-attr">      containers:</span><br><span class="hljs-attr">      - name:</span> <span class="hljs-string">nginx</span><br><span class="hljs-attr">        image:</span> <span class="hljs-attr">nginx:1.7.9</span><br><span class="hljs-attr">        ports:</span><br><span class="hljs-attr">        - containerPort:</span> <span class="hljs-number">80</span><br></code></pre></td></tr></table></figure><p>执行<code>kubectl create</code>的命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">#create command</span><br>kubectl create -f https://k8s.io/docs/user-guide/nginx-deployment.yaml --record<br><span class="hljs-comment">#output</span><br>deployment <span class="hljs-string">"nginx-deployment"</span> created<br></code></pre></td></tr></table></figure><h1 id="4-必须字段"><a href="#4-必须字段" class="headerlink" title="4. 必须字段"></a>4. 必须字段</h1><p>在对象描述文件<code>.yaml</code>中，必须包含以下字段。</p><ul><li>apiVersion：kubernetes API的版本</li><li>kind：kubernetes对象的类型</li><li>metadata：唯一标识该对象的元数据，包括<code>name</code>，UID，可选的<code>namespace</code></li><li>spec：标识对象的详细信息，不同对象的<code>spec</code>的格式不同，可以嵌套其他对象的字段。</li></ul><p>文章参考：</p><p><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vulnhub靶机HA:NARAk渗透</title>
    <link href="/2020/11/11/vulnhub%E9%9D%B6%E6%9C%BAHA-NARAk%E6%B8%97%E9%80%8F/"/>
    <url>/2020/11/11/vulnhub%E9%9D%B6%E6%9C%BAHA-NARAk%E6%B8%97%E9%80%8F/</url>
    
    <content type="html"><![CDATA[<h3 id="Vulnhub是一个提供各种漏洞环境的靶场，"><a href="#Vulnhub是一个提供各种漏洞环境的靶场，" class="headerlink" title="Vulnhub是一个提供各种漏洞环境的靶场，"></a>Vulnhub是一个提供各种漏洞环境的靶场，</h3><h4 id="镜像地址：-https-download-vulnhub-com-ha-narak-ova"><a href="#镜像地址：-https-download-vulnhub-com-ha-narak-ova" class="headerlink" title="镜像地址： https://download.vulnhub.com/ha/narak.ova"></a>镜像地址： <a href="https://download.vulnhub.com/ha/narak.ova" target="_blank" rel="noopener">https://download.vulnhub.com/ha/narak.ova</a></h4><h3 id="下载镜像后导入到VMware虚拟机，采用nat模式桥接即可。"><a href="#下载镜像后导入到VMware虚拟机，采用nat模式桥接即可。" class="headerlink" title="下载镜像后导入到VMware虚拟机，采用nat模式桥接即可。"></a>下载镜像后导入到VMware虚拟机，采用nat模式桥接即可。</h3><h5 id="以下操作均在Mac系统下-VMware-下操作"><a href="#以下操作均在Mac系统下-VMware-下操作" class="headerlink" title="以下操作均在Mac系统下 VMware 下操作"></a>以下操作均在Mac系统下 VMware 下操作</h5><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-number">1.</span> 打开Kali root 模式下 nmap 扫描发现开放端口<br>Nmap 扫面开放主机  Nmap -sN <span class="hljs-number">192.168</span><span class="hljs-number">.166</span><span class="hljs-number">.1</span>/<span class="hljs-number">24</span><br></code></pre></td></tr></table></figure><p>####<br><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">```<br></code></pre></td></tr></table></figure></p><p>一共开放了4个端口，80端口的web，22端口的ssh，69的tftp和68的dhcpc<br>其中80开放http服务<br><code>`</code></p><blockquote></blockquote><p><a href="https://raw.githubusercontent.com/willie-lin/Hexo-Blog-Picture/master/narak/1.png" target="_blank" rel="noopener">TCP端口开放</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>防火墙开放端口，查看状态，查看开放端口</title>
    <link href="/2020/09/09/%E9%98%B2%E7%81%AB%E5%A2%99%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3%EF%BC%8C%E6%9F%A5%E7%9C%8B%E7%8A%B6%E6%80%81%EF%BC%8C%E6%9F%A5%E7%9C%8B%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3/"/>
    <url>/2020/09/09/%E9%98%B2%E7%81%AB%E5%A2%99%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3%EF%BC%8C%E6%9F%A5%E7%9C%8B%E7%8A%B6%E6%80%81%EF%BC%8C%E6%9F%A5%E7%9C%8B%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3/</url>
    
    <content type="html"><![CDATA[<h3 id="CentOS7-端口的开放关闭查看都是用防火墙来控制的，具体命令如下："><a href="#CentOS7-端口的开放关闭查看都是用防火墙来控制的，具体命令如下：" class="headerlink" title="CentOS7 端口的开放关闭查看都是用防火墙来控制的，具体命令如下："></a>CentOS7 端口的开放关闭查看都是用防火墙来控制的，具体命令如下：</h3><h5 id="查看防火墙状态：（active-running-即是开启状态）"><a href="#查看防火墙状态：（active-running-即是开启状态）" class="headerlink" title="查看防火墙状态：（active (running) 即是开启状态）"></a>查看防火墙状态：（active (running) 即是开启状态）</h5><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><br>[root<span class="hljs-symbol">@WSS</span> bin]<span class="hljs-meta"># systemctl firewalld status</span><br>Unknown operation <span class="hljs-string">'firewalld'</span>.<br></code></pre></td></tr></table></figure><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@WSS bin]<span class="hljs-comment"># systemctl status firewalld</span><br>● firewalld.service - firewalld - dynamic firewall daemon<br>   Loaded: loaded <span class="hljs-params">(/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)</span><br>   Active: active <span class="hljs-params">(running)</span> since 四 2019-07-11 09<span class="hljs-function">:37</span><span class="hljs-function">:11</span> CST; 7h ago<br>     Docs: man<span class="hljs-function">:firewalld</span><span class="hljs-params">(1)</span><br> Main PID: 44370 <span class="hljs-params">(firewalld)</span><br>    Tasks: 2<br>   CGroup: <span class="hljs-string">/system.slice/firewalld.service</span><br>           └─44370 <span class="hljs-string">/usr/bin/python</span> -Es <span class="hljs-string">/usr/sbin/firewalld</span> <span class="hljs-params">--nofork</span> <span class="hljs-params">--nopid</span><br><br>7月 11 09<span class="hljs-function">:37</span><span class="hljs-function">:13</span> WSS firewalld[44370]: WARNING: COMMAND_FAILED: '<span class="hljs-string">/usr/sbin/iptables</span> -w2 -w <span class="hljs-params">--table</span> filter <span class="hljs-params">--delete</span> FORWARD.<span class="hljs-string">..hain</span>?).<br>7月 11 09<span class="hljs-function">:37</span><span class="hljs-function">:13</span> WSS firewalld[44370]: WARNING: COMMAND_FAILED: '<span class="hljs-string">/usr/sbin/iptables</span> -w2 -w <span class="hljs-params">--table</span> filter <span class="hljs-params">--delete</span> FORWARD.<span class="hljs-string">..hain</span>?).<br>7月 11 09<span class="hljs-function">:37</span><span class="hljs-function">:13</span> WSS firewalld[44370]: WARNING: COMMAND_FAILED: '<span class="hljs-string">/usr/sbin/iptables</span> -w2 -w <span class="hljs-params">--table</span> filter <span class="hljs-params">--delete</span> FORWARD.<span class="hljs-string">..hain</span>?).<br>7月 11 09<span class="hljs-function">:37</span><span class="hljs-function">:13</span> WSS firewalld[44370]: WARNING: COMMAND_FAILED: '<span class="hljs-string">/usr/sbin/iptables</span> -w2 -w <span class="hljs-params">--table</span> filter <span class="hljs-params">--delete</span> FORWARD.<span class="hljs-string">..t</span> name.<br>7月 11 09<span class="hljs-function">:37</span><span class="hljs-function">:13</span> WSS firewalld[44370]: WARNING: COMMAND_FAILED: '<span class="hljs-string">/usr/sbin/iptables</span> -w2 -w <span class="hljs-params">--table</span> filter <span class="hljs-params">--delete</span> FORWARD.<span class="hljs-string">..t</span> name.<br>7月 11 09<span class="hljs-function">:37</span><span class="hljs-function">:13</span> WSS firewalld[44370]: WARNING: COMMAND_FAILED: '<span class="hljs-string">/usr/sbin/iptables</span> -w2 -w <span class="hljs-params">--table</span> filter <span class="hljs-params">--delete</span> INPUT -<span class="hljs-string">...hain</span>?).<br>7月 11 09<span class="hljs-function">:37</span><span class="hljs-function">:13</span> WSS firewalld[44370]: WARNING: COMMAND_FAILED: '<span class="hljs-string">/usr/sbin/iptables</span> -w2 -w <span class="hljs-params">--table</span> filter <span class="hljs-params">--delete</span> INPUT -<span class="hljs-string">...hain</span>?).<br>7月 11 09<span class="hljs-function">:37</span><span class="hljs-function">:13</span> WSS firewalld[44370]: WARNING: COMMAND_FAILED: '<span class="hljs-string">/usr/sbin/iptables</span> -w2 -w <span class="hljs-params">--table</span> filter <span class="hljs-params">--delete</span> OUTPUT <span class="hljs-string">...hain</span>?).<br>7月 11 09<span class="hljs-function">:37</span><span class="hljs-function">:13</span> WSS firewalld[44370]: WARNING: COMMAND_FAILED: '<span class="hljs-string">/usr/sbin/iptables</span> -w2 -w <span class="hljs-params">--table</span> filter <span class="hljs-params">--delete</span> INPUT -<span class="hljs-string">...hain</span>?).<br>7月 11 09<span class="hljs-function">:37</span><span class="hljs-function">:13</span> WSS firewalld[44370]: WARNING: COMMAND_FAILED: '<span class="hljs-string">/usr/sbin/iptables</span> -w2 -w <span class="hljs-params">--table</span> filter <span class="hljs-params">--delete</span> INPUT -<span class="hljs-string">...hain</span>?).<br>Hint: Some lines were ellipsized, use -l to show in full.<br></code></pre></td></tr></table></figure><h4 id="查看已开放端口：（8080和3306-即是已开放端口）"><a href="#查看已开放端口：（8080和3306-即是已开放端口）" class="headerlink" title="查看已开放端口：（8080和3306 即是已开放端口）"></a>查看已开放端口：（8080和3306 即是已开放端口）</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><br><span class="hljs-string">[root@WSS</span> <span class="hljs-string">bin]#</span> <span class="hljs-string">firewall-cmd</span> <span class="hljs-bullet">--list-all</span><br><span class="hljs-string">public</span> <span class="hljs-string">(active)</span><br><span class="hljs-attr">  target:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">  icmp-block-inversion:</span> <span class="hljs-literal">no</span><br><span class="hljs-attr">  interfaces:</span> <span class="hljs-string">ens3</span> <span class="hljs-string">ens4</span><br><span class="hljs-attr">  sources:</span> <br><span class="hljs-attr">  services:</span> <span class="hljs-string">ssh</span> <span class="hljs-string">dhcpv6-client</span><br><span class="hljs-attr">  ports:</span> <span class="hljs-number">8080</span><span class="hljs-string">/tcp</span> <span class="hljs-number">3306</span><span class="hljs-string">/tcp</span><br><span class="hljs-attr">  protocols:</span> <br><span class="hljs-attr">  masquerade:</span> <span class="hljs-literal">no</span><br><span class="hljs-attr">  forward-ports:</span> <br><span class="hljs-attr">  source-ports:</span> <br><span class="hljs-attr">  icmp-blocks:</span> <br>  <span class="hljs-string">rich</span> <span class="hljs-attr">rules:</span><br></code></pre></td></tr></table></figure><h4 id="防火墙开放端口：（开放端口后需重载防火墙）"><a href="#防火墙开放端口：（开放端口后需重载防火墙）" class="headerlink" title="防火墙开放端口：（开放端口后需重载防火墙）"></a>防火墙开放端口：（开放端口后需重载防火墙）</h4><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@WSS bin]# firewall-cmd --zone=public --add-port=3306/tcp --permanent<br><span class="hljs-keyword">success<br></span>[root@WSS bin]# <br>[root@WSS bin]# firewall-cmd --reload<br><span class="hljs-keyword">success<br></span>[root@WSS bin]#<br>命令含义：<br>–zone #作用域<br>–add-port=80/tcp #添加端口，格式为：端口/通讯协议<br>–permanent #永久生效，没有此参数重启后失效<br>firewall-cmd --reload 并不中断用户连接，即不丢失状态信息<br></code></pre></td></tr></table></figure><h4 id="firewalld的基本使用"><a href="#firewalld的基本使用" class="headerlink" title="firewalld的基本使用"></a>firewalld的基本使用</h4><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined">启动：<span class="hljs-keyword"> system</span>ctl start firewalld<br>关闭：<span class="hljs-keyword"> system</span>ctl stop firewalld<br>查看状态：<span class="hljs-keyword"> system</span>ctl status firewalld<br>开机禁用 ：<span class="hljs-keyword"> system</span>ctl disable firewalld<br>开机启用 ：<span class="hljs-keyword"> system</span>ctl enable firewalld<br></code></pre></td></tr></table></figure><h4 id="systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。"><a href="#systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。" class="headerlink" title="systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。"></a>systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。</h4><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs undefined">启动一个服务：<span class="hljs-string">systemctl </span><span class="hljs-string">start </span><span class="hljs-string">firewalld.</span><span class="hljs-string">service<br></span>关闭一个服务：<span class="hljs-string">systemctl </span><span class="hljs-string">stop </span><span class="hljs-string">firewalld.</span><span class="hljs-string">service<br></span>重启一个服务：<span class="hljs-string">systemctl </span><span class="hljs-string">restart </span><span class="hljs-string">firewalld.</span><span class="hljs-string">service<br></span>显示一个服务的状态：<span class="hljs-string">systemctl </span><span class="hljs-string">status </span><span class="hljs-string">firewalld.</span><span class="hljs-string">service<br></span>在开机时启用一个服务：<span class="hljs-string">systemctl </span><span class="hljs-string">enable </span><span class="hljs-string">firewalld.</span><span class="hljs-string">service<br></span>在开机时禁用一个服务：<span class="hljs-string">systemctl </span><span class="hljs-string">disable </span><span class="hljs-string">firewalld.</span><span class="hljs-string">service<br></span>查看服务是否开机启动：<span class="hljs-string">systemctl </span><span class="hljs-string">is-enabled </span><span class="hljs-string">firewalld.</span><span class="hljs-string">service<br></span>查看已启动的服务列表：<span class="hljs-string">systemctl </span><span class="hljs-built_in">list-unit-files|grep</span> <span class="hljs-string">enabled<br></span>查看启动失败的服务列表：<span class="hljs-string">systemctl </span><span class="hljs-built_in">--failed</span><br></code></pre></td></tr></table></figure><h4 id="配置firewalld-cmd"><a href="#配置firewalld-cmd" class="headerlink" title="配置firewalld-cmd"></a>配置firewalld-cmd</h4><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">查看版本： firewall-<span class="hljs-keyword">cmd</span><span class="bash"> --version</span><br>查看帮助： firewall-<span class="hljs-keyword">cmd</span><span class="bash"> --<span class="hljs-built_in">help</span></span><br>显示状态： firewall-<span class="hljs-keyword">cmd</span><span class="bash"> --state</span><br>查看所有打开的端口： firewall-<span class="hljs-keyword">cmd</span><span class="bash"> --zone=public --list-ports</span><br>更新防火墙规则： firewall-<span class="hljs-keyword">cmd</span><span class="bash"> --reload</span><br>查看区域信息: firewall-<span class="hljs-keyword">cmd</span><span class="bash"> --get-active-zones</span><br>查看指定接口所属区域： firewall-<span class="hljs-keyword">cmd</span><span class="bash"> --get-zone-of-interface=eth0</span><br>拒绝所有包：firewall-<span class="hljs-keyword">cmd</span><span class="bash"> --panic-on</span><br>取消拒绝状态： firewall-<span class="hljs-keyword">cmd</span><span class="bash"> --panic-off</span><br>查看是否拒绝： firewall-<span class="hljs-keyword">cmd</span><span class="bash"> --query-panic</span><br></code></pre></td></tr></table></figure><h3 id="二、开放或限制端口"><a href="#二、开放或限制端口" class="headerlink" title="二、开放或限制端口"></a>二、开放或限制端口</h3><h4 id="1、开放端口"><a href="#1、开放端口" class="headerlink" title="1、开放端口"></a>1、开放端口</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs undefined">如我们需要开启XShell连接时需要使用的22端口<br>firewall-cmd <span class="hljs-attribute">--zone</span>=public <span class="hljs-attribute">--add-port</span>=22/tcp --permanent<br>其中--permanent的作用是使设置永久生效，不加的话机器重启之后失效<br>重新载入一下防火墙设置，使设置生效<br>firewall-cmd --reload<br>可通过如下命令查看是否生效<br>firewall-cmd <span class="hljs-attribute">--zone</span>=public <span class="hljs-attribute">--query-port</span>=22/tcp<br>如下命令可查看当前系统打开的所有端口<br>firewall-cmd <span class="hljs-attribute">--zone</span>=public --list-ports<br></code></pre></td></tr></table></figure><p><code>`</code><br>2、限制端口<br>（1）比如我们现在需要关掉刚刚打开的22端口</p><p>firewall-cmd –zone=public –remove-port=22/tcp –permanent</p><p>（2）重新载入一下防火墙设置，使设置生效</p><p>firewall-cmd –reload</p><p>（3）再去查看系统所有开放的端口，已经看到没有22端口了</p><p>firewall-cmd –zone=public –list-ports</p><p>3、批量开放或限制端口<br>（1）批量开放端口，如从100到500这之间的端口我们全部要打开</p><p>firewall-cmd –zone=public –add-port=100-500/tcp –permanent</p><p>（2）重新载入一下防火墙设置，使设置生效</p><p>firewall-cmd –reload</p><p>（3）查看系统所有开放的端口，可以看到从100到500的端口已被全部开放</p><p>firewall-cmd –zone=public –list-ports</p><p>（4）同理，批量限制端口为</p><p>firewall-cmd –zone=public –remove-port=100-500/tcp –permanent<br>firewall-cmd –reload<br>三、开放或限制IP<br>1、限制IP地址访问<br>（1）比如限制IP为192.168.0.200的地址禁止访问80端口即禁止访问机器</p><p>firewall-cmd –permanent –add-rich-rule=”rule family=”ipv4” source address=”192.168.0.200” port protocol=”tcp” port=”80” reject”</p><p>（2）重新载入一下防火墙设置，使设置生效</p><p>firewall-cmd –reload<br>（3）查看已经设置的规则</p><p>firewall-cmd –zone=public –list-rich-rules</p><p>2、解除IP地址限制<br>（1）解除刚才被限制的192.168.0.200</p><p>firewall-cmd –permanent –add-rich-rule=”rule family=”ipv4” source address=”192.168.0.200” port protocol=”tcp” port=”80” accept”</p><p>（2）重新载入一下防火墙设置，使设置生效</p><p>firewall-cmd –reload<br>（3）再查看规则设置发现已经没有192.168.0.200的限制了</p><p>firewall-cmd –zone=public –list-rich-rules</p><p>如设置未生效，可尝试直接编辑规则文件，删掉原来的设置规则，重新载入一下防火墙即可</p><p>vi /etc/firewalld/zones/public.xml</p><p>3、限制IP地址段<br>（1）如我们需要限制10.0.0.0-10.0.0.255这一整个段的IP，禁止他们访问</p><p>firewall-cmd –permanent –add-rich-rule=”rule family=”ipv4” source address=”10.0.0.0/24” port protocol=”tcp” port=”80” reject”<br>其中10.0.0.0/24表示为从10.0.0.0这个IP开始，24代表子网掩码为255.255.255.0，共包含256个地址，即从0-255共256个IP，即正好限制了这一整段的IP地址，具体的设置规则可参考下表</p><p>（2）重新载入一下防火墙设置，使设置生效</p><p>firewall-cmd –reload<br>（3）查看规则，确认是否生效</p><p>firewall-cmd –zone=public –list-rich-rules<br>（4）同理，打开限制为</p><p>firewall-cmd –permanent –add-rich-rule=”rule family=”ipv4” source address=”10.0.0.0/24” port protocol=”tcp” port=”80” accept”<br>firewall-cmd –reload</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>构建一个html转pdf的docker镜像</title>
    <link href="/2020/06/01/%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AAhtml%E8%BD%ACpdf%E7%9A%84docker%E9%95%9C%E5%83%8F/"/>
    <url>/2020/06/01/%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AAhtml%E8%BD%ACpdf%E7%9A%84docker%E9%95%9C%E5%83%8F/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Dockerfile最佳实践</title>
    <link href="/2020/04/21/Dockerfile%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
    <url>/2020/04/21/Dockerfile%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
    
    <content type="html"><![CDATA[<p>###Dockerfile入门之后面临一个问题：如何在实际的开发过程中正确配置 Dockerfile？<br>Dockerfile 有两个方向上的使用方式：<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-bullet">1. </span>只用 Dockerfile 管理镜像<br><span class="hljs-bullet">2. </span>使用docker-compose 容器编排技术 共同管理镜像的 build。<br></code></pre></td></tr></table></figure></p><h3 id="下面是我对Dockerfile最佳实践的总结："><a href="#下面是我对Dockerfile最佳实践的总结：" class="headerlink" title="下面是我对Dockerfile最佳实践的总结："></a>下面是我对Dockerfile最佳实践的总结：</h3><p>共同遵守的原则:<br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined">保持常见的指令像 <span class="hljs-keyword">MAINTAINER</span> 以及从上至下更新 Dockerfile 命令。<br>当构建镜像时使用可理解的标签，以便更好地管理镜像。<br>避免在 Dockerfile 中映射公有端口。<br><span class="hljs-keyword">CMD</span><span class="bash"> 与 ENTRYPOINT 命令请使用数组语法。</span><br>针对一：只用 Dockerfile 管理镜像的 build<br></code></pre></td></tr></table></figure></p><h3 id="一个简单的镜像示例"><a href="#一个简单的镜像示例" class="headerlink" title="一个简单的镜像示例"></a>一个简单的镜像示例</h3><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-meta"># 依赖最小Linux环境 alpine 只用5M大小</span><br>FROM alpine<br><span class="hljs-meta"># MAINTAINER：设置该镜像的作者。</span><br>MAINTAINER jinlong<br><span class="hljs-meta"># 将当前目录下的所有文件都拷贝进入 image 文件的/app目录</span><br>COPY ./bin/ /app<br><span class="hljs-meta"># 指定接下来的工作路径为/app</span><br><span class="hljs-meta"># WORKDIR：指定RUN、CMD与ENTRYPOINT命令的工作目录。</span><br>WORKDIR /app<br><span class="hljs-meta"># RUN：在shell或者exec的环境下执行的命令。RUN指令会在新创建的镜像上添加新的层面，接下来提交的结果用在Dockerfile的下一条指令中。</span><br><span class="hljs-meta">#RUN cd bin/</span><br><span class="hljs-meta"># ADD：复制文件指令。它有两个参数&lt;source&gt;和&lt;destination&gt;。destination是容器内的路径。source可以是URL或者是启动配置上下文中的一个文件。</span><br><span class="hljs-meta"># ADD 《src》 《destination》</span><br><span class="hljs-meta"># CMD：提供了容器默认的执行命令。 Dockerfile只允许使用一次CMD指令。 使用多个CMD会抵消之前所有的指令，只有最后一个指令生效。 CMD有三种形式：</span><br><span class="hljs-meta">#CMD ["executable","param1","param2"]</span><br><span class="hljs-meta">#CMD ["param1","param2"]</span><br><span class="hljs-meta">#CMD command param1 param2</span><br><span class="hljs-meta">#CMD ./gin</span><br>CMD [<span class="hljs-string">"./gin"</span>]<br><span class="hljs-meta">#-------------</span><br><span class="hljs-meta"># 开放端口 永远都不要在这里使用端口映射，这违反了可移植性</span><br><span class="hljs-meta"># EXPOSE：指定容器在运行时监听的端口。</span><br><span class="hljs-meta"># private and public mapping</span><br><span class="hljs-meta"># EXPOSE 80:8080</span><br><span class="hljs-meta"># ENTRYPOINT：配置给容器一个可执行的命令</span><br><span class="hljs-meta"># 这意味着在每次使用镜像创建容器时一个特定的应用程序可以被设置为默认程序。同时也意味着该镜像每次被调用时仅能运行指定的应用。类似于CMD</span><br><span class="hljs-meta"># ENTRYPOINT ["executable", "param1","param2"]</span><br><span class="hljs-meta"># ENV：设置环境变量。它们使用键值对，增加运行程序的灵活性。</span><br><span class="hljs-meta"># ENV &lt;key&gt; &lt;value&gt;</span><br><span class="hljs-meta"># USER：镜像正在运行时设置一个UID。语法如下</span><br><span class="hljs-meta"># USER &lt;uid&gt;</span><br><span class="hljs-meta"># VOLUME：授权访问从容器内到主机上的目录。</span><br><span class="hljs-meta"># VOLUME ["/data"]</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
      <tag>Docker</tag>
      
      <tag>Centos7</tag>
      
      <tag>image</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>centos8</title>
    <link href="/2020/04/04/centos8/"/>
    <url>/2020/04/04/centos8/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>使用kubeadm升级kubernetes集群</title>
    <link href="/2020/03/27/%E4%BD%BF%E7%94%A8kubeadm%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/03/27/%E4%BD%BF%E7%94%A8kubeadm%E5%8D%87%E7%BA%A7kubernetes%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h3 id="在你开始之前"><a href="#在你开始之前" class="headerlink" title="在你开始之前"></a>在你开始之前</h3><blockquote><ul><li>您需要具有运行版本1.17.0或更高版本的kubeadm Kubernetes集群。</li><li>必须禁用掉期交易。</li><li>集群应使用静态控制平面和etcd pod或外部etcd。</li><li>确保您仔细阅读发行说明。</li><li>确保备份所有重要组件，例如存储在数据库中的应用程序级别状态。 kubeadm upgrade不会影响您的工作负载，仅涉及Kubernetes内部的组件，但是备份始终是最佳实践。</li><li>附加信息升级后，所有容器都会重新启动，因为容器规范哈希值已更改。您只能从一个MINOR版本升级到下一个MINOR版本，或者在同一MINOR的PATCH版本之间升级。也就是说，升级时不能跳过MINOR版本。例如，您可以从1.y升级到1.y + 1，但不能从1.y升级到1.y + 2。</li></ul></blockquote><h4 id="确定要升级到的版本"><a href="#确定要升级到的版本" class="headerlink" title="确定要升级到的版本"></a>确定要升级到的版本</h4><blockquote><p>查找最新的稳定的1.17版本：Ubuntu，Debian或HypriotOSCentOS，RHEL或Fedora<br>apt update<br>apt-cache madison kubeadm</p></blockquote><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs undefined">升级控制平面节点<br>升级第一个控制平面节点<br>在您的第一个控制平面节点上，升级kubeadm：<br>RHEL或Fedora<br># replace x in <span class="hljs-number">1.18</span>.x<span class="hljs-number">-0</span> with the latest patch version<br>yum install -y kubeadm<span class="hljs-number">-1.18</span>.x<span class="hljs-number">-0</span> --disableexcludes=kubernetes<br></code></pre></td></tr></table></figure><blockquote><p>确认下载正常并且具有预期的版本：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angular2html">kubeadm version<br></code></pre></td></tr></table></figure></p></blockquote><p>kubeadm version</p><blockquote><p>排空控制平面节点：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs angular2html"><br><br># replace &lt;cp-node-name&gt; with the name of your control plane node<br>kubectl drain &lt;cp-node-name&gt; --ignore-daemonsets<br></code></pre></td></tr></table></figure></p></blockquote><blockquote><p>在控制平面节点上，运行：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">sudo kubeadm<span class="hljs-built_in"> upgrade </span>plan<br></code></pre></td></tr></table></figure></p></blockquote><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs undefined">您应该看到类似于以下的输出：<br>[upgrade/config] Making sure the configuration is correct:<br>[upgrade/config] Reading configuration from the cluster...<br>[upgrade/config] FYI: You can look <span class="hljs-meta">at</span> this config file with <span class="hljs-string">'kubectl -n kube-system get cm kubeadm-config -oyaml'</span><br>[preflight] Running pre-flight checks.<br>[upgrade] Running cluster health checks<br>[upgrade] Fetching available versions to upgrade to<br>[upgrade/versions] Cluster version: v1<span class="hljs-meta">.17</span><span class="hljs-meta">.3</span><br>[upgrade/versions] kubeadm version: v1<span class="hljs-meta">.18</span><span class="hljs-meta">.0</span><br>[upgrade/versions] Latest stable version: v1<span class="hljs-meta">.18</span><span class="hljs-meta">.0</span><br>[upgrade/versions] Latest version <span class="hljs-keyword">in</span> the v1<span class="hljs-meta">.17</span> series: v1<span class="hljs-meta">.18</span><span class="hljs-meta">.0</span><br><br>Components that must be upgraded manually after you have upgraded the control plane with <span class="hljs-string">'kubeadm upgrade apply'</span>:<br>COMPONENT   CURRENT             AVAILABLE<br>Kubelet     <span class="hljs-number">1</span> x v1<span class="hljs-meta">.17</span><span class="hljs-meta">.3</span>   v1<span class="hljs-meta">.18</span><span class="hljs-meta">.0</span><br><br>Upgrade to the latest version <span class="hljs-keyword">in</span> the v1<span class="hljs-meta">.17</span> series:<br><br>COMPONENT            CURRENT   AVAILABLE<br>API Server           v1<span class="hljs-meta">.17</span><span class="hljs-meta">.3</span>   v1<span class="hljs-meta">.18</span><span class="hljs-meta">.0</span><br>Controller Manager   v1<span class="hljs-meta">.17</span><span class="hljs-meta">.3</span>   v1<span class="hljs-meta">.18</span><span class="hljs-meta">.0</span><br>Scheduler            v1<span class="hljs-meta">.17</span><span class="hljs-meta">.3</span>   v1<span class="hljs-meta">.18</span><span class="hljs-meta">.0</span><br>Kube Proxy           v1<span class="hljs-meta">.17</span><span class="hljs-meta">.3</span>   v1<span class="hljs-meta">.18</span><span class="hljs-meta">.0</span><br>CoreDNS              <span class="hljs-number">1.6</span><span class="hljs-meta">.5</span>     <span class="hljs-number">1.6</span><span class="hljs-meta">.7</span><br>Etcd                 <span class="hljs-number">3.4</span><span class="hljs-meta">.3</span>     <span class="hljs-number">3.4</span><span class="hljs-meta">.3</span>-<span class="hljs-number">0</span><br><br>You can now apply the upgrade by executing the following command:<br><br>    kubeadm upgrade apply v1<span class="hljs-meta">.18</span><span class="hljs-meta">.0</span><br><br>_____________________________________________________________________<br>此命令检查集群是否可以升级，并获取可以升级到的版本。<br></code></pre></td></tr></table></figure><blockquote><p>注意： kubeadm upgrade还会自动续订它在此节点上管理的证书。要选择退出证书更新，–certificate-renewal=false可以使用该标志。有关更多信息，请参阅证书管理指南。<br>选择要升级到的版本，然后运行相应的命令。例如：</p></blockquote><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><code class="hljs undefined"># replace x <span class="hljs-keyword">with</span> the patch version you picked <span class="hljs-keyword">for</span> this upgrade<br>sudo kubeadm upgrade apply v1.<span class="hljs-number">18</span>.x<br>您应该看到类似于以下的输出：<br><br>[upgrade/config] Making sure the <span class="hljs-keyword">configuration</span> <span class="hljs-keyword">is</span> correct:<br>[upgrade/config] Reading <span class="hljs-keyword">configuration</span> from the cluster...<br>[upgrade/config] FYI: You can look at this config <span class="hljs-keyword">file</span> <span class="hljs-keyword">with</span> <span class="hljs-symbol">'kubectl</span> -n kube-system get cm kubeadm-config -oyaml'<br>[preflight] Running pre-flight checks.<br>[upgrade] Running cluster health checks<br>[upgrade/version] You have chosen <span class="hljs-keyword">to</span> change the cluster version <span class="hljs-keyword">to</span> <span class="hljs-string">"v1.18.0"</span><br>[upgrade/versions] Cluster version: v1.<span class="hljs-number">17.3</span><br>[upgrade/versions] kubeadm version: v1.<span class="hljs-number">18.0</span><br>[upgrade/confirm] Are you sure you want <span class="hljs-keyword">to</span> proceed <span class="hljs-keyword">with</span> the upgrade? [y/N]: y<br>[upgrade/prepull] Will prepull images <span class="hljs-keyword">for</span> components [kube-apiserver kube-controller-manager kube-scheduler etcd]<br>[upgrade/prepull] Prepulling image <span class="hljs-keyword">for</span> <span class="hljs-keyword">component</span> etcd.<br>[upgrade/prepull] Prepulling image <span class="hljs-keyword">for</span> <span class="hljs-keyword">component</span> kube-apiserver.<br>[upgrade/prepull] Prepulling image <span class="hljs-keyword">for</span> <span class="hljs-keyword">component</span> kube-controller-manager.<br>[upgrade/prepull] Prepulling image <span class="hljs-keyword">for</span> <span class="hljs-keyword">component</span> kube-scheduler.<br>[apiclient] Found <span class="hljs-number">1</span> Pods <span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span> selector k8s-app=upgrade-prepull-kube-controller-manager<br>[apiclient] Found <span class="hljs-number">0</span> Pods <span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span> selector k8s-app=upgrade-prepull-etcd<br>[apiclient] Found <span class="hljs-number">0</span> Pods <span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span> selector k8s-app=upgrade-prepull-kube-scheduler<br>[apiclient] Found <span class="hljs-number">1</span> Pods <span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span> selector k8s-app=upgrade-prepull-kube-apiserver<br>[apiclient] Found <span class="hljs-number">1</span> Pods <span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span> selector k8s-app=upgrade-prepull-etcd<br>[apiclient] Found <span class="hljs-number">1</span> Pods <span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span> selector k8s-app=upgrade-prepull-kube-scheduler<br>[upgrade/prepull] Prepulled image <span class="hljs-keyword">for</span> <span class="hljs-keyword">component</span> etcd.<br>[upgrade/prepull] Prepulled image <span class="hljs-keyword">for</span> <span class="hljs-keyword">component</span> kube-apiserver.<br>[upgrade/prepull] Prepulled image <span class="hljs-keyword">for</span> <span class="hljs-keyword">component</span> kube-controller-manager.<br>[upgrade/prepull] Prepulled image <span class="hljs-keyword">for</span> <span class="hljs-keyword">component</span> kube-scheduler.<br>[upgrade/prepull] Successfully prepulled the images <span class="hljs-keyword">for</span> <span class="hljs-keyword">all</span> the control plane components<br>[upgrade/apply] Upgrading your Static Pod-hosted control plane <span class="hljs-keyword">to</span> version <span class="hljs-string">"v1.18.0"</span>...<br>Static pod: kube-apiserver-myhost hash: <span class="hljs-number">2</span>cc222e1a577b40a8c2832320db54b46<br>Static pod: kube-controller-manager-myhost hash: f7ce4bc35cb6e646161578ac69910f18<br>Static pod: kube-scheduler-myhost hash: e3025acd90e7465e66fa19c71b916366<br>[upgrade/etcd] Upgrading <span class="hljs-keyword">to</span> TLS <span class="hljs-keyword">for</span> etcd<br>[upgrade/etcd] Non fatal issue encountered during upgrade: the desired etcd version <span class="hljs-keyword">for</span> this Kubernetes version <span class="hljs-string">"v1.18.0"</span> <span class="hljs-keyword">is</span> <span class="hljs-string">"3.4.3-0"</span>, but the current etcd version <span class="hljs-keyword">is</span> <span class="hljs-string">"3.4.3"</span>. Won<span class="hljs-symbol">'t</span> downgrade etcd, instead just continue<br>[upgrade/staticpods] Writing <span class="hljs-keyword">new</span> Static Pod manifests <span class="hljs-keyword">to</span> <span class="hljs-string">"/etc/kubernetes/tmp/kubeadm-upgraded-manifests308527012"</span><br>W0308 <span class="hljs-number">18</span>:<span class="hljs-number">48</span>:<span class="hljs-number">14.535122</span>    <span class="hljs-number">3082</span> manifests.go:<span class="hljs-number">225</span>] the <span class="hljs-keyword">default</span> kube-apiserver authorization-mode <span class="hljs-keyword">is</span> <span class="hljs-string">"Node,RBAC"</span>; using <span class="hljs-string">"Node,RBAC"</span><br>[upgrade/staticpods] Preparing <span class="hljs-keyword">for</span> <span class="hljs-string">"kube-apiserver"</span> upgrade<br>[upgrade/staticpods] Renewing apiserver certificate<br>[upgrade/staticpods] Renewing apiserver-kubelet-client certificate<br>[upgrade/staticpods] Renewing front-proxy-client certificate<br>[upgrade/staticpods] Renewing apiserver-etcd-client certificate<br>[upgrade/staticpods] Moved <span class="hljs-keyword">new</span> manifest <span class="hljs-keyword">to</span> <span class="hljs-string">"/etc/kubernetes/manifests/kube-apiserver.yaml"</span> <span class="hljs-keyword">and</span> backed up old manifest <span class="hljs-keyword">to</span> <span class="hljs-string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-03-08-18-48-14/kube-apiserver.yaml"</span><br>[upgrade/staticpods] Waiting <span class="hljs-keyword">for</span> the kubelet <span class="hljs-keyword">to</span> restart the <span class="hljs-keyword">component</span><br>[upgrade/staticpods] This might take a minute <span class="hljs-keyword">or</span> longer depending <span class="hljs-keyword">on</span> the <span class="hljs-keyword">component</span>/version gap (timeout <span class="hljs-number">5</span>m0s)<br>Static pod: kube-apiserver-myhost hash: <span class="hljs-number">2</span>cc222e1a577b40a8c2832320db54b46<br>Static pod: kube-apiserver-myhost hash: <span class="hljs-number">609429</span>acb0d71dce6725836dd97d8bf4<br>[apiclient] Found <span class="hljs-number">1</span> Pods <span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span> selector <span class="hljs-keyword">component</span>=kube-apiserver<br>[upgrade/staticpods] <span class="hljs-keyword">Component</span> <span class="hljs-string">"kube-apiserver"</span> upgraded successfully!<br>[upgrade/staticpods] Preparing <span class="hljs-keyword">for</span> <span class="hljs-string">"kube-controller-manager"</span> upgrade<br>[upgrade/staticpods] Renewing controller-manager.conf certificate<br>[upgrade/staticpods] Moved <span class="hljs-keyword">new</span> manifest <span class="hljs-keyword">to</span> <span class="hljs-string">"/etc/kubernetes/manifests/kube-controller-manager.yaml"</span> <span class="hljs-keyword">and</span> backed up old manifest <span class="hljs-keyword">to</span> <span class="hljs-string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-03-08-18-48-14/kube-controller-manager.yaml"</span><br>[upgrade/staticpods] Waiting <span class="hljs-keyword">for</span> the kubelet <span class="hljs-keyword">to</span> restart the <span class="hljs-keyword">component</span><br>[upgrade/staticpods] This might take a minute <span class="hljs-keyword">or</span> longer depending <span class="hljs-keyword">on</span> the <span class="hljs-keyword">component</span>/version gap (timeout <span class="hljs-number">5</span>m0s)<br>Static pod: kube-controller-manager-myhost hash: f7ce4bc35cb6e646161578ac69910f18<br>Static pod: kube-controller-manager-myhost hash: c7a1232ba2c5dc15641c392662fe5156<br>[apiclient] Found <span class="hljs-number">1</span> Pods <span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span> selector <span class="hljs-keyword">component</span>=kube-controller-manager<br>[upgrade/staticpods] <span class="hljs-keyword">Component</span> <span class="hljs-string">"kube-controller-manager"</span> upgraded successfully!<br>[upgrade/staticpods] Preparing <span class="hljs-keyword">for</span> <span class="hljs-string">"kube-scheduler"</span> upgrade<br>[upgrade/staticpods] Renewing scheduler.conf certificate<br>[upgrade/staticpods] Moved <span class="hljs-keyword">new</span> manifest <span class="hljs-keyword">to</span> <span class="hljs-string">"/etc/kubernetes/manifests/kube-scheduler.yaml"</span> <span class="hljs-keyword">and</span> backed up old manifest <span class="hljs-keyword">to</span> <span class="hljs-string">"/etc/kubernetes/tmp/kubeadm-backup-manifests-2020-03-08-18-48-14/kube-scheduler.yaml"</span><br>[upgrade/staticpods] Waiting <span class="hljs-keyword">for</span> the kubelet <span class="hljs-keyword">to</span> restart the <span class="hljs-keyword">component</span><br>[upgrade/staticpods] This might take a minute <span class="hljs-keyword">or</span> longer depending <span class="hljs-keyword">on</span> the <span class="hljs-keyword">component</span>/version gap (timeout <span class="hljs-number">5</span>m0s)<br>Static pod: kube-scheduler-myhost hash: e3025acd90e7465e66fa19c71b916366<br>Static pod: kube-scheduler-myhost hash: b1b721486ae0ac504c160dcdc457ab0d<br>[apiclient] Found <span class="hljs-number">1</span> Pods <span class="hljs-keyword">for</span> <span class="hljs-keyword">label</span> selector <span class="hljs-keyword">component</span>=kube-scheduler<br>[upgrade/staticpods] <span class="hljs-keyword">Component</span> <span class="hljs-string">"kube-scheduler"</span> upgraded successfully!<br>[upload-config] Storing the <span class="hljs-keyword">configuration</span> used <span class="hljs-keyword">in</span> ConfigMap <span class="hljs-string">"kubeadm-config"</span> <span class="hljs-keyword">in</span> the <span class="hljs-string">"kube-system"</span> Namespace<br>[kubelet] Creating a ConfigMap <span class="hljs-string">"kubelet-config-1.18"</span> <span class="hljs-keyword">in</span> namespace kube-system <span class="hljs-keyword">with</span> the <span class="hljs-keyword">configuration</span> <span class="hljs-keyword">for</span> the kubelets <span class="hljs-keyword">in</span> the cluster<br>[kubelet-start] Downloading <span class="hljs-keyword">configuration</span> <span class="hljs-keyword">for</span> the kubelet from the <span class="hljs-string">"kubelet-config-1.18"</span> ConfigMap <span class="hljs-keyword">in</span> the kube-system namespace<br>[kubelet-start] Writing kubelet <span class="hljs-keyword">configuration</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">file</span> <span class="hljs-string">"/var/lib/kubelet/config.yaml"</span><br>[bootstrap-token] configured RBAC rules <span class="hljs-keyword">to</span> allow Node Bootstrap tokens <span class="hljs-keyword">to</span> post CSRs <span class="hljs-keyword">in</span> order <span class="hljs-keyword">for</span> nodes <span class="hljs-keyword">to</span> get long term certificate credentials<br>[bootstrap-token] configured RBAC rules <span class="hljs-keyword">to</span> allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token<br>[bootstrap-token] configured RBAC rules <span class="hljs-keyword">to</span> allow certificate rotation <span class="hljs-keyword">for</span> <span class="hljs-keyword">all</span> node client certificates <span class="hljs-keyword">in</span> the cluster<br>[addons] Applied essential addon: CoreDNS<br>[addons] Applied essential addon: kube-proxy<br><br>[upgrade/successful] SUCCESS! Your cluster was upgraded <span class="hljs-keyword">to</span> <span class="hljs-string">"v1.18.0"</span>. Enjoy!<br><br>[upgrade/kubelet] Now that your control plane <span class="hljs-keyword">is</span> upgraded, please proceed <span class="hljs-keyword">with</span> upgrading your kubelets <span class="hljs-keyword">if</span> you haven<span class="hljs-symbol">'t</span> already done so.<br>手动升级您的CNI提供程序插件。<br></code></pre></td></tr></table></figure><blockquote><p>您的容器网络接口（CNI）提供程序可能有其自己的升级说明。检查插件页面以找到您的CNI提供程序，并查看是否需要其他升级步骤。</p></blockquote><blockquote><p>取消控制平面节点的密码：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment"># replace &lt;cp-node-name&gt; with the name of your control plane node</span><br>kubectl uncordon &lt;cp-node-name&gt;<br>升级其他控制平面节点<br>与第一个控制平面节点相同，但使用：<br><br>sudo kubeadm<span class="hljs-built_in"> upgrade </span>node<br>代替：<br><br>sudo kubeadm<span class="hljs-built_in"> upgrade </span>apply<br>也sudo kubeadm<span class="hljs-built_in"> upgrade </span>plan没有必要。<br></code></pre></td></tr></table></figure></p></blockquote><blockquote><p>升级kubelet和kubectl<br>在所有控制平面节点上升级kubelet和kubectl：<br>Ubuntu，Debian或HypriotOSCentOS，RHEL或Fedora</p></blockquote><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs undefined"># replace x in <span class="hljs-number">1.18</span>.x<span class="hljs-number">-0</span> with the latest patch version<br>yum install -y kubelet<span class="hljs-number">-1.18</span>.x<span class="hljs-number">-0</span> kubectl<span class="hljs-number">-1.18</span>.x<span class="hljs-number">-0</span> --disableexcludes=kubernetes<br>重新启动kubelet<br>sudo systemctl restart kubelet<br></code></pre></td></tr></table></figure><blockquote><p>升级工作程序节点<br>在不牺牲运行工作负载所需的最小容量的前提下，应在一个工作节点上一次执行一个升级程序，或者一次执行几个节点。</p></blockquote><blockquote><p>升级kubeadm<br>在所有工作节点上升级kubeadm：<br>Ubuntu，Debian或HypriotOSCentOS，RHEL或Fedora<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined"># replace x in <span class="hljs-number">1.18</span>.x<span class="hljs-number">-0</span> with the latest patch version<br>yum install -y kubeadm<span class="hljs-number">-1.18</span>.x<span class="hljs-number">-0</span> --disableexcludes=kubernetes<br></code></pre></td></tr></table></figure></p></blockquote><blockquote><p>排空节点<br>通过将节点标记为不可计划并逐出工作负载来准备要维护的节点：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment"># replace &lt;node-to-drain&gt; with the name of your node you are draining</span><br>kubectl drain <span class="hljs-tag">&lt;node-to-drain&gt;</span> --ignore-daemonsets<br>您应该看到类似于以下的输出：<br><br><span class="hljs-keyword">node</span><span class="hljs-title">/ip-172-31-85-18</span> cordoned<br>WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-proxy-dj7d7, kube-system/weave-net-z65qx<br><span class="hljs-keyword">node</span><span class="hljs-title">/ip-172-31-85-18</span> drained<br></code></pre></td></tr></table></figure></p></blockquote><blockquote><p>升级kubelet配置<br>调用以下命令：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">sudo kubeadm<span class="hljs-built_in"> upgrade </span>node<br></code></pre></td></tr></table></figure></p></blockquote><blockquote><p>升级kubelet和kubectl<br>在所有工作节点上升级kubelet和kubectl：<br>Ubuntu，Debian或HypriotOSCentOS，RHEL或Fedora<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined"># replace x in <span class="hljs-number">1.18</span>.x<span class="hljs-number">-0</span> with the latest patch version<br>yum install -y kubelet<span class="hljs-number">-1.18</span>.x<span class="hljs-number">-0</span> kubectl<span class="hljs-number">-1.18</span>.x<span class="hljs-number">-0</span> --disableexcludes=kubernetes<br></code></pre></td></tr></table></figure></p></blockquote><blockquote><p>重新启动kubelet<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">sudo systemctl restart kubelet</span><br></code></pre></td></tr></table></figure></p></blockquote><blockquote><p>取消密码节点<br>通过将节点标记为可调度来使其重新联机：<br><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined"># replace &lt;node-<span class="hljs-keyword">to</span>-drain&gt; <span class="hljs-keyword">with</span> the <span class="hljs-keyword">name</span> <span class="hljs-keyword">of</span> your node<br>kubectl uncordon &lt;node-<span class="hljs-keyword">to</span>-drain&gt;<br></code></pre></td></tr></table></figure></p></blockquote><h4 id="验证集群的状态"><a href="#验证集群的状态" class="headerlink" title="验证集群的状态"></a>验证集群的状态</h4><blockquote><p>在所有节点上升级kubelet之后，通过在kubectl可以访问群集的任何位置运行以下命令来验证所有节点是否再次可用：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-builtin-name">get</span> nodes<br>该STATUS列应显示Ready所有节点，并且应更新版本号。<br></code></pre></td></tr></table></figure></p></blockquote><blockquote><p>从故障状态中恢复<br>如果kubeadm upgrade失败并且不回滚（例如由于执行期间意外关闭），则可以kubeadm upgrade再次运行。此命令是幂等的，并最终确保实际状态是您声明的所需状态。<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs undefined">要从不良状态中恢复，您也可以在kubeadm upgrade apply --force不更改集群运行版本的情况下运行。<br><br>在升级期间，kubeadm在以下位置写入以下备份文件夹/etc/kubernetes/tmp：kubeadm-backup-etcd-&lt;date&gt;-&lt;time&gt; --kubeadm-backup-manifests-&lt;date&gt;-&lt;time&gt;<br><br>kubeadm-backup-etcd包含此控制平面节点的本地etcd成员数据的备份。如果etcd升级失败并且自动回滚不起作用，则可以在中手动还原此文件夹的内容/var/<span class="hljs-class"><span class="hljs-keyword">lib</span>/<span class="hljs-title">etcd</span>。如果使用外部<span class="hljs-title">etcd</span>，则此备份文件夹将为空。</span><br><br>kubeadm-backup-manifests包含此控制平面节点的静态Pod清单文件的备份。如果升级失败并且自动回滚不起作用，则可以在中手动还原此文件夹的内容/etc/kubernetes/manifests。如果某个组件的升级前清单文件和升级后清单文件之间没有区别，则不会写入该文件的备份文件。<br></code></pre></td></tr></table></figure></p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>-kubeadm -kubernetes -golang</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>贷款利率那些事</title>
    <link href="/2020/03/15/%E8%B4%B7%E6%AC%BE%E5%88%A9%E7%8E%87%E9%82%A3%E4%BA%9B%E4%BA%8B/"/>
    <url>/2020/03/15/%E8%B4%B7%E6%AC%BE%E5%88%A9%E7%8E%87%E9%82%A3%E4%BA%9B%E4%BA%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="分期真实利率计算器"><a href="#分期真实利率计算器" class="headerlink" title="分期真实利率计算器"></a>分期真实利率计算器</h2><hr><blockquote><p>很多同学不会算网络贷款和信用卡的分期真实利率的，这里提供个方式去帮助你计算网络贷款和信用卡分期情况下的真实利率。</p></blockquote><h3 id="例子说明"><a href="#例子说明" class="headerlink" title="例子说明"></a>例子说明</h3><blockquote><p>以借 20000 元，分 12 期，每个月还款 2267 元 ，问年化利率是多少？</p></blockquote><h3 id="GO-分期利息计算法（不限分期数目）："><a href="#GO-分期利息计算法（不限分期数目）：" class="headerlink" title="GO 分期利息计算法（不限分期数目）："></a>GO 分期利息计算法（不限分期数目）：</h3><h4 id="计算源码"><a href="#计算源码" class="headerlink" title="计算源码"></a>计算源码</h4><blockquote></blockquote><p> <a href="https://github.com/willie-lin/RealInterestRateCalculator" target="_blank" rel="noopener">GO 分期利息计算法</a></p><blockquote></blockquote><p><a href="https://github.com/willie-lin/RealInterestRateCalculator/pkg" target="_blank" rel="noopener">编译后的二进制代码文件</a></p><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><blockquote><p>[2 万元分 12 期，月还 2266.67，计算链接]</p></blockquote><h5 id="计算结果"><a href="#计算结果" class="headerlink" title="计算结果"></a>计算结果</h5><blockquote></blockquote><p><a href="https://raw.githubusercontent.com/willie-lin/Hexo-Blog-Picture/master/money/271.png" target="_blank" rel="noopener">网络贷款和信用卡 12 期分期利息计算结果</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>-贷款 -利息 -真实利率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>推荐好用的VPS</title>
    <link href="/2019/09/18/%E6%8E%A8%E8%8D%90%E5%A5%BD%E7%94%A8%E7%9A%84VPS/"/>
    <url>/2019/09/18/%E6%8E%A8%E8%8D%90%E5%A5%BD%E7%94%A8%E7%9A%84VPS/</url>
    
    <content type="html"><![CDATA[<h3 id="直接上链接"><a href="#直接上链接" class="headerlink" title="直接上链接"></a>直接上链接</h3><h4 id="大家都懂得，需要的自己点击下单。"><a href="#大家都懂得，需要的自己点击下单。" class="headerlink" title="大家都懂得，需要的自己点击下单。"></a>大家都懂得，需要的自己点击下单。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angular2">https://www.virtono.com/aff.php?aff=258<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>vps ssr</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用kubeadm部署高可用kubernetes集群</title>
    <link href="/2019/07/17/%E4%BD%BF%E7%94%A8kubeadm%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8kubernetes%E9%9B%86%E7%BE%A4/"/>
    <url>/2019/07/17/%E4%BD%BF%E7%94%A8kubeadm%E9%83%A8%E7%BD%B2%E9%AB%98%E5%8F%AF%E7%94%A8kubernetes%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h1 id="kubernetes-1-15-0"><a href="#kubernetes-1-15-0" class="headerlink" title="kubernetes 1.15.0"></a>kubernetes 1.15.0</h1><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">本文基于 kubeadm 方式部署，kubeadm 在<span class="hljs-number">1.13</span> 版本以后正式进入 GA.<br>目前国内各大厂商都有 kubeadm 的镜像源，对于部署 kubernetes 来说是大大的便利.<br>从官方对 kubeadm 的更新频繁度来看，kubeadm 应该是后面的趋势，毕竟二进制部署确实麻烦了点.<br></code></pre></td></tr></table></figure><h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">本次部署使用非独立的<span class="hljs-selector-tag">etcd</span>集群, 与<span class="hljs-selector-tag">master</span>节点混布. <br>部署环境为<span class="hljs-selector-tag">CentOS</span> 7<span class="hljs-selector-class">.6</span>, 默认内核版本为3<span class="hljs-selector-class">.10</span>, 缺少<span class="hljs-selector-tag">ip_vs_fo</span><span class="hljs-selector-class">.ko</span>模块, 将使<span class="hljs-selector-tag">kube-proxy</span>无法启用<span class="hljs-selector-tag">ipvs</span>功能,<br>目前最新稳定版内核为5<span class="hljs-selector-class">.0</span>, 同时<span class="hljs-selector-tag">Kubernetes</span> 1<span class="hljs-selector-class">.15</span>也支持, 故此处会讲内核升级到最新稳定版部署架构如下, 三个<span class="hljs-selector-tag">master</span>节点, 三个<span class="hljs-selector-tag">worker</span>节点<br></code></pre></td></tr></table></figure><p> <img src="https://github.com/willie-lin/Hexo-Blog-Picture/blob/master/kubernetes/kubeadm-ha-topology-stacked-etcd.svg" srcset="/img/loading.gif" lazyload alt="kubeadm"></p><h3 id="1-1-环境说明"><a href="#1-1-环境说明" class="headerlink" title="1.1 环境说明"></a>1.1 环境说明</h3><table><thead><tr><th>HOST</th><th>IP</th><th>Hostname</th><th>Hardware env</th><th>Role</th></tr></thead><tbody><tr><td>Master1</td><td>10.122.144.153</td><td>k8s-master1</td><td>4Core 4G</td><td>master</td></tr><tr><td>Master2</td><td>10.122.144.154</td><td>k8s-master2</td><td>4Core 4G</td><td>master</td></tr><tr><td>Master3</td><td>10.122.144.155</td><td>k8s-master3</td><td>4Core 4G</td><td>master</td></tr><tr><td>LSB1</td><td>10.122.144.150</td><td>LSB1</td><td>4Core 4G</td><td>LSB1</td></tr><tr><td>LSB2</td><td>10.122.144.151</td><td>LSB2</td><td>4Core 4G</td><td>LSB2</td></tr><tr><td>VIP</td><td>10.122.144.152</td><td>VIP</td><td>4Core 4G</td><td>VIP</td></tr><tr><td>Worker1</td><td>10.122.144.156</td><td>k8s-node1</td><td>4Core 4G</td><td>node</td></tr><tr><td>Worker2</td><td>10.122.144.157</td><td>k8s-node2</td><td>4Core 4G</td><td>node</td></tr><tr><td>Worker3</td><td>10.122.144.158</td><td>k8s-node3</td><td>4Core 4G</td><td>node</td></tr></tbody></table><h3 id="1-2-基础环境"><a href="#1-2-基础环境" class="headerlink" title="1.2 基础环境"></a>1.2 基础环境</h3><ul><li><p>所有节点关闭不必要的服务</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined">systemctl disable <span class="hljs-comment">--now firewalld</span><br>setenforce <span class="hljs-number">0</span><br>sed -i <span class="hljs-string">'s@SELINUX=enforcing@SELINUX=disabled@'</span> /etc/selinux/<span class="hljs-built_in">config</span><br>systemctl disable <span class="hljs-comment">--now NetworkManager</span><br>systemctl disable <span class="hljs-comment">--now dnsmasq</span><br></code></pre></td></tr></table></figure><p>创建/etc/sysctl.d/k8s.conf文件，添加如下内容：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">net<span class="hljs-selector-class">.bridge</span><span class="hljs-selector-class">.bridge-nf-call-ip6tables</span> = <span class="hljs-number">1</span><br>net<span class="hljs-selector-class">.bridge</span><span class="hljs-selector-class">.bridge-nf-call-iptables</span> = <span class="hljs-number">1</span><br>net<span class="hljs-selector-class">.ipv4</span><span class="hljs-selector-class">.ip_forward</span> = <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>执行命令使修改生效。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">modprobe br_netfilter<br>sysctl -<span class="hljs-selector-tag">p</span> /etc/sysctl.d/k8s.conf<br></code></pre></td></tr></table></figure></li></ul><h3 id="1-2-1-kube-proxy开启ipvs的前置条件"><a href="#1-2-1-kube-proxy开启ipvs的前置条件" class="headerlink" title="1.2.1 kube-proxy开启ipvs的前置条件"></a>1.2.1 kube-proxy开启ipvs的前置条件</h3><p> 由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块：<br> <figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">ip_vs</span><br>ip_vs_rr<br>ip_vs_wrr<br>ip_vs_sh<br>nf_conntrack<br></code></pre></td></tr></table></figure></p><p> 在所有的Kubernetes节点node1和node2上执行以下脚本:</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF<br><span class="hljs-meta">#!/bin/bash</span><br>modprobe -- ip_vs<br>modprobe -- ip_vs_rr<br>modprobe -- ip_vs_wrr<br>modprobe -- ip_vs_sh<br>modprobe -- nf_conntrack_ipv4 <br> <br>EOF<br>chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack<br></code></pre></td></tr></table></figure> <figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"># keruse <span class="hljs-symbol">'nf_conntrack</span>' instead <span class="hljs-keyword">of</span> <span class="hljs-symbol">'nf_conntrack_ipv4</span>' <span class="hljs-keyword">for</span> linux kernel &gt;= <span class="hljs-number">4.19</span><br></code></pre></td></tr></table></figure><p> 上面脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。</p><p> 接下来还需要确保各个节点上已经安装了ipset软件包yum install ipset。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm yum install ipvsadm。</p><p> 如果以上前提条件如果不满足，则即使kube-proxy的配置开启了ipvs模式，也会退回到iptables模式。</p><h3 id="1-3-安装Docker"><a href="#1-3-安装Docker" class="headerlink" title="1.3 安装Docker"></a>1.3 安装Docker</h3><p> Kubernetes从1.6开始使用CRI(Container Runtime Interface)容器运行时接口。默认的容器运行时仍然是Docker，使用的是kubelet中内置dockershim CRI实现。</p><p> 内网状态下直接执行：<br> <figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum <span class="hljs-keyword">install</span> docker<br></code></pre></td></tr></table></figure></p><p> 安装docker的yum源:<br> <figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum install -<span class="hljs-keyword">y</span> yum-utils device-mapper-persistent-data lvm2<br>yum-config-manager \<br>    --<span class="hljs-built_in">add</span>-repo \<br>    http<span class="hljs-variable">s:</span>//download.docker.<span class="hljs-keyword">com</span>/linux/centos/docker-<span class="hljs-keyword">ce</span>.repo<br>查看最新的Docker版本：<br><br>yum <span class="hljs-keyword">list</span> docker-<span class="hljs-keyword">ce</span>.x86_64  --showduplicates |<span class="hljs-keyword">sort</span> -r<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">3</span>:<span class="hljs-number">18.09</span>.<span class="hljs-number">0</span>-<span class="hljs-number">3</span>.el7                     docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">18.06</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">3</span>.el7                    docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">18.06</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">3</span>.el7                    docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">18.03</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">18.03</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.12</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.12</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.09</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.09</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.06</span>.<span class="hljs-number">2</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.06</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.06</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.03</span>.<span class="hljs-number">3</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7                    docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.03</span>.<span class="hljs-number">2</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.03</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.03</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br></code></pre></td></tr></table></figure></p><p> Kubernetes 1.12已经针对Docker的1.11.1, 1.12.1, 1.13.1, 17.03, 17.06, 17.09, 18.06等版本做了验证，需要注意Kubernetes 1.12最低支持的Docker版本是1.11.1。Kubernetes 1.13对Docker的版本依赖方面没有变化。 我们这里在各节点安装docker的18.06.1版本。</p> <figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum makecache fast<br><br>yum install -y --setopt=obsoletes=<span class="hljs-number">0</span> \<br>  docker-ce<span class="hljs-number">-18.06</span><span class="hljs-number">.1</span>.ce<span class="hljs-number">-3.</span>el7<br><br>systemctl start docker<br>systemctl enable docker<br></code></pre></td></tr></table></figure><p> 确认一下iptables filter表中FOWARD链的默认策略(pllicy)为ACCEPT。<br> <figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs undefined">iptables -nvL<br>Chain INPUT (policy ACCEPT <span class="hljs-number">263</span> packets, <span class="hljs-number">19209</span> bytes)<br> pkts bytes target     prot opt in     out     source               destination<br><br>Chain FORWARD (policy ACCEPT <span class="hljs-number">0</span> packets, <span class="hljs-number">0</span> bytes)<br> pkts bytes target     prot opt in     out     source               destination<br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> DOCKER-USER  all  --  *      *       <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span><br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> DOCKER-ISOLATION-STAGE<span class="hljs-number">-1</span>  all  --  *      *       <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span><br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> ACCEPT     all  --  *      docker0  <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            ctstate RELATED,ESTABLISHED<br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> DOCKER     all  --  *      docker0  <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span><br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> ACCEPT     all  --  docker0 !docker0  <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span><br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> ACCEPT     all  --  docker0 docker0  <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure></p><blockquote><p>Docker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨Node的Pod无法通信。但这里通过安装docker 1806，发现默认策略又改回了ACCEPT，这个不知道是从哪个版本改回的，因为我们线上版本使用的1706还是需要手动调整这个策略的。</p></blockquote><h1 id="2-使用kubeadm部署Kubernetes"><a href="#2-使用kubeadm部署Kubernetes" class="headerlink" title="2. 使用kubeadm部署Kubernetes"></a>2. 使用kubeadm部署Kubernetes</h1><h3 id="2-1-安装kubeadm和kubelet"><a href="#2-1-安装kubeadm和kubelet" class="headerlink" title="2.1 安装kubeadm和kubelet"></a>2.1 安装kubeadm和kubelet</h3><p>  下面在各节点安装kubeadm和kubelet：<br> <figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo<br>[kubernetes]<br>name=Kubernetes<br>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64<br>enabled=1<br>gpgcheck=1<br>repo_gpgcheck=1<br>gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg<br>        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg<br>EOF<br></code></pre></td></tr></table></figure></p><p> 测试地址<a href="https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64是否可用，如果不可用需要科学上网。" target="_blank" rel="noopener">https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64是否可用，如果不可用需要科学上网。</a><br> <figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">curl https:<span class="hljs-regexp">//</span>packages.cloud.google.com<span class="hljs-regexp">/yum/</span>repos<span class="hljs-regexp">/kubernetes-el7-x86_64</span><br></code></pre></td></tr></table></figure></p> <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-selector-tag">yum</span> <span class="hljs-selector-tag">makecache</span> <span class="hljs-selector-tag">fast</span><br><span class="hljs-selector-tag">yum</span> <span class="hljs-selector-tag">install</span> <span class="hljs-selector-tag">-y</span> <span class="hljs-selector-tag">kubelet</span> <span class="hljs-selector-tag">kubeadm</span> <span class="hljs-selector-tag">kubectl</span> <span class="hljs-selector-tag">ipvsadm</span> <span class="hljs-selector-tag">ipset</span><br><br><br><span class="hljs-selector-tag">Installed</span>:<br><br><span class="hljs-selector-tag">socat-1</span><span class="hljs-selector-class">.7</span><span class="hljs-selector-class">.3</span><span class="hljs-selector-class">.2-2</span><span class="hljs-selector-class">.el7</span><span class="hljs-selector-class">.x86_64</span>               <br><span class="hljs-selector-tag">libnetfilter_queue-1</span><span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.2-2</span><span class="hljs-selector-class">.el7_2</span><span class="hljs-selector-class">.x86_64</span>  <br><span class="hljs-selector-tag">libnetfilter_cttimeout-1</span><span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.0-6</span><span class="hljs-selector-class">.el7</span><span class="hljs-selector-class">.x86_64</span><br><span class="hljs-selector-tag">kubectl-1</span><span class="hljs-selector-class">.14</span><span class="hljs-selector-class">.1-0</span><span class="hljs-selector-class">.x86_64</span>                  <br><span class="hljs-selector-tag">libnetfilter_cthelper-1</span><span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.0-9</span><span class="hljs-selector-class">.el7</span><span class="hljs-selector-class">.x86_64</span> <br><span class="hljs-selector-tag">conntrack-tools-1</span><span class="hljs-selector-class">.4</span><span class="hljs-selector-class">.4-4</span><span class="hljs-selector-class">.el7</span><span class="hljs-selector-class">.x86_64</span>       <br><span class="hljs-selector-tag">kubernetes-cni-0</span><span class="hljs-selector-class">.7</span><span class="hljs-selector-class">.5-0</span><span class="hljs-selector-class">.x86_64</span>            <br><span class="hljs-selector-tag">kubelet-1</span><span class="hljs-selector-class">.14</span><span class="hljs-selector-class">.1-0</span><span class="hljs-selector-class">.x86_64</span>                  <br><span class="hljs-selector-tag">cri-tools-1</span><span class="hljs-selector-class">.12</span><span class="hljs-selector-class">.0-0</span><span class="hljs-selector-class">.x86_64</span>                <br><span class="hljs-selector-tag">kubeadm-1</span><span class="hljs-selector-class">.14</span><span class="hljs-selector-class">.1-0</span><span class="hljs-selector-class">.x86_64</span><br><span class="hljs-selector-tag">ipvsadm</span><span class="hljs-selector-class">.x86_64</span> 0<span class="hljs-selector-pseudo">:1.27-7.el7</span><br><span class="hljs-selector-tag">ipset-6</span><span class="hljs-selector-class">.38-3</span><span class="hljs-selector-class">.el7_6</span><span class="hljs-selector-class">.x86_64</span><br></code></pre></td></tr></table></figure><p> 关闭系统的Swap方法如下:<br> <figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">swapoff -a</span><br></code></pre></td></tr></table></figure></p><p> 修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行：<br> <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attr">vm.swappiness</span>=<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure></p><p> 执行<br> <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">sysctl -<span class="hljs-selector-tag">p</span> /etc/sysctl.d/k8s<span class="hljs-selector-class">.conf</span><br><br>使修改生效。<br></code></pre></td></tr></table></figure></p><hr><p> 因为这里本次用于测试两台主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。 之前的Kubernetes版本我们都是通过kubelet的启动参数–fail-swap-on=false去掉这个限制的。前面已经分析了Kubernetes不再推荐使用启动参数，而推荐使用配置文件。 所以这里我们改成配置文件配置的形式。</p><p> 查看/etc/systemd/system/kubelet.service.d/10-kubeadm.conf，看到了下面的内容：</p> <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment"># <span class="hljs-doctag">Note:</span> This dropin only works with kubeadm and kubelet v1.11+</span><br><span class="hljs-section">[Service]</span><br><span class="hljs-attr">Environment</span>=<span class="hljs-string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span><br><span class="hljs-attr">Environment</span>=<span class="hljs-string">"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"</span><br><span class="hljs-comment"># This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically</span><br><span class="hljs-attr">EnvironmentFile</span>=-/var/lib/kubelet/kubeadm-flags.env<br><span class="hljs-comment"># This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use</span><br><span class="hljs-comment"># the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.</span><br><span class="hljs-attr">EnvironmentFile</span>=-/etc/sysconfig/kubelet<br><span class="hljs-attr">ExecStart</span>=<br><span class="hljs-attr">ExecStart</span>=/usr/bin/kubelet <span class="hljs-variable">$KUBELET_KUBECONFIG_ARGS</span> <span class="hljs-variable">$KUBELET_CONFIG_ARGS</span> <span class="hljs-variable">$KUBELET_KUBEADM_ARGS</span> <span class="hljs-variable">$KUBELET_EXTRA_ARGS</span><br></code></pre></td></tr></table></figure><p> 上面显示kubeadm部署的kubelet的配置文件–config=/var/lib/kubelet/config.yaml，实际去查看/var/lib/kubelet和这个config.yaml的配置文件都没有被创建。 可以猜想肯定是运行kubeadm初始化集群时会自动生成这个配置文件，而如果我们不关闭Swap的话，第一次初始化集群肯定会失败的。</p><p> 所以还是老老实实的回到使用kubelet的启动参数–fail-swap-on=false去掉必须关闭Swap的限制。 修改/etc/sysconfig/kubelet，加入：<br> <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attr">KUBELET_EXTRA_ARGS</span>=--fail-swap-<span class="hljs-literal">on</span>=<span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure></p><h3 id="配置keepalived和Haproxy"><a href="#配置keepalived和Haproxy" class="headerlink" title="配置keepalived和Haproxy"></a>配置keepalived和Haproxy</h3><ul><li><p>在lbs1和lbs2节点配置keepalived和haproxy, lbs1为MASTER,  lbs2为BACKUP ls1, ls2 配置文件相同</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum <span class="hljs-keyword">install</span> -y keepalived haproxy<br></code></pre></td></tr></table></figure></li><li><p>配置lbs1节点上的keepalived</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs undefined">cat &lt;&lt; <span class="hljs-literal">EOF</span> | tee /etc/keepalived/keepalived.conf<br>! Configuration File for keepalived<br><br>global_defs &#123;<br>   notification_email &#123;<br>      root@localhost<br>   &#125;<br>   notification_email_from keepalived@localhost<br>   smtp_server <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span><br>   smtp_connect_timeout <span class="hljs-number">30</span><br>   router_id kh01<br>   vrrp_mcast_group4 <span class="hljs-number">224.0</span><span class="hljs-number">.100</span><span class="hljs-number">.100</span><br>&#125;<br><br>vrrp_instance VI_1 &#123;<br>    <span class="hljs-section">state</span> MASTER<br>    interface ens192<br>    virtual_router_id <span class="hljs-number">51</span><br>    priority <span class="hljs-number">100</span><br>    advert_int <span class="hljs-number">1</span><br>    authentication &#123;<br>        auth_type PASS<br>        auth_pass <span class="hljs-number">0</span>d25bedd3b13081c5ba5<br>    &#125;<br>    virtual_ipaddress &#123;<br>        <span class="hljs-number">10.122</span><span class="hljs-number">.144</span><span class="hljs-number">.152</span>/<span class="hljs-number">16</span><br>    &#125;<br>&#125;<br><span class="hljs-literal">EOF</span><br><br>systemctl enable keepalived<br>systemctl start keepalived<br></code></pre></td></tr></table></figure></li><li><p>配置lbs1节点上的haproxy</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs undefined">cat &lt;&lt; EOF | tee /etc/haproxy/haproxy.cfg<br>global<br>    log         127.0.0.1 local2<br>    chroot      /var/lib/haproxy<br>    pidfile     /var/run/haproxy.pid<br>    maxconn     4000<br>   <span class="hljs-built_in"> user </span>       haproxy<br>   <span class="hljs-built_in"> group </span>      haproxy<br>    daemon<br><br>defaults<br>    mode                    tcp<br>    log                     global<br>    retries                 3<br>    timeout connect         10s<br>    timeout<span class="hljs-built_in"> client </span>         1m<br>    timeout<span class="hljs-built_in"> server </span>         1m<br><br>frontend kubernetes<br>    bind *:6443<br>    mode tcp<br>    default_backend kubernetes-master<br><br>backend kubernetes-master<br>    balance roundrobin<br>   <span class="hljs-built_in"> server </span>master  10.122.144.153:6443 check maxconn 2000<br>   <span class="hljs-built_in"> server </span>master2 10.122.144.153:6443 check maxconn 2000<br>   <span class="hljs-built_in"> server </span>master3 10.122.144.153:6443 check maxconn 2000<br>EOF<br><br>systemctl <span class="hljs-builtin-name">enable</span> haproxy<br>systemctl start haproxy<br></code></pre></td></tr></table></figure></li></ul><h3 id="修改-kubeadm-配置信息"><a href="#修改-kubeadm-配置信息" class="headerlink" title="修改 kubeadm 配置信息"></a>修改 kubeadm 配置信息</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs angular2">导出 配置 信息<br>kubeadm config print init-defaults &gt; kubeadm-init.yaml<br></code></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs angular2"># 修改相关配置 ，本文配置信息如下<br><br><br>apiVersion: kubeadm.k8s.io/v1beta1<br>bootstrapTokens:<br>- groups:<br>  - system:bootstrappers:kubeadm:default-node-token<br>  token: abcdef.0123456789abcdef<br>  ttl: 24h0m0s<br>  usages:<br>  - signing<br>  - authentication<br>kind: InitConfiguration<br>localAPIEndpoint:<br>  advertiseAddress: 127.0.0.1<br>  bindPort: 6443<br>nodeRegistration:<br>  criSocket: /var/run/dockershim.sock<br>  name: kubernetes-1<br>  taints:<br>  - effect: NoSchedule<br>    key: node-role.kubernetes.io/master<br>---<br>apiServer:<br>  timeoutForControlPlane: 4m0s<br>apiVersion: kubeadm.k8s.io/v1beta1<br>certificatesDir: /etc/kubernetes/pki<br>clusterName: kubernetes<br>controlPlaneEndpoint: &quot;10.122.144.152:6443&quot;<br>controllerManager: &#123;&#125;<br>dns:<br>  type: CoreDNS<br>etcd:<br>  local:<br>    dataDir: /var/lib/etcd<br>imageRepository: registry.gcr.io/google_containers<br>kind: ClusterConfiguration<br>kubernetesVersion: v1.15.0<br>networking:<br>  dnsDomain: cluster.local<br>  podSubnet: &quot;10.254.64.0/18&quot;<br>  serviceSubnet: &quot;10.254.0.0/18&quot;<br>scheduler: &#123;&#125;<br>---<br>apiVersion: kubeproxy.config.k8s.io/v1alpha1<br>kind: KubeProxyConfiguration<br>mode: &quot;ipvs&quot;<br></code></pre></td></tr></table></figure><h3 id="3-4-初始化集群"><a href="#3-4-初始化集群" class="headerlink" title="3.4 初始化集群"></a>3.4 初始化集群</h3><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment">kubeadm</span> <span class="hljs-comment">init</span> <span class="hljs-literal">-</span><span class="hljs-literal">-</span><span class="hljs-comment">config=kubeadm</span><span class="hljs-literal">-</span><span class="hljs-comment">config</span><span class="hljs-string">.</span><span class="hljs-comment">yaml</span> <span class="hljs-literal">-</span><span class="hljs-literal">-</span><span class="hljs-comment">upload</span><span class="hljs-literal">-</span><span class="hljs-comment">certs</span><br><br><span class="hljs-comment">该</span><span class="hljs-literal">-</span><span class="hljs-literal">-</span><span class="hljs-comment">upload</span><span class="hljs-literal">-</span><span class="hljs-comment">certs标志用于将应在所有控制平面实例之间共享的证书上载到群集。相反，如果您希望手动或使用自动化工具跨控制平面节点复制证书，请删除此标志并参考下面的手动证书分发部分。</span><br></code></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[bootstrap-token] creating the "cluster-info" ConfigMap in the "kube-public" namespace<br>[addons] Applied essential addon: CoreDNS<br>[addons] Applied essential addon: kube-proxy<br><br>Your Kubernetes control-plane has initialized successfully!<br><br>To <span class="hljs-keyword">start</span> <span class="hljs-keyword">using</span> your cluster, you need <span class="hljs-keyword">to</span> run the <span class="hljs-keyword">following</span> <span class="hljs-keyword">as</span> a regular <span class="hljs-keyword">user</span>:<br><br>  mkdir -p $HOME/.kube<br>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br>  sudo chown $(<span class="hljs-keyword">id</span> -u):$(<span class="hljs-keyword">id</span> -g) $HOME/.kube/config<br><br>You should <span class="hljs-keyword">now</span> deploy a pod network <span class="hljs-keyword">to</span> the cluster.<br>Run <span class="hljs-string">"kubectl apply -f [podnetwork].yaml"</span> <span class="hljs-keyword">with</span> one <span class="hljs-keyword">of</span> the options listed <span class="hljs-keyword">at</span>:<br>  https://kubernetes.io/docs/concepts/cluster-administration/addons/<br><br>You can <span class="hljs-keyword">now</span> <span class="hljs-keyword">join</span> <span class="hljs-keyword">any</span> <span class="hljs-built_in">number</span> <span class="hljs-keyword">of</span> control-plane nodes <span class="hljs-keyword">by</span> copying certificate authorities <br><span class="hljs-keyword">and</span> service <span class="hljs-keyword">account</span> <span class="hljs-keyword">keys</span> <span class="hljs-keyword">on</span> <span class="hljs-keyword">each</span> node <span class="hljs-keyword">and</span> <span class="hljs-keyword">then</span> running the <span class="hljs-keyword">following</span> <span class="hljs-keyword">as</span> root:<br><br>  kubeadm <span class="hljs-keyword">join</span> <span class="hljs-number">10.122</span><span class="hljs-number">.144</span><span class="hljs-number">.152</span>:<span class="hljs-number">6443</span> <span class="hljs-comment">--token abcdef.0123456789abcdef \</span><br>    <span class="hljs-comment">--discovery-token-ca-cert-hash sha256:5dff9a228bf55f1e8a6a754f3c087017247734b06c24138c0474d448d8aa5569 \</span><br>    <span class="hljs-comment">--experimental-control-plane          </span><br><br><span class="hljs-keyword">Then</span> you can <span class="hljs-keyword">join</span> <span class="hljs-keyword">any</span> <span class="hljs-built_in">number</span> <span class="hljs-keyword">of</span> worker nodes <span class="hljs-keyword">by</span> running the <span class="hljs-keyword">following</span> <span class="hljs-keyword">on</span> <span class="hljs-keyword">each</span> <span class="hljs-keyword">as</span> root:<br><br>kubeadm <span class="hljs-keyword">join</span> <span class="hljs-number">10.122</span><span class="hljs-number">.144</span><span class="hljs-number">.152</span>:<span class="hljs-number">6443</span> <span class="hljs-comment">--token abcdef.0123456789abcdef \</span><br>    <span class="hljs-comment">--discovery-token-ca-cert-hash sha256:5dff9a228bf55f1e8a6a754f3c087017247734b06c24138c0474d448d8aa5569</span><br></code></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs angular2">mkdir -p $HOME/.kube<br>cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br>chown $(id -u):$(id -g) $HOME/.kube/config<br></code></pre></td></tr></table></figure><h4 id="查看集群状态"><a href="#查看集群状态" class="headerlink" title="查看集群状态"></a>查看集群状态</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@kubernetes-1 ~]# kubectl <span class="hljs-builtin-name">get</span> cs<br>NAME                 STATUS    MESSAGE             <span class="hljs-builtin-name">ERROR</span><br>controller-manager   Healthy   ok                  <span class="hljs-built_in"><br>scheduler </span>           Healthy   ok                  <br>etcd-0               Healthy   &#123;<span class="hljs-string">"health"</span>:<span class="hljs-string">"true"</span>&#125;<br></code></pre></td></tr></table></figure><h3 id="其余控制平面节点的步骤"><a href="#其余控制平面节点的步骤" class="headerlink" title="其余控制平面节点的步骤"></a>其余控制平面节点的步骤</h3><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-selector-tag">kubeadm</span> <span class="hljs-selector-tag">join</span> 10<span class="hljs-selector-class">.122</span><span class="hljs-selector-class">.144</span><span class="hljs-selector-class">.152</span><span class="hljs-selector-pseudo">:6443</span> <span class="hljs-selector-tag">--token</span> <span class="hljs-selector-tag">abcdef</span><span class="hljs-selector-class">.0123456789abcdef</span> \<br>    <span class="hljs-selector-tag">--discovery-token-ca-cert-hash</span> <span class="hljs-selector-tag">sha256</span><span class="hljs-selector-pseudo">:5dff9a228bf55f1e8a6a754f3c087017247734b06c24138c0474d448d8aa5569</span> \<br>    <span class="hljs-selector-tag">--experimental-control-plane</span><br></code></pre></td></tr></table></figure><h3 id="加入工作节点"><a href="#加入工作节点" class="headerlink" title="加入工作节点"></a>加入工作节点</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubeadm join <span class="hljs-number">10.122</span><span class="hljs-number">.144</span><span class="hljs-number">.152</span>:<span class="hljs-number">6443</span> <span class="hljs-comment">--token abcdef.0123456789abcdef \</span><br>    <span class="hljs-comment">--discovery-token-ca-cert-hash sha256:5dff9a228bf55f1e8a6a754f3c087017247734b06c24138c0474d448d8aa5569</span><br><br>输出如下:<br><br>This node has joined <span class="hljs-keyword">the</span> cluster:<br>* Certificate signing request was sent <span class="hljs-built_in">to</span> apiserver <span class="hljs-keyword">and</span> <span class="hljs-keyword">a</span> response was received.<br>* The Kubelet was informed <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> <span class="hljs-built_in">new</span> secure connection details.<br><br>Run <span class="hljs-string">'kubectl get nodes'</span> <span class="hljs-keyword">on</span> <span class="hljs-title">the</span> <span class="hljs-title">control-plane</span> <span class="hljs-title">to</span> <span class="hljs-title">see</span> <span class="hljs-title">this</span> <span class="hljs-title">node</span> <span class="hljs-title">join</span> <span class="hljs-title">the</span> <span class="hljs-title">cluster</span>.<br></code></pre></td></tr></table></figure><h3 id="验证节点"><a href="#验证节点" class="headerlink" title="验证节点"></a>验证节点</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs undefined">root@kubernetes<span class="hljs-number">-1</span> ~]# kubectl get nodes<br>NAME           STATUS     ROLES    AGE     VERSION<br>kubernetes<span class="hljs-number">-1</span>   NotReady   master   <span class="hljs-number">13</span>m     v1<span class="hljs-number">.15</span><span class="hljs-number">.0</span><br>kubernetes<span class="hljs-number">-2</span>   NotReady   master   <span class="hljs-number">8</span>m20s   v1<span class="hljs-number">.15</span><span class="hljs-number">.0</span><br>kubernetes<span class="hljs-number">-3</span>   NotReady   master   <span class="hljs-number">5</span>m21s   v1<span class="hljs-number">.15</span><span class="hljs-number">.0</span><br>kubernetes<span class="hljs-number">-4</span>   NotReady   &lt;none&gt;   <span class="hljs-number">45</span>s     v1<span class="hljs-number">.15</span><span class="hljs-number">.0</span><br>kubernetes<span class="hljs-number">-5</span>   NotReady   &lt;none&gt;   <span class="hljs-number">39</span>s     v1<span class="hljs-number">.15</span><span class="hljs-number">.0</span><br></code></pre></td></tr></table></figure><h3 id="3-7-安装网络组件"><a href="#3-7-安装网络组件" class="headerlink" title="3.7 安装网络组件"></a>3.7 安装网络组件</h3><p>   Calico 网络</p><pre><code>官方文档 https://docs.projectcalico.org/v3.7/introduction</code></pre><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs undefined">下载 yaml 文件<br><br>wget <span class="hljs-string">https:</span><span class="hljs-comment">//docs.projectcalico.org/v3.7/manifests/calico.yaml</span><br><br>vi calico.yaml<br><br><br><br>修改 pods 分配的 IP 段<br>            - <span class="hljs-string">name:</span> CALICO_IPV4POOL_CIDR<br><span class="hljs-symbol">              value:</span> <span class="hljs-string">"10.254.64.0/18"</span><br></code></pre></td></tr></table></figure><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment"># 导入 yaml 文件</span><br><br>[root@kubernetes-<span class="hljs-number">1</span> calico]<span class="hljs-comment"># kubectl apply -f calico.yaml </span><br>configmap/calico-config created<br>customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/cluster<span class="hljs-literal">inf</span>ormations.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created<br>clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created<br>clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created<br>clusterrole.rbac.authorization.k8s.io/calico-<span class="hljs-keyword">node</span> <span class="hljs-title">created</span><br>clusterrolebinding.rbac.authorization.k8s.io/calico-<span class="hljs-keyword">node</span> <span class="hljs-title">created</span><br>daemonset.extensions/calico-<span class="hljs-keyword">node</span> <span class="hljs-title">created</span><br>serviceaccount/calico-<span class="hljs-keyword">node</span> <span class="hljs-title">created</span><br>deployment.extensions/calico-kube-controllers created<br>serviceaccount/calico-kube-controllers created<br><br><br><br><span class="hljs-comment"># 查看服务</span><br><br>[root@kubernetes-<span class="hljs-number">1</span> calico]<span class="hljs-comment"># kubectl get pods -n kube-system |grep calico</span><br>calico-kube-controllers-<span class="hljs-number">8646</span>dd497f-<span class="hljs-number">4</span>d4hr   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">28s</span><br>calico-<span class="hljs-keyword">node</span><span class="hljs-title">-455mf</span>                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">28s</span><br>calico-<span class="hljs-keyword">node</span><span class="hljs-title">-b8mrr</span>                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">28s</span><br>calico-<span class="hljs-keyword">node</span><span class="hljs-title">-fq9dj</span>                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">28s</span><br>calico-<span class="hljs-keyword">node</span><span class="hljs-title">-sk77j</span>                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">28s</span><br>calico-<span class="hljs-keyword">node</span><span class="hljs-title">-xxnb8</span>                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">28s</span><br></code></pre></td></tr></table></figure><h3 id="检验整体集群"><a href="#检验整体集群" class="headerlink" title="检验整体集群"></a>检验整体集群</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs undefined">root@kubernetes<span class="hljs-number">-1</span> ~]# kubectl get nodes<br>NAME           STATUS     ROLES    AGE     VERSION<br>kubernetes<span class="hljs-number">-1</span>   NotReady   master   <span class="hljs-number">13</span>m     v1<span class="hljs-number">.15</span><span class="hljs-number">.0</span><br>kubernetes<span class="hljs-number">-2</span>   NotReady   master   <span class="hljs-number">8</span>m20s   v1<span class="hljs-number">.15</span><span class="hljs-number">.0</span><br>kubernetes<span class="hljs-number">-3</span>   NotReady   master   <span class="hljs-number">5</span>m21s   v1<span class="hljs-number">.15</span><span class="hljs-number">.0</span><br>kubernetes<span class="hljs-number">-4</span>   NotReady   &lt;none&gt;   <span class="hljs-number">45</span>s     v1<span class="hljs-number">.15</span><span class="hljs-number">.0</span><br>kubernetes<span class="hljs-number">-5</span>   NotReady   &lt;none&gt;   <span class="hljs-number">39</span>s     v1<span class="hljs-number">.15</span><span class="hljs-number">.0</span><br></code></pre></td></tr></table></figure><h3 id="查看-pods-状态"><a href="#查看-pods-状态" class="headerlink" title="查看 pods 状态"></a>查看 pods 状态</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@kubernetes<span class="hljs-number">-1</span> ~]# kubectl get pods --all-namespaces<br>NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE<br>kube-system   calico-kube-controllers<span class="hljs-number">-8646</span>dd497f<span class="hljs-number">-4</span>d4hr   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">3</span>m33s<br>kube-system   calico-node<span class="hljs-number">-455</span>mf                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">3</span>m33s<br>kube-system   calico-node-b8mrr                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">3</span>m33s<br>kube-system   calico-node-fq9dj                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">3</span>m33s<br>kube-system   calico-node-sk77j                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">3</span>m33s<br>kube-system   calico-node-xxnb8                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">3</span>m33s<br>kube-system   coredns-d5947d4b-q4pst                     <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">35</span>m<br>kube-system   coredns-d5947d4b-sz2bm                     <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">35</span>m<br>kube-system   etcd-kubernetes<span class="hljs-number">-1</span>                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">34</span>m<br>kube-system   etcd-kubernetes<span class="hljs-number">-2</span>                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">30</span>m<br>kube-system   etcd-kubernetes<span class="hljs-number">-3</span>                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">27</span>m<br>kube-system   kube-apiserver-kubernetes<span class="hljs-number">-1</span>                <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">35</span>m<br>kube-system   kube-apiserver-kubernetes<span class="hljs-number">-2</span>                <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">30</span>m<br>kube-system   kube-apiserver-kubernetes<span class="hljs-number">-3</span>                <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">26</span>m<br>kube-system   kube-controller-manager-kubernetes<span class="hljs-number">-1</span>       <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">1</span>          <span class="hljs-number">34</span>m<br>kube-system   kube-controller-manager-kubernetes<span class="hljs-number">-2</span>       <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">30</span>m<br>kube-system   kube-controller-manager-kubernetes<span class="hljs-number">-3</span>       <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">26</span>m<br>kube-system   kube-proxy<span class="hljs-number">-2</span>ht7m                           <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">35</span>m<br>kube-system   kube-proxy-bkg7z                           <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">22</span>m<br>kube-system   kube-proxy-sdxnj                           <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">23</span>m<br>kube-system   kube-proxy-tpc4x                           <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">27</span>m<br>kube-system   kube-proxy-wvllx                           <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">30</span>m<br>kube-system   kube-scheduler-kubernetes<span class="hljs-number">-1</span>                <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">1</span>          <span class="hljs-number">35</span>m<br>kube-system   kube-scheduler-kubernetes<span class="hljs-number">-2</span>                <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">30</span>m<br>kube-system   kube-scheduler-kubernetes<span class="hljs-number">-3</span>                <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">26</span>m<br></code></pre></td></tr></table></figure><h3 id="查看-IPVS-的状态"><a href="#查看-IPVS-的状态" class="headerlink" title="查看 IPVS 的状态"></a>查看 IPVS 的状态</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs undefined">root@kubernetes-1 ~]# ipvsadm -L -n<br><span class="hljs-built_in"><br>IP </span>Virtual<span class="hljs-built_in"> Server </span>version 1.2.1 (<span class="hljs-attribute">size</span>=4096)<br>Prot LocalAddress:Port<span class="hljs-built_in"> Scheduler </span>Flags<br>  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn<br>TCP  10.254.0.1:443 rr<br>  -&gt; 192.168.168.11:6443          Masq    1      2          0         <br>  -&gt; 192.168.168.12:6443          Masq    1      0          0         <br>  -&gt; 192.168.168.13:6443          Masq    1      1          0         <br>TCP  10.254.0.10:53 rr<br>  -&gt; 10.254.91.66:53              Masq    1      0          0         <br>  -&gt; 10.254.105.65:53             Masq    1      0          0         <br>TCP  10.254.0.10:9153 rr<br>  -&gt; 10.254.91.66:9153            Masq    1      0          0         <br>  -&gt; 10.254.105.65:9153           Masq    1      0          0         <br>UDP  10.254.0.10:53 rr<br>  -&gt; 10.254.91.66:53              Masq    1      0          0         <br>  -&gt; 10.254.105.65:53             Masq    1      0          0<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>kubernetes</tag>
      
      <tag>kubeadm</tag>
      
      <tag>centos7</tag>
      
      <tag>Haproxy</tag>
      
      <tag>keepalived</tag>
      
      <tag>scope</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>centos7配置全局proxy代理</title>
    <link href="/2019/06/20/centos7%E9%85%8D%E7%BD%AE%E5%85%A8%E5%B1%80proxy%E4%BB%A3%E7%90%86/"/>
    <url>/2019/06/20/centos7%E9%85%8D%E7%BD%AE%E5%85%A8%E5%B1%80proxy%E4%BB%A3%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h3 id="1-配置epel源，安装python-pip"><a href="#1-配置epel源，安装python-pip" class="headerlink" title="1 配置epel源，安装python-pip"></a>1 配置epel源，安装python-pip</h3><pre><code>wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repoyum install python-pippip install --upgrade pip</code></pre><h3 id="2-安装-pip"><a href="#2-安装-pip" class="headerlink" title="2 安装 pip"></a>2 安装 pip</h3><p>Pip 是 Python 的包管理工具，这里我们用 pip 安装 shadowsocks。</p><p>有些文章会介绍用 yum install -y pip 安装，我用的是官方一个最小化的 CentOS，没有这个包，所以手动安装。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">curl <span class="hljs-string">"https://bootstrap.pypa.io/get-pip.py"</span> -<span class="hljs-keyword">o</span> <span class="hljs-string">"get-pip.py"</span><br><span class="hljs-keyword">python</span> <span class="hljs-built_in">get</span>-pip.<span class="hljs-keyword">py</span><br></code></pre></td></tr></table></figure><h3 id="3-Shadowsocks-客户端"><a href="#3-Shadowsocks-客户端" class="headerlink" title="3 Shadowsocks 客户端"></a>3 Shadowsocks 客户端</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">pip <span class="hljs-keyword">install</span> <span class="hljs-comment">--upgrade pip</span><br>pip <span class="hljs-keyword">install</span> shadowsocks<br></code></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>####新建配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angular2">vi /etc/shadowsocks.json<br></code></pre></td></tr></table></figure><p>填写以下内容<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">&#123;<br>  <span class="hljs-string">"server"</span>:<span class="hljs-string">"x.x.x.x"</span>,             #你的 ss 服务器<span class="hljs-built_in"> ip<br></span>  <span class="hljs-string">"server_port"</span>:0,                #你的 ss 服务器端口<br>  <span class="hljs-string">"local_address"</span>: <span class="hljs-string">"127.0.0.1"</span>,   #本地ip<br>  <span class="hljs-string">"local_port"</span>:0,                 #本地端口<br>  <span class="hljs-string">"password"</span>:<span class="hljs-string">"password"</span>,          #连接 ss 密码<br>  <span class="hljs-string">"timeout"</span>:300,                  #等待超时<br>  <span class="hljs-string">"method"</span>:<span class="hljs-string">"aes-256-cfb"</span>,         #加密方式<br>  <span class="hljs-string">"workers"</span>: 1                    #工作线程数<br>&#125;<br></code></pre></td></tr></table></figure></p><h3 id="配置自启动"><a href="#配置自启动" class="headerlink" title="配置自启动"></a>配置自启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angular2">vi /etc/systemd/system/shadowsocks.service<br></code></pre></td></tr></table></figure><p>添加以下内容</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-section">[Unit]</span><br><span class="hljs-attr">Description</span>=Shadowsocks<br><span class="hljs-section">[Service]</span><br><span class="hljs-attr">TimeoutStartSec</span>=<span class="hljs-number">0</span><br><span class="hljs-attr">ExecStart</span>=/usr/bin/sslocal -c /etc/shadowsocks/shadowsocks.json<br><span class="hljs-section">[Install]</span><br><span class="hljs-attr">WantedBy</span>=multi-user.target]<br></code></pre></td></tr></table></figure><h3 id="启动shadowsocks客户端"><a href="#启动shadowsocks客户端" class="headerlink" title="启动shadowsocks客户端"></a>启动shadowsocks客户端</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs angular2">systemctl enable shadowsocks.service<br>systemctl start shadowsocks.service<br>systemctl status shadowsocks<br></code></pre></td></tr></table></figure><p>验证shadowsocks客户端是否正常运行，正常会返回你的代理IP<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angular2">curl --socks5 127.0.0.1:1080 http://httpbin.org/ip<br></code></pre></td></tr></table></figure></p><h3 id="安装编译工具"><a href="#安装编译工具" class="headerlink" title="安装编译工具"></a>安装编译工具</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angular2">yum groupinstall &quot;Development Tools&quot;<br></code></pre></td></tr></table></figure><p>下载privoxy最新版<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angular2">http://sourceforge.net/projects/ijbswa/files/Sources/<br></code></pre></td></tr></table></figure></p><p>编译<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs angular2">tar xzvf privoxy-3.0.23-stable-src.tar.gz<br>cd privoxy-3.0.23-stable<br>autoheader<br>autoconf<br>./configure      # (--help to see options)<br>make             # (the make from GNU, sometimes called gmake)<br></code></pre></td></tr></table></figure></p><p>建立账户<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angular2">sudo useradd privoxy -r -s /usr/sbin/nologin<br></code></pre></td></tr></table></figure></p><p>安装<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs angular2">sudo make install<br></code></pre></td></tr></table></figure></p><p>更改侦听地址<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">nano /usr/local/etc/privoxy<span class="hljs-built_in">/config<br></span>将listen-address值更改为 0.0.0.0:8118<br></code></pre></td></tr></table></figure></p><p>重启<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">systemctl restart privoxy</span><br></code></pre></td></tr></table></figure></p><p>打开防火墙端口<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs angular2">firewall-cmd --permanent --add-port=8118/tcp<br>firewall-cmd --reload<br></code></pre></td></tr></table></figure></p><p>范例配置(使用本地socks5上级代理，本地地址不走代理)<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs undefined">nano /usr/local/etc/privoxy<span class="hljs-built_in">/config<br></span>增加<br><br>forward-socks5 / 127.0.0.1:12080 .<br>forward 10.*.*.*/ .<br>forward 192.168.*.*/ .<br>forward 127.*.*.*/ .<br>forward localhost/ .<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
    <tags>
      
      <tag>centos7 privoxy ss</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes-Dashboard</title>
    <link href="/2019/05/21/Kubernetes-Dashboard/"/>
    <url>/2019/05/21/Kubernetes-Dashboard/</url>
    
    <content type="html"><![CDATA[<h2 id="下载官方提供的-Dashboard-组件部署的-yaml-文件"><a href="#下载官方提供的-Dashboard-组件部署的-yaml-文件" class="headerlink" title="下载官方提供的 Dashboard 组件部署的 yaml 文件"></a>下载官方提供的 Dashboard 组件部署的 yaml 文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml<br></code></pre></td></tr></table></figure><h2 id="修改-yaml-文件中的镜像"><a href="#修改-yaml-文件中的镜像" class="headerlink" title="修改 yaml 文件中的镜像"></a>修改 yaml 文件中的镜像</h2><p>k8s.gcr.io 修改为 registry.cn-hangzhou.aliyuncs.com/google_containers，后续所有 yaml 文件中，只要涉及到 image 的，都需要做同样的修改，因为国内 k8s.gcr.io 这个地址被墙了。</p><h2 id="修改-yaml-文件中的-Dashboard-Service，暴露服务使外部能够访问"><a href="#修改-yaml-文件中的-Dashboard-Service，暴露服务使外部能够访问" class="headerlink" title="修改 yaml 文件中的 Dashboard Service，暴露服务使外部能够访问"></a>修改 yaml 文件中的 Dashboard Service，暴露服务使外部能够访问</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs language">kind: Service<br>apiVersion: v1<br>metadata:<br>  labels:<br>    k8s-app: kubernetes-dashboard<br>  name: kubernetes-dashboard<br>  namespace: kube-system<br>spec:<br>  ports:<br>    - port: 443<br>      targetPort: 8443<br>      nodePort: 31111<br>  selector:<br>    k8s-app: kubernetes-dashboard<br>  type: NodePort<br></code></pre></td></tr></table></figure><h2 id="启动-Dashboard"><a href="#启动-Dashboard" class="headerlink" title="启动 Dashboard"></a>启动 Dashboard</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">kubectl apply -f kubernetes-dashboard.yaml<br></code></pre></td></tr></table></figure><h2 id="访问-Dashboard"><a href="#访问-Dashboard" class="headerlink" title="访问 Dashboard"></a>访问 Dashboard</h2><p>地址： https://<your-ip>:31111/<br>注意：必须是 https</your-ip></p><h2 id="创建能够访问-Dashboard-的用户"><a href="#创建能够访问-Dashboard-的用户" class="headerlink" title="创建能够访问 Dashboard 的用户"></a>创建能够访问 Dashboard 的用户</h2><p>新建文件 account.yaml ，内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs language"># Create Service Account<br>apiVersion: v1<br>kind: ServiceAccount<br>metadata:<br>  name: admin-user<br>  namespace: kube-system<br>---<br># Create ClusterRoleBinding<br>apiVersion: rbac.authorization.k8s.io/v1beta1<br>kind: ClusterRoleBinding<br>metadata:<br>  name: admin-user<br>roleRef:<br>  apiGroup: rbac.authorization.k8s.io<br>  kind: ClusterRole<br>  name: cluster-admin<br>subjects:<br>- kind: ServiceAccount<br>  name: admin-user<br>  namespace: kube-system<br></code></pre></td></tr></table></figure><h2 id="获取登录-Dashboard-的令牌-（Token）"><a href="#获取登录-Dashboard-的令牌-（Token）" class="headerlink" title="获取登录 Dashboard 的令牌 （Token）"></a>获取登录 Dashboard 的令牌 （Token）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &apos;&#123;print $1&#125;&apos;)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>k8s dashboard</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes-开发环境</title>
    <link href="/2019/05/21/Kubernetes-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"/>
    <url>/2019/05/21/Kubernetes-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</url>
    
    <content type="html"><![CDATA[<p><a href="https://kubernetes.feisky.xyz/she-qu-gong-xian/index" target="_blank" rel="noopener">k8s 搭建本地调试环境</a></p><h1 id="Kubernetes-开发环境"><a href="#Kubernetes-开发环境" class="headerlink" title="Kubernetes 开发环境"></a>Kubernetes 开发环境</h1><h2 id="配置开发环境"><a href="#配置开发环境" class="headerlink" title="配置开发环境"></a>配置开发环境</h2><p>以 Ubuntu 为例，配置一个 Kubernetes 的开发环境</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs sh">apt-get install -y gcc make socat git build-essential<br><br><span class="hljs-comment"># 安装 Docker</span><br>sh -c <span class="hljs-string">'echo"deb https://apt.dockerproject.org/repo ubuntu-$(lsb_release -cs) main"&gt; /etc/apt/sources.list.d/docker.list'</span><br>curl -fsSL https://apt.dockerproject.org/gpg | sudo apt-key add -<br>apt-key fingerprint 58118E89F3A912897C070ADBF76221572C52609D<br>apt-get update<br>apt-get -y install <span class="hljs-string">"docker-engine=1.13.1-0~ubuntu-<span class="hljs-variable">$(lsb_release -cs)</span>"</span><br><br><span class="hljs-comment"># 安装 etcd</span><br>ETCD_VER=v3.2.18<br>DOWNLOAD_URL=<span class="hljs-string">"https://github.com/coreos/etcd/releases/download"</span><br>curl -L <span class="hljs-variable">$&#123;DOWNLOAD_URL&#125;</span>/<span class="hljs-variable">$&#123;ETCD_VER&#125;</span>/etcd-<span class="hljs-variable">$&#123;ETCD_VER&#125;</span>-linux-amd64.tar.gz -o /tmp/etcd-<span class="hljs-variable">$&#123;ETCD_VER&#125;</span>-linux-amd64.tar.gz<br>tar xzvf /tmp/etcd-<span class="hljs-variable">$&#123;ETCD_VER&#125;</span>-linux-amd64.tar.gz<br>sudo /bin/cp -f etcd-<span class="hljs-variable">$&#123;ETCD_VER&#125;</span>-linux-amd64/&#123;etcd,etcdctl&#125; /usr/bin<br>rm -rf /tmp/etcd-<span class="hljs-variable">$&#123;ETCD_VER&#125;</span>-linux-amd64.tar.gz etcd-<span class="hljs-variable">$&#123;ETCD_VER&#125;</span>-linux-amd64<br><br><span class="hljs-comment"># 安装 Go</span><br>curl -sL https://storage.googleapis.com/golang/go1.10.2.linux-amd64.tar.gz | tar -C /usr/<span class="hljs-built_in">local</span> -zxf -<br><span class="hljs-built_in">export</span> GOPATH=/gopath<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:<span class="hljs-variable">$GOPATH</span>/bin:/usr/<span class="hljs-built_in">local</span>/bin:/usr/<span class="hljs-built_in">local</span>/go/bin/<br><br><span class="hljs-comment"># 下载 Kubernetes 代码</span><br>mkdir -p <span class="hljs-variable">$GOPATH</span>/src/k8s.io<br>git <span class="hljs-built_in">clone</span> https://github.com/kubernetes/kubernetes <span class="hljs-variable">$GOPATH</span>/src/k8s.io/kubernetes<br><span class="hljs-built_in">cd</span> <span class="hljs-variable">$GOPATH</span>/src/k8s.io/kubernetes<br><br><span class="hljs-comment"># 启动一个本地集群</span><br><span class="hljs-built_in">export</span> KUBERNETES_PROVIDER=<span class="hljs-built_in">local</span><br>hack/<span class="hljs-built_in">local</span>-up-cluster.sh<br></code></pre></td></tr></table></figure><p>打开另外一个终端，配置 kubectl 之后就可以开始使用了:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">cd</span> <span class="hljs-variable">$GOPATH</span>/src/k8s.io/kubernetes<br><span class="hljs-built_in">export</span> KUBECONFIG=/var/run/kubernetes/admin.kubeconfig<br>cluster/kubectl.sh<br></code></pre></td></tr></table></figure><h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p>单元测试是 Kubernetes 开发中不可缺少的，一般在代码修改的同时还要更新或添加对应的单元测试。这些单元测试大都支持在不同的系统上直接运行，比如 OSX、Linux 等。</p><p>比如，加入修改了 <code>pkg/kubelet/kuberuntime</code> 的代码后，</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 可以加上 Go package 的全路径来测试</span><br>go <span class="hljs-built_in">test</span> -v k8s.io/kubernetes/pkg/kubelet/kuberuntime<br><span class="hljs-comment"># 也可以用相对目录</span><br>go <span class="hljs-built_in">test</span> -v ./pkg/kubelet/kuberuntime<br></code></pre></td></tr></table></figure><h2 id="端到端测试"><a href="#端到端测试" class="headerlink" title="端到端测试"></a>端到端测试</h2><p>端到端（e2e）测试需要启动一个 Kubernetes 集群，仅支持在 Linux 系统上运行。</p><p>本地运行方法示例：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sh">make WHAT=<span class="hljs-string">'test/e2e/e2e.test'</span><br>make ginkgo<br><br><span class="hljs-built_in">export</span> KUBERNETES_PROVIDER=<span class="hljs-built_in">local</span><br>go run hack/e2e.go -v -<span class="hljs-built_in">test</span> --test_args=<span class="hljs-string">'--ginkgo.focus=Port\sforwarding'</span><br>go run hack/e2e.go -v -<span class="hljs-built_in">test</span> --test_args=<span class="hljs-string">'--ginkgo.focus=Feature:SecurityContext'</span><br></code></pre></td></tr></table></figure><blockquote><p>注：Kubernetes 的每个 PR 都会自动运行一系列的 e2e 测试。</p></blockquote><h2 id="Node-e2e-测试"><a href="#Node-e2e-测试" class="headerlink" title="Node e2e 测试"></a>Node e2e 测试</h2><p>Node e2e 测试需要启动 Kubelet，目前仅支持在 Linux 系统上运行。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">export</span> KUBERNETES_PROVIDER=<span class="hljs-built_in">local</span><br>make <span class="hljs-built_in">test</span>-e2e-node FOCUS=<span class="hljs-string">"InitContainer"</span><br></code></pre></td></tr></table></figure><blockquote><p>注：Kubernetes 的每个 PR 都会自动运行 node e2e 测试。</p></blockquote><h2 id="有用的-git-命令"><a href="#有用的-git-命令" class="headerlink" title="有用的 git 命令"></a>有用的 git 命令</h2><p>很多时候，我们需要把 Pull Request 拉取到本地来测试，比如拉取 Pull Request #365 的方法为</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">git fetch upstream pull/365/merge:branch-fix-1<br>git checkout branch-fix-1<br></code></pre></td></tr></table></figure><p>当然，也可以配置 <code>.git/config</code> 并运行 <code>git fetch</code> 拉取所有的 Pull Requests（注意 Kubernetes 的 Pull Requests 非常多，这个过程可能会很慢）:</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">fetch = +refs/pull/*<span class="hljs-symbol">:refs/remotes/origin/pull/*</span><br></code></pre></td></tr></table></figure><h2 id="其他参考"><a href="#其他参考" class="headerlink" title="其他参考"></a>其他参考</h2><ul><li>编译 release 版：<code>make quick-release</code></li><li>机器人命令：<a href="https://prow.k8s.io/command-help" target="_blank" rel="noopener">命令列表</a> 和 <a href="https://prow.k8s.io/plugins" target="_blank" rel="noopener">使用文档</a>。</li><li><a href="https://k8s-testgrid.appspot.com/" target="_blank" rel="noopener">Kubernetes TestGrid</a>，包含所有的测试历史</li><li><a href="https://submit-queue.k8s.io/#/queue" target="_blank" rel="noopener">Kuberentes Submit Queue Status</a>，包含所有的 Pull Request 状态以及合并队列</li><li><a href="http://node-perf-dash.k8s.io/#/builds" target="_blank" rel="noopener">Node Performance Dashboard</a>，包含 Node 组性能测试报告</li><li><a href="http://perf-dash.k8s.io/" target="_blank" rel="noopener">Kubernetes Performance Dashboard</a>，包含 Density 和 Load 测试报告</li><li><a href="https://k8s-gubernator.appspot.com/pr" target="_blank" rel="noopener">Kubernetes PR Dashboard</a>，包含主要关注的 Pull Request 列表（需要以 Github 登录）</li><li><a href="https://k8s-gubernator.appspot.com/" target="_blank" rel="noopener">Jenkins Logs</a> 和 <a href="http://prow.k8s.io/?type=presubmit" target="_blank" rel="noopener">Prow Status</a>，包含所有 Pull Request 的 Jenkins 测试日志</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>k8s ubuntu test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kubectl-命令技巧大全</title>
    <link href="/2019/05/20/kubectl-%E5%91%BD%E4%BB%A4%E6%8A%80%E5%B7%A7%E5%A4%A7%E5%85%A8/"/>
    <url>/2019/05/20/kubectl-%E5%91%BD%E4%BB%A4%E6%8A%80%E5%B7%A7%E5%A4%A7%E5%85%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="kubectl-命令技巧大全"><a href="#kubectl-命令技巧大全" class="headerlink" title="kubectl 命令技巧大全"></a>kubectl 命令技巧大全</h2><p>Kubctl 命令是操作 kubernetes 集群的最直接和最 skillful 的途径，这个60多MB大小的二进制文件，到底有啥能耐呢？请看下文：</p><h4 id="Kubectl-自动补全"><a href="#Kubectl-自动补全" class="headerlink" title="Kubectl 自动补全"></a>Kubectl 自动补全</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> &lt;(kubectl completion bash) <span class="hljs-comment"># setup autocomplete in bash, bash-completion package should be installed first.</span></span><br><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">source</span> &lt;(kubectl completion zsh)  <span class="hljs-comment"># setup autocomplete in zsh</span></span><br></code></pre></td></tr></table></figure><h3 id="Kubectl-上下文和配置"><a href="#Kubectl-上下文和配置" class="headerlink" title="Kubectl 上下文和配置"></a>Kubectl 上下文和配置</h3><p>设置 kubectl 命令交互的 kubernetes 集群并修改配置信息。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ kubectl<span class="hljs-built_in"> config </span>view # 显示合并后的 kubeconfig 配置<br><br><span class="hljs-comment"># 同时使用多个 kubeconfig 文件并查看合并后的配置</span><br>$ <span class="hljs-attribute">KUBECONFIG</span>=~/.kube/config:~/.kube/kubconfig2 kubectl<span class="hljs-built_in"> config </span>view<br><br><span class="hljs-comment"># 获取 e2e 用户的密码</span><br>$ kubectl<span class="hljs-built_in"> config </span>view -o <span class="hljs-attribute">jsonpath</span>=<span class="hljs-string">'&#123;.users[?(@.name == "e2e")].user.password&#125;'</span><br><br>$ kubectl<span class="hljs-built_in"> config </span>current-context              # 显示当前的上下文<br>$ kubectl<span class="hljs-built_in"> config </span>use-context my-cluster-name  # 设置默认上下文为 my-cluster-name<br><br><span class="hljs-comment"># 向 kubeconf 中增加支持基本认证的新集群</span><br>$ kubectl<span class="hljs-built_in"> config </span>set-credentials kubeuser/foo.kubernetes.com <span class="hljs-attribute">--username</span>=kubeuser <span class="hljs-attribute">--password</span>=kubepassword<br><br><span class="hljs-comment"># 使用指定的用户名和 namespace 设置上下文</span><br>$ kubectl<span class="hljs-built_in"> config </span>set-context gce <span class="hljs-attribute">--user</span>=cluster-admin <span class="hljs-attribute">--namespace</span>=foo \<br>  &amp;&amp; kubectl<span class="hljs-built_in"> config </span>use-context gce<br></code></pre></td></tr></table></figure><h3 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h3><p>Kubernetes 的清单文件可以使用 json 或 yaml 格式定义。可以以 .yaml、.yml、或者 .json 为扩展名。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-string">$</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">create</span> <span class="hljs-bullet">-f</span> <span class="hljs-string">./my-manifest.yaml</span>           <span class="hljs-comment"># 创建资源</span><br><span class="hljs-string">$</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">create</span> <span class="hljs-bullet">-f</span> <span class="hljs-string">./my1.yaml</span> <span class="hljs-bullet">-f</span> <span class="hljs-string">./my2.yaml</span>     <span class="hljs-comment"># 使用多个文件创建资源</span><br><span class="hljs-string">$</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">create</span> <span class="hljs-bullet">-f</span> <span class="hljs-string">./dir</span>                        <span class="hljs-comment"># 使用目录下的所有清单文件来创建资源</span><br><span class="hljs-string">$</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">create</span> <span class="hljs-bullet">-f</span> <span class="hljs-attr">https://git.io/vPieo</span>         <span class="hljs-comment"># 使用 url 来创建资源</span><br><span class="hljs-string">$</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">run</span> <span class="hljs-string">nginx</span> <span class="hljs-bullet">--image=nginx</span>                <span class="hljs-comment"># 启动一个 nginx 实例</span><br><span class="hljs-string">$</span> <span class="hljs-string">kubectl</span> <span class="hljs-string">explain</span> <span class="hljs-string">pods,svc</span>                       <span class="hljs-comment"># 获取 pod 和 svc 的文档</span><br><br><span class="hljs-comment"># 从 stdin 输入中创建多个 YAML 对象</span><br><span class="hljs-string">$</span> <span class="hljs-string">cat</span> <span class="hljs-string">&lt;&lt;EOF</span> <span class="hljs-string">| kubectl create -f -<br></span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">busybox-sleep</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  containers:</span><br><span class="hljs-attr">  - name:</span> <span class="hljs-string">busybox</span><br><span class="hljs-attr">    image:</span> <span class="hljs-string">busybox</span><br><span class="hljs-attr">    args:</span><br><span class="hljs-bullet">    -</span> <span class="hljs-string">sleep</span><br><span class="hljs-bullet">    -</span> <span class="hljs-string">"1000000"</span><br><span class="hljs-meta">---</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">busybox-sleep-less</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  containers:</span><br><span class="hljs-attr">  - name:</span> <span class="hljs-string">busybox</span><br><span class="hljs-attr">    image:</span> <span class="hljs-string">busybox</span><br><span class="hljs-attr">    args:</span><br><span class="hljs-bullet">    -</span> <span class="hljs-string">sleep</span><br><span class="hljs-bullet">    -</span> <span class="hljs-string">"1000"</span><br><span class="hljs-string">EOF</span><br><br><span class="hljs-comment"># 创建包含几个 key 的 Secret</span><br><span class="hljs-string">$</span> <span class="hljs-string">cat</span> <span class="hljs-string">&lt;&lt;EOF</span> <span class="hljs-string">| kubectl create -f -<br></span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Secret</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">mysecret</span><br><span class="hljs-attr">type:</span> <span class="hljs-string">Opaque</span><br><span class="hljs-attr">data:</span><br><span class="hljs-attr">  password:</span> <span class="hljs-string">$(echo</span> <span class="hljs-string">"s33msi4"</span> <span class="hljs-string">| base64)<br></span><span class="hljs-attr">  username:</span> <span class="hljs-string">$(echo</span> <span class="hljs-string">"jane"</span> <span class="hljs-string">| base64)<br>EOF</span><br></code></pre></td></tr></table></figure></p><h3 id="显示和查找资源"><a href="#显示和查找资源" class="headerlink" title="显示和查找资源"></a>显示和查找资源</h3><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment"># Get commands with basic output</span><br>$ kubectl <span class="hljs-keyword">get</span> services                          <span class="hljs-comment"># 列出所有 namespace 中的所有 service</span><br>$ kubectl <span class="hljs-keyword">get</span> pods <span class="hljs-comment">--all-namespaces             # 列出所有 namespace 中的所有 pod</span><br>$ kubectl <span class="hljs-keyword">get</span> pods -o wide                      <span class="hljs-comment"># 列出所有 pod 并显示详细信息</span><br>$ kubectl <span class="hljs-keyword">get</span> deployment <span class="hljs-keyword">my</span>-dep                 <span class="hljs-comment"># 列出指定 deployment</span><br>$ kubectl <span class="hljs-keyword">get</span> pods <span class="hljs-comment">--include-uninitialized      # 列出该 namespace 中的所有 pod 包括未初始化的</span><br><br><span class="hljs-comment"># 使用详细输出来描述命令</span><br>$ kubectl describe nodes <span class="hljs-keyword">my</span>-node<br>$ kubectl describe pods <span class="hljs-keyword">my</span>-pod<br><br>$ kubectl <span class="hljs-keyword">get</span> services <span class="hljs-comment">--sort-by=.metadata.name # List Services Sorted by Name</span><br><br><span class="hljs-comment"># 根据重启次数排序列出 pod</span><br>$ kubectl <span class="hljs-keyword">get</span> pods <span class="hljs-comment">--sort-by='.status.containerStatuses[0].restartCount'</span><br><br><span class="hljs-comment"># 获取所有具有 app=cassandra 的 pod 中的 version 标签</span><br>$ kubectl <span class="hljs-keyword">get</span> pods <span class="hljs-comment">--selector=app=cassandra rc -o \</span><br>  jsonpath='&#123;.items[*].metadata.labels.<span class="hljs-built_in">version</span>&#125;'<br><br><span class="hljs-comment"># 获取所有节点的 ExternalIP</span><br>$ kubectl <span class="hljs-keyword">get</span> nodes -o jsonpath='&#123;.items[*].status.addresses[?(@.type==<span class="hljs-string">"ExternalIP"</span>)].address&#125;'<br><br><span class="hljs-comment"># 列出属于某个 PC 的 Pod 的名字</span><br><span class="hljs-comment"># “jq”命令用于转换复杂的 jsonpath，参考 https://stedolan.github.io/jq/</span><br>$ sel=$&#123;$(kubectl <span class="hljs-keyword">get</span> rc <span class="hljs-keyword">my</span>-rc <span class="hljs-comment">--output=json | jq -j '.spec.selector | to_entries | .[] | "\(.key)=\(.value),"')%?&#125;</span><br>$ echo $(kubectl <span class="hljs-keyword">get</span> pods <span class="hljs-comment">--selector=$sel --output=jsonpath=&#123;.items..metadata.name&#125;)</span><br><br><span class="hljs-comment"># 查看哪些节点已就绪</span><br>$ JSONPATH='&#123;range .items[*]&#125;&#123;@.metadata.<span class="hljs-built_in">name</span>&#125;:&#123;range @.status.conditions[*]&#125;&#123;@.type&#125;=&#123;@.status&#125;;&#123;<span class="hljs-keyword">end</span>&#125;&#123;<span class="hljs-keyword">end</span>&#125;' \<br> &amp;&amp; kubectl <span class="hljs-keyword">get</span> nodes -o jsonpath=<span class="hljs-string">"$JSONPATH"</span> | grep <span class="hljs-string">"Ready=True"</span><br><br><span class="hljs-comment"># 列出当前 Pod 中使用的 Secret</span><br>$ kubectl <span class="hljs-keyword">get</span> pods -o json | jq '.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.<span class="hljs-built_in">name</span>' | grep -v null | sort | uniq<br></code></pre></td></tr></table></figure><h3 id="更新资源"><a href="#更新资源" class="headerlink" title="更新资源"></a>更新资源</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ kubectl rolling-<span class="hljs-keyword">update</span> frontend-v1 -f frontend-v2.json           <span class="hljs-comment"># 滚动更新 pod frontend-v1</span><br>$ kubectl <span class="hljs-keyword">rolling</span>-<span class="hljs-keyword">update</span> frontend-v1 frontend-v2 <span class="hljs-comment">--image=image:v2  # 更新资源名称并更新镜像</span><br>$ kubectl <span class="hljs-keyword">rolling</span>-<span class="hljs-keyword">update</span> frontend <span class="hljs-comment">--image=image:v2                 # 更新 frontend pod 中的镜像</span><br>$ kubectl <span class="hljs-keyword">rolling</span>-<span class="hljs-keyword">update</span> frontend-v1 frontend-v2 <span class="hljs-comment">--rollback        # 退出已存在的进行中的滚动更新</span><br>$ cat pod.json | kubectl <span class="hljs-keyword">replace</span> -f -                              <span class="hljs-comment"># 基于 stdin 输入的 JSON 替换 pod</span><br><br><span class="hljs-comment"># 强制替换，删除后重新创建资源。会导致服务中断。</span><br>$ kubectl <span class="hljs-keyword">replace</span> <span class="hljs-comment">--force -f ./pod.json</span><br><br><span class="hljs-comment"># 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口</span><br>$ kubectl expose rc nginx <span class="hljs-comment">--port=80 --target-port=8000</span><br><br><span class="hljs-comment"># 更新单容器 pod 的镜像版本（tag）到 v4</span><br>$ kubectl <span class="hljs-keyword">get</span> pod mypod -o yaml | sed <span class="hljs-string">'s/\(image: myimage\):.*$/\1:v4/'</span> | kubectl <span class="hljs-keyword">replace</span> -f -<br><br>$ kubectl label pods my-pod <span class="hljs-keyword">new</span>-label=awesome                      <span class="hljs-comment"># 添加标签</span><br>$ kubectl annotate pods my-pod icon-<span class="hljs-keyword">url</span>=<span class="hljs-keyword">http</span>://goo.gl/XXBTWq       <span class="hljs-comment"># 添加注解</span><br>$ kubectl autoscale deployment foo <span class="hljs-comment">--min=2 --max=10                # 自动扩展 deployment “foo”</span><br></code></pre></td></tr></table></figure><h3 id="修补资源"><a href="#修补资源" class="headerlink" title="修补资源"></a>修补资源</h3><p>使用策略合并补丁并修补资源。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ kubectl patch <span class="hljs-keyword">node</span> <span class="hljs-title">k8s-node-1</span> -p '&#123;<span class="hljs-string">"spec"</span>:&#123;<span class="hljs-string">"unschedulable"</span>:<span class="hljs-literal">true</span>&#125;&#125;' <span class="hljs-comment"># 部分更新节点</span><br><br><span class="hljs-comment"># 更新容器镜像； spec.containers[*].name 是必须的，因为这是合并的关键字</span><br>$ kubectl patch pod valid-pod -p '&#123;<span class="hljs-string">"spec"</span>:&#123;<span class="hljs-string">"containers"</span>:[&#123;<span class="hljs-string">"name"</span>:<span class="hljs-string">"kubernetes-serve-hostname"</span>,<span class="hljs-string">"image"</span>:<span class="hljs-string">"new image"</span>&#125;]&#125;&#125;'<br><br><span class="hljs-comment"># 使用具有位置数组的 json 补丁更新容器镜像</span><br>$ kubectl patch pod valid-pod --<span class="hljs-attr">type=</span>'json' -p='[&#123;<span class="hljs-string">"op"</span>: <span class="hljs-string">"replace"</span>, <span class="hljs-string">"path"</span>: <span class="hljs-string">"/spec/containers/0/image"</span>, <span class="hljs-string">"value"</span>:<span class="hljs-string">"new image"</span>&#125;]'<br><br><span class="hljs-comment"># 使用具有位置数组的 json 补丁禁用 deployment 的 livenessProbe</span><br>$ kubectl patch deployment valid-deployment  --<span class="hljs-keyword">type</span> json   -p='[&#123;<span class="hljs-string">"op"</span>: <span class="hljs-string">"remove"</span>, <span class="hljs-string">"path"</span>: <span class="hljs-string">"/spec/template/spec/containers/0/livenessProbe"</span>&#125;]'<br></code></pre></td></tr></table></figure><h3 id="编辑资源"><a href="#编辑资源" class="headerlink" title="编辑资源"></a>编辑资源</h3><p>在编辑器中编辑任何 API 资源。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ kubectl <span class="hljs-builtin-name">edit</span> svc/docker-registry                      # 编辑名为 docker-registry 的<span class="hljs-built_in"> service<br></span>$ <span class="hljs-attribute">KUBE_EDITOR</span>=<span class="hljs-string">"nano"</span> kubectl <span class="hljs-builtin-name">edit</span> svc/docker-registry   # 使用其它编辑器<br></code></pre></td></tr></table></figure></p><h3 id="Scale-资源"><a href="#Scale-资源" class="headerlink" title="Scale 资源"></a>Scale 资源</h3><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ <span class="hljs-string">kubectl </span><span class="hljs-string">scale </span><span class="hljs-built_in">--replicas=3</span> <span class="hljs-string">rs/</span><span class="hljs-string">foo </span>                                <span class="hljs-comment"># Scale a replicaset named 'foo' to 3</span><br>$ <span class="hljs-string">kubectl </span><span class="hljs-string">scale </span><span class="hljs-built_in">--replicas=3</span> -f <span class="hljs-string">foo.</span><span class="hljs-string">yaml </span>                           <span class="hljs-comment"># Scale a resource specified in "foo.yaml" to 3</span><br>$ <span class="hljs-string">kubectl </span><span class="hljs-string">scale </span><span class="hljs-built_in">--current-replicas=2</span> <span class="hljs-built_in">--replicas=3</span> <span class="hljs-string">deployment/</span><span class="hljs-string">mysql </span> <span class="hljs-comment"># If the deployment named mysql's current size is 2, scale mysql to 3</span><br>$ <span class="hljs-string">kubectl </span><span class="hljs-string">scale </span><span class="hljs-built_in">--replicas=5</span> <span class="hljs-string">rc/</span><span class="hljs-string">foo </span><span class="hljs-string">rc/</span><span class="hljs-string">bar </span><span class="hljs-string">rc/</span><span class="hljs-string">baz </span>                  <span class="hljs-comment"># Scale multiple replication controllers</span><br></code></pre></td></tr></table></figure><h3 id="删除资源"><a href="#删除资源" class="headerlink" title="删除资源"></a>删除资源</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ kubectl delete -f ./pod.json                                              # 删除 pod.json 文件中定义的类型和名称的 pod<br>$ kubectl delete pod,service baz foo                                        # 删除名为“baz”的 pod 和名为“foo”的<span class="hljs-built_in"> service<br></span>$ kubectl delete pods,services -l <span class="hljs-attribute">name</span>=myLabel                              # 删除具有 <span class="hljs-attribute">name</span>=myLabel 标签的 pod 和 serivce<br>$ kubectl delete pods,services -l <span class="hljs-attribute">name</span>=myLabel --include-uninitialized      # 删除具有 <span class="hljs-attribute">name</span>=myLabel 标签的 pod 和 service，包括尚未初始化的<br>$ kubectl -n my-ns delete po,svc --all                                      # 删除 my-ns namespace 下的所有 pod 和 serivce，包括尚未初始化的<br></code></pre></td></tr></table></figure><h3 id="与运行中的-Pod-交互"><a href="#与运行中的-Pod-交互" class="headerlink" title="与运行中的 Pod 交互"></a>与运行中的 Pod 交互</h3><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ kubectl logs <span class="hljs-keyword">my</span>-pod                                 <span class="hljs-comment"># dump 输出 pod 的日志（stdout）</span><br>$ kubectl logs <span class="hljs-keyword">my</span>-pod -c <span class="hljs-keyword">my</span>-container                 <span class="hljs-comment"># dump 输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用）</span><br>$ kubectl logs -f <span class="hljs-keyword">my</span>-pod                              <span class="hljs-comment"># 流式输出 pod 的日志（stdout）</span><br>$ kubectl logs -f <span class="hljs-keyword">my</span>-pod -c <span class="hljs-keyword">my</span>-container              <span class="hljs-comment"># 流式输出 pod 中容器的日志（stdout，pod 中有多个容器的情况下使用）</span><br>$ kubectl run -i --tty busybox --image=busybox -- sh  <span class="hljs-comment"># 交互式 shell 的方式运行 pod</span><br>$ kubectl attach <span class="hljs-keyword">my</span>-pod -i                            <span class="hljs-comment"># 连接到运行中的容器</span><br>$ kubectl port-forward <span class="hljs-keyword">my</span>-pod <span class="hljs-number">5000</span>:<span class="hljs-number">6000</span>               <span class="hljs-comment"># 转发 pod 中的 6000 端口到本地的 5000 端口</span><br>$ kubectl <span class="hljs-keyword">exec</span> <span class="hljs-keyword">my</span>-pod -- ls /                         <span class="hljs-comment"># 在已存在的容器中执行命令（只有一个容器的情况下）</span><br>$ kubectl <span class="hljs-keyword">exec</span> <span class="hljs-keyword">my</span>-pod -c <span class="hljs-keyword">my</span>-container -- ls /         <span class="hljs-comment"># 在已存在的容器中执行命令（pod 中有多个容器的情况下）</span><br>$ kubectl top pod POD_NAME --containers               <span class="hljs-comment"># 显示指定 pod 和容器的指标度量</span><br></code></pre></td></tr></table></figure><h3 id="与节点和集群交互"><a href="#与节点和集群交互" class="headerlink" title="与节点和集群交互"></a>与节点和集群交互</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ kubectl cordon my-<span class="hljs-keyword">node</span>                                                <span class="hljs-title"># 标记 my-node</span> 不可调度<br>$ kubectl drain my-<span class="hljs-keyword">node</span>                                                 <span class="hljs-title"># 清空 my-node</span> 以待维护<br>$ kubectl uncordon my-<span class="hljs-keyword">node</span>                                              <span class="hljs-title"># 标记 my-node</span> 可调度<br>$ kubectl top <span class="hljs-keyword">node</span> <span class="hljs-title">my-node</span>                                              <span class="hljs-comment"># 显示 my-node 的指标度量</span><br>$ kubectl cluster<span class="hljs-literal">-inf</span>o                                                  <span class="hljs-comment"># 显示 master 和服务的地址</span><br>$ kubectl cluster<span class="hljs-literal">-inf</span>o dump                                             <span class="hljs-comment"># 将当前集群状态输出到 stdout                                    </span><br>$ kubectl cluster<span class="hljs-literal">-inf</span>o dump --<span class="hljs-attr">output-directory=</span>/path/to/cluster-state   <span class="hljs-comment"># 将当前集群状态输出到 /path/to/cluster-state</span><br><span class="hljs-comment"># 如果该键和影响的污点（taint）已存在，则使用指定的值替换</span><br>$ kubectl taint nodes foo <span class="hljs-attr">dedicated=</span>special-user:NoSchedule<br></code></pre></td></tr></table></figure><h3 id="资源类型"><a href="#资源类型" class="headerlink" title="资源类型"></a>资源类型</h3><p>下表列出的是 kubernetes 中所有支持的类型和缩写的别名。</p><table><thead><tr><th>资源类型</th><th>缩写别名</th></tr></thead><tbody><tr><td>clusters</td><td></td></tr><tr><td>componentstatuses</td><td>cs</td></tr><tr><td>configmaps</td><td>cm</td></tr><tr><td>daemonsets</td><td>ds</td></tr><tr><td>deployments</td><td>deploy</td></tr><tr><td>endpoints</td><td>ep</td></tr><tr><td>event</td><td>ev</td></tr><tr><td>horizontalpodautoscalers</td><td>hpa</td></tr><tr><td>ingresses</td><td>ing</td></tr><tr><td>jobs</td><td></td></tr><tr><td>limitranges</td><td>limits</td></tr><tr><td>namespaces</td><td>ns</td></tr><tr><td>networkpolicies</td><td></td></tr><tr><td>nodes</td><td>no</td></tr><tr><td>statefulsets</td><td></td></tr><tr><td>persistentvolumeclaims</td><td>pvc</td></tr><tr><td>persistentvolumes</td><td>pv</td></tr><tr><td>pods</td><td>po</td></tr><tr><td>podsecuritypolicies</td><td>psp</td></tr><tr><td>podtemplates</td></tr><tr><td>replicasets</td><td>rs</td></tr><tr><td>replicationcontrollers</td><td>rc</td></tr><tr><td>resourcequotas</td><td>quota</td></tr><tr><td>cronjob</td></tr><tr><td>secrets</td></tr><tr><td>serviceaccount</td><td>sa</td></tr><tr><td>services</td><td>svc</td></tr><tr><td>storageclasses</td></tr><tr><td>thirdpartyresources</td></tr></tbody></table><h3 id="格式化输出"><a href="#格式化输出" class="headerlink" title="格式化输出"></a>格式化输出</h3><p>要以特定的格式向终端窗口输出详细信息，可以在 kubectl 命令中添加 -o 或者 -output 标志。</p><table><thead><tr><th>输出格式</th><th>描述</th></tr></thead><tbody><tr><td>-o=custom-columns=<spec></spec></td><td>使用逗号分隔的自定义列列表打印表格</td></tr><tr><td>-o=custom-columns-file=<filename></filename></td><td>使用 文件中的自定义列模板打印表格</td></tr><tr><td>-o=json</td><td>输出 JSON 格式的 API 对象</td></tr><tr><td>-o=jsonpath-file=’filename’</td><td>打印 jsonpath 表达式中定义的字段</td></tr><tr><td>-o=jsonpath-file=’filename’</td><td>打印由 文件中的 jsonpath 表达式定义的字段</td></tr><tr><td>-o=name</td><td>仅打印资源名称</td></tr><tr><td>-o=wide</td><td>以纯文本格式输出任何附加信息，对于 Pod ，包含节点名称</td></tr><tr><td>-o=yaml</td><td>输出 YAML 格式的 API 对象</td></tr></tbody></table><h3 id="Kubectl-详细输出和调试"><a href="#Kubectl-详细输出和调试" class="headerlink" title="Kubectl 详细输出和调试"></a>Kubectl 详细输出和调试</h3><p>使用 -v 或 –v 标志跟着一个整数来指定日志级别。这里 描述了通用的 kubernetes 日志约定和相关的日志级别。</p><table><thead><tr><th>详细等级</th><th>描述</th></tr></thead><tbody><tr><td>–v=0</td><td>总是对操作人员可见。</td></tr><tr><td>–v=1</td><td>合理的默认日志级别，如果您不需要详细输出。</td></tr><tr><td>–v=2</td><td>可能与系统的重大变化相关的，有关稳定状态的信息和重要的日志信息。这是对大多数系统推荐的日志级别。</td></tr><tr><td>–v=3</td><td>有关更改的扩展信息。</td></tr><tr><td>–v=4</td><td>调试级别详细输出。</td></tr><tr><td>–v=6</td><td>显示请求的资源。</td></tr><tr><td>–v=7</td><td>显示HTTP请求的header。</td></tr><tr><td>–v=8</td><td>显示HTTP请求的内容。</td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>k8s kubectl</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubeadm入门</title>
    <link href="/2019/05/20/Kubeadm%E5%85%A5%E9%97%A8/"/>
    <url>/2019/05/20/Kubeadm%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="第1步-初始化Master"><a href="#第1步-初始化Master" class="headerlink" title="第1步 - 初始化Master"></a>第1步 - 初始化Master</h3><p>Kubeadm已安装在节点上。软件包适用于Ubuntu 16.04 +，CentOS 7或HypriotOS v1.0.1 +。</p><p>初始化集群的第一个阶段是启动主节点。主服务器负责运行控制平面组件，etcd和API服务器。客户端将与API通信以调度工作负载并管理群集的状态。</p><p>任务<br>以下命令将使用已知令牌初始化集群，以简化以下步骤。<br><figure class="highlight d"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubeadm init --token=<span class="hljs-number">102952.1</span>a7dd4cc8d1f4cc5 --kubernetes-<span class="hljs-keyword">version</span> $(kubeadm <span class="hljs-keyword">version</span> -o <span class="hljs-built_in">short</span>)<br></code></pre></td></tr></table></figure></p><p>在生产中，建议排除令牌，导致kubeadm代表您生成一个令牌。</p><p>要管理Kubernetes集群，需要客户端配置和证书。当kubeadm初始化集群时，将创建此配置。该命令将配置复制到用户主目录，并设置环境变量以供CLI使用。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">sudo cp /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/<br>sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/admin.conf<br><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">KUBECONFIG</span>=<span class="hljs-variable">$HOME</span>/admin.conf<br></code></pre></td></tr></table></figure></p><h3 id="第2步-加入群集"><a href="#第2步-加入群集" class="headerlink" title="第2步 - 加入群集"></a>第2步 - 加入群集</h3><p>Master初始化后，只要具有正确的令牌，其他节点就可以加入群集。kubeadm token例如，可以通过管理令牌kubeadm token list。</p><p>任务<br>在第二个节点上，运行命令以加入提供主节点IP地址的群集。<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kub<br>```eadm <span class="hljs-keyword">join</span> --discovery-token-<span class="hljs-keyword">unsafe</span>-<span class="hljs-keyword">skip</span>-ca-verification --token=<span class="hljs-number">102952.1</span>a7dd4cc8d1f4cc5 <span class="hljs-number">172.17</span>.<span class="hljs-number">0.100</span>:<span class="hljs-number">6443</span><br></code></pre></td></tr></table></figure></p><p>这与Master初始化后提供的命令相同。</p><p>该–discovery-token-unsafe-skip-ca-verification标记用于绕过发现令牌验证。由于此令牌是动态生成的，因此我们无法将其包含在步骤中。在生产中，使用提供的令牌kubeadm init。</p><h3 id="第3步-查看节点"><a href="#第3步-查看节点" class="headerlink" title="第3步 - 查看节点"></a>第3步 - 查看节点</h3><p>群集现已初始化。主节点将管理集群，而我们的一个工作节点将运行我们的容器工作负载。</p><p>任务<br>Kubernetes CLI（称为kubectl）现在可以使用该配置来访问群集。例如，以下命令将返回群集中的两个节点。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-builtin-name">get</span> nodes<br></code></pre></td></tr></table></figure></p><p>此时，节点将不会准备就绪。</p><p>这是因为尚未部署容器网络接口。这将在下一步中修复。</p><h3 id="第4步-部署容器网络接口（CNI）"><a href="#第4步-部署容器网络接口（CNI）" class="headerlink" title="第4步 - 部署容器网络接口（CNI）"></a>第4步 - 部署容器网络接口（CNI）</h3><p>容器网络接口（CNI）定义了不同节点及其工作负载应如何通信。有多个网络提供商可用，其中一些列这里。</p><p>任务<br>在这种情况下，我们将使用WeaveWorks。可以在以下位置查看部署定义<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-keyword">cat</span> /<span class="hljs-keyword">opt</span>/weave-kube<br></code></pre></td></tr></table></figure></p><p>这可以使用kubectl apply。<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-symbol">kubectl</span> apply -f /<span class="hljs-meta">opt</span>/weave-kube<br></code></pre></td></tr></table></figure></p><p>Weave现在将作为一系列Pod部署在集群上。可以使用该命令查看此状态<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-built_in">get</span> pod -n kube-<span class="hljs-keyword">system</span><br></code></pre></td></tr></table></figure></p><p>在群集上安装Weave时，请访问<a href="https://www.weave.works/docs/net/latest/kube-addon/" target="_blank" rel="noopener"> https://www.weave.works/docs/net/latest/kube-addon/</a>了解详细信息。</p><h3 id="第5步-部署Pod"><a href="#第5步-部署Pod" class="headerlink" title="第5步 - 部署Pod"></a>第5步 - 部署Pod</h3><p>现在，群集中两个节点的状态应为Ready。这意味着我们可以安排和启动我们的部署。</p><p>使用Kubectl，可以部署pod。始终为主服务器发出命令，每个节点仅负责执行工作负载。</p><p>下面的命令基于Docker Image katacoda / docker-http-server创建一个Pod 。<br><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-built_in">create</span> deployment <span class="hljs-keyword">http</span> <span class="hljs-comment">--image=katacoda/docker-http-server:latest</span><br></code></pre></td></tr></table></figure></p><p>可以使用查看Pod创建的状态<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-builtin-name">get</span> pods<br></code></pre></td></tr></table></figure></p><p>运行后，您可以看到节点上运行的Docker Container。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">docker <span class="hljs-keyword">ps</span> | <span class="hljs-keyword">grep</span> docker-http-server<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
    <tags>
      
      <tag>k8s kubeadm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>K8S的三个网段</title>
    <link href="/2019/05/20/K8S%E7%9A%84%E4%B8%89%E4%B8%AA%E7%BD%91%E6%AE%B5/"/>
    <url>/2019/05/20/K8S%E7%9A%84%E4%B8%89%E4%B8%AA%E7%BD%91%E6%AE%B5/</url>
    
    <content type="html"><![CDATA[<h2 id="K8S的三个网段"><a href="#K8S的三个网段" class="headerlink" title="K8S的三个网段"></a>K8S的三个网段</h2><h4 id="node网段"><a href="#node网段" class="headerlink" title="node网段"></a>node网段</h4><hr><p>主要负责K8S集群中的各个node之间通信的网络；<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-symbol">root@zenlin:</span>~# kubectl get nodes<br>NAME          STATUS    AGE       VERSION<br>zenlin        Ready     <span class="hljs-number">19d</span>       v1<span class="hljs-meta">.7</span><span class="hljs-meta">.3</span><br>zenlinnode1   Ready     <span class="hljs-number">19d</span>       v1<span class="hljs-meta">.7</span><span class="hljs-meta">.3</span><br>zenlinnode2   Ready     <span class="hljs-number">19d</span>       v1<span class="hljs-meta">.7</span><span class="hljs-meta">.3</span><br></code></pre></td></tr></table></figure></p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment">#以下为各node的物理IP</span><br>root<span class="hljs-variable">@zenlin</span><span class="hljs-symbol">:~</span><span class="hljs-comment"># kubectl get nodes zenlin -o 'jsonpath=&#123;.status.addresses[0].address&#125;' </span><br><span class="hljs-number">10.229</span>.<span class="hljs-number">43.65</span><br>root<span class="hljs-variable">@zenlin</span><span class="hljs-symbol">:~</span><span class="hljs-comment"># kubectl get nodes zenlinnode1 -o 'jsonpath=&#123;.status.addresses[0].address&#125;'</span><br><span class="hljs-number">10.229</span>.<span class="hljs-number">53.146</span><br>root<span class="hljs-variable">@zenlin</span><span class="hljs-symbol">:~</span><span class="hljs-comment"># kubectl get nodes zenlinnode2 -o 'jsonpath=&#123;.status.addresses[0].address</span><br><span class="hljs-number">10.229</span>.<span class="hljs-number">45.161</span><br>root<span class="hljs-variable">@zenlin</span><span class="hljs-symbol">:~</span><span class="hljs-comment"># kubectl get nodes zenlin -o 'jsonpath=&#123;.spec.podCIDR&#125;'</span><br><span class="hljs-number">10.244</span>.<span class="hljs-number">0</span>.<span class="hljs-number">0</span>/<span class="hljs-number">24</span><br>root<span class="hljs-variable">@zenlin</span><span class="hljs-symbol">:~</span><span class="hljs-comment"># kubectl get nodes zenlinnode1 -o 'jsonpath=&#123;.spec.podCIDR&#125;'</span><br><span class="hljs-number">10.244</span>.<span class="hljs-number">1.0</span>/<span class="hljs-number">24</span><br>root<span class="hljs-variable">@zenlin</span><span class="hljs-symbol">:~</span><span class="hljs-comment"># kubectl get nodes zenlinnode2 -o 'jsonpath=&#123;.spec.podCIDR&#125;'</span><br><span class="hljs-number">10.244</span>.<span class="hljs-number">2.0</span>/<span class="hljs-number">24</span><br></code></pre></td></tr></table></figure><p>这是K8S的每个node上的podCIDR，用于给每个pod设置ip连接到node的网桥上，这将会在后面的章节中用到。</p><h4 id="service网段"><a href="#service网段" class="headerlink" title="service网段"></a>service网段</h4><p>每个新创建的service都会分配到一个service的cluster IP，在笔者的集群中为10.0.0.0这个网段内分配。<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-symbol">root@zenlin:</span>~# kubectl get svc<br>NAME            CLUSTER-<span class="hljs-built_in">IP</span>       EXTERNAL-<span class="hljs-built_in">IP</span>   PORT(S)                      AGE<br>details         <span class="hljs-number">10.111</span><span class="hljs-meta">.84</span><span class="hljs-meta">.67</span>     &lt;none&gt;        <span class="hljs-number">9080</span>/TCP                     <span class="hljs-number">3d</span><br>grafana         <span class="hljs-number">10.110</span><span class="hljs-meta">.131</span><span class="hljs-meta">.254</span>   &lt;none&gt;        <span class="hljs-number">3000</span>/TCP                     <span class="hljs-number">7d</span><br>istio-egress    <span class="hljs-number">10.99</span><span class="hljs-meta">.236</span><span class="hljs-meta">.93</span>     &lt;none&gt;        <span class="hljs-number">80</span>/TCP                       <span class="hljs-number">7d</span><br>kubernetes      <span class="hljs-number">10.96</span><span class="hljs-meta">.0</span><span class="hljs-meta">.1</span>        &lt;none&gt;        <span class="hljs-number">443</span>/TCP                      <span class="hljs-number">19d</span><br>nginx-service   <span class="hljs-number">10.105</span><span class="hljs-meta">.211</span><span class="hljs-meta">.16</span>    &lt;none&gt;        <span class="hljs-number">8000</span>/TCP                     <span class="hljs-number">9d</span><br>php-apache      <span class="hljs-number">10.106</span><span class="hljs-meta">.156</span><span class="hljs-meta">.64</span>    &lt;none&gt;        <span class="hljs-number">80</span>/TCP                       <span class="hljs-number">13d</span><br>productpage     <span class="hljs-number">10.104</span><span class="hljs-meta">.17</span><span class="hljs-meta">.86</span>     &lt;none&gt;        <span class="hljs-number">9080</span>/TCP                     <span class="hljs-number">3d</span><br>prometheus      <span class="hljs-number">10.109</span><span class="hljs-meta">.48</span><span class="hljs-meta">.180</span>    &lt;none&gt;        <span class="hljs-number">9090</span>/TCP                     <span class="hljs-number">7d</span><br>ratings         <span class="hljs-number">10.100</span><span class="hljs-meta">.168</span><span class="hljs-meta">.14</span>    &lt;none&gt;        <span class="hljs-number">9080</span>/TCP                     <span class="hljs-number">3d</span><br>reviews         <span class="hljs-number">10.110</span><span class="hljs-meta">.130</span><span class="hljs-meta">.11</span>    &lt;none&gt;        <span class="hljs-number">9080</span>/TCP                     <span class="hljs-number">3d</span><br>servicegraph    <span class="hljs-number">10.104</span><span class="hljs-meta">.205</span><span class="hljs-meta">.192</span>   &lt;none&gt;        <span class="hljs-number">8088</span>/TCP                     <span class="hljs-number">7d</span><br>zipkin          <span class="hljs-number">10.105</span><span class="hljs-meta">.183</span><span class="hljs-meta">.37</span>    &lt;nodes&gt;       <span class="hljs-number">9411</span>:<span class="hljs-number">30305</span>/TCP               <span class="hljs-number">3d</span><br></code></pre></td></tr></table></figure></p><h4 id="pod网段"><a href="#pod网段" class="headerlink" title="pod网段"></a>pod网段</h4><p>在笔者环境中，安装了fannal隧道网络，则pod网络其实就是fannel 网络，以帮助不同node之间的pod之间进行通信，fannel使整个K8S集群网络扁平化，不管是在node内还是node之间，pod之间的通信都可以通过pod IP进行。</p><p>可以看到环境中，pod的网络都位于10.244.0.0/16网段内，所有的pod将会被分配在这个网段。<br><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-symbol">root@zenlin:</span>~# kubectl get po -owide<br>NAME                                        READY     STATUS             RESTARTS   AGE       <span class="hljs-built_in">IP</span>             NODE<br>details-v1-<span class="hljs-number">3006205406</span>-dnm69                 <span class="hljs-number">2</span>/<span class="hljs-number">2</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">3d</span>        <span class="hljs-number">10.244</span><span class="hljs-meta">.1</span><span class="hljs-meta">.137</span>   zenlinnode1<br>grafana-<span class="hljs-number">1011650190</span>-1f9h4                    <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">7d</span>        <span class="hljs-number">10.244</span><span class="hljs-meta">.1</span><span class="hljs-meta">.114</span>   zenlinnode1<br>nginx-deployment-<span class="hljs-number">1885164871</span>-dzls7           <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">9d</span>        <span class="hljs-number">10.244</span><span class="hljs-meta">.1</span><span class="hljs-meta">.53</span>    zenlinnode1<br>nginx-deployment-<span class="hljs-number">1885164871</span>-q37zh           <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">9d</span>        <span class="hljs-number">10.244</span><span class="hljs-meta">.2</span><span class="hljs-meta">.199</span>   zenlinnode2<br>productpage-v1-<span class="hljs-number">4256385220</span>-nvq0d             <span class="hljs-number">2</span>/<span class="hljs-number">2</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">3d</span>        <span class="hljs-number">10.244</span><span class="hljs-meta">.2</span><span class="hljs-meta">.240</span>   zenlinnode2<br>prometheus-<span class="hljs-number">4245872192</span>-xz47s                 <span class="hljs-number">1</span>/<span class="hljs-number">1</span>       Running            <span class="hljs-number">0</span>          <span class="hljs-number">7d</span><br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
    <tags>
      
      <tag>k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用kubectlCLI启动集群</title>
    <link href="/2019/05/20/%E4%BD%BF%E7%94%A8kubectlCLI%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4/"/>
    <url>/2019/05/20/%E4%BD%BF%E7%94%A8kubectlCLI%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h3 id="使用kubectl-CLI-启动集群"><a href="#使用kubectl-CLI-启动集群" class="headerlink" title="使用kubectl CLI 启动集群"></a>使用kubectl CLI 启动集群</h3><ol><li>可以使用kubectl CLI 与集群进行交互。这是用于管理Kubernetes和在群集上运行的应用程序的主要方法。</li><li><p>可以通过以下方式发现群集及其运行状况的详细信息 </p> <figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">kubectl</span> cluster-<span class="hljs-literal">info</span><br></code></pre></td></tr></table></figure></li><li><p>使用查看群集中的节点 </p> <figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-builtin-name">get</span> nodes<br></code></pre></td></tr></table></figure><p> 如果节点标记为NotReady，则它仍在启动组件。</p><p> 此命令显示可用于托管应用程序的所有节点。现在我们只有一个节点，我们可以看到它的状态已准备好（它已准备好接受部署的应用程序）</p></li></ol><h3 id="第3步-部署容器"><a href="#第3步-部署容器" class="headerlink" title="第3步 - 部署容器"></a>第3步 - 部署容器</h3><p>通过运行Kubernetes集群，现在可以部署容器。</p><ol><li><p>使用kubectl run它，它允许将容器部署到集群上 -</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-builtin-name">run</span> first-deployment <span class="hljs-attribute">--image</span>=katacoda/docker-http-server <span class="hljs-attribute">--port</span>=80<br></code></pre></td></tr></table></figure></li><li><p>可以通过正在运行的Pod发现部署状态 </p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-builtin-name">get</span> pods<br></code></pre></td></tr></table></figure></li><li><p>容器运行后，可以根据需要通过不同的网络选项进行暴露。一种可能的解决方案是NodePort，它为容器提供动态端口。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl expose deployment first-deployment <span class="hljs-attribute">--port</span>=80 <span class="hljs-attribute">--type</span>=NodePort<br></code></pre></td></tr></table></figure></li><li><p>以下命令查找已分配的端口并执行HTTP请求。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-builtin-name">export</span> <span class="hljs-attribute">PORT</span>=$(kubectl <span class="hljs-builtin-name">get</span> svc first-deployment -o <span class="hljs-attribute">go-template</span>=<span class="hljs-string">'&#123;&#123;range.spec.ports&#125;&#125;&#123;&#123;if .nodePort&#125;&#125;&#123;&#123;.nodePort&#125;&#125;&#123;&#123;"\n"&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;end&#125;&#125;'</span>)<br>echo <span class="hljs-string">"Accessing host01:<span class="hljs-variable">$PORT</span>"</span><br>curl host01:<span class="hljs-variable">$PORT</span><br></code></pre></td></tr></table></figure></li></ol><p>结果是处理请求的容器。</p>]]></content>
    
    
    
    <tags>
      
      <tag>k8s kubectl</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k8s安装kiali</title>
    <link href="/2019/05/20/k8s%E5%AE%89%E8%A3%85kiali/"/>
    <url>/2019/05/20/k8s%E5%AE%89%E8%A3%85kiali/</url>
    
    <content type="html"><![CDATA[<h1 id="kiali"><a href="#kiali" class="headerlink" title="kiali"></a>kiali</h1><p>kiali安装需要依赖promethus。</p><h2 id="安装prometheus"><a href="#安装prometheus" class="headerlink" title="安装prometheus"></a>安装prometheus</h2><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-built_in">apply</span> -f ./myprometheus_service.yaml<br>kubectl <span class="hljs-built_in">apply</span> -f ./myprometheus.yaml<br></code></pre></td></tr></table></figure><p>一个是配置deplement，另一个是配置service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs language">cat myprometheus_service.yaml <br>kind: Service<br>apiVersion: v1<br>metadata:<br>  labels:<br>    app: promethues<br>  name: promethues<br>  namespace: default<br>spec:<br>  ports:<br>    - port: 9090<br>      targetPort: 9090<br>      nodePort: 31090<br>  selector:<br>    app: promethues<br>  type: NodePort<br></code></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs language">cat myprometheus.yaml<br>kind: Deployment<br>apiVersion: apps/v1<br>metadata:<br>  labels:<br>    prom-app: promethues<br>  name: promethues<br>  namespace: default<br>spec:<br>  replicas: 1<br>  selector:<br>    matchLabels:<br>      app: promethues<br>  template:<br>    metadata:<br>      labels:<br>        app: promethues<br>    spec:<br>      containers:<br>      - name: promethues<br>        #image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1<br>        image: prom/prometheus:latest<br>        ports:<br>        - containerPort: 9090<br>          protocol: TCP<br></code></pre></td></tr></table></figure><p>安装完后查看service<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">kubectl get svc --all-namespaces -o wide<br></code></pre></td></tr></table></figure></p><p>之后通过浏览器访问，就可以看到promethus的界面</p><p><a href="http://127.0.0.1:31090" target="_blank" rel="noopener">http://127.0.0.1:31090</a></p><p>#安装kiali<br>进入kiali目录，修改deploy/kubernetes/configmap.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs language">apiVersion: v1<br>kind: ConfigMap<br>metadata:<br>  name: kiali<br>  labels:<br>    app: kiali<br>    version: $&#123;VERSION_LABEL&#125;<br>data:<br>  config.yaml: |<br>    istio_namespace: $&#123;ISTIO_NAMESPACE&#125;<br>    server:<br>      port: 20001<br>      web_root: /kiali<br>    external_services:<br>      jaeger:<br>        url: $&#123;JAEGER_URL&#125;<br>      grafana:<br>        url: $&#123;GRAFANA_URL&#125;<br>      prometheus_service_url: http://192.168.158.146:31090<br>    #identity:<br>    #  cert_file: /kiali-cert/cert-chain.pem<br>    #  private_key_file: /kiali-cert/key.pem<br></code></pre></td></tr></table></figure><p>注意，ip地址一定要改成host的实际ip地址。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">make k8s-deploy<br></code></pre></td></tr></table></figure><p>浏览器访问：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">http:<span class="hljs-regexp">//</span><span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">32520</span><span class="hljs-regexp">/kiali/</span><br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
    <tags>
      
      <tag>kiali k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>安装Scope</title>
    <link href="/2019/05/20/%E5%AE%89%E8%A3%85Scope/"/>
    <url>/2019/05/20/%E5%AE%89%E8%A3%85Scope/</url>
    
    <content type="html"><![CDATA[<h3 id="安装Scope"><a href="#安装Scope" class="headerlink" title="安装Scope"></a>安装Scope</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl apply -f <span class="hljs-string">"https://cloud.weave.works/k8s/scope.yaml?k8s-version=<span class="hljs-variable">$(kubectl version | base64 | tr -d '\n')</span>&amp;k8s-service-type=NodePort"</span><br></code></pre></td></tr></table></figure><p>安装完成显示如下：</p><figure class="highlight d"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root<span class="hljs-keyword">@k8s</span>-master yyhmmwan]# kubectl apply -f <span class="hljs-string">"https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl vers<br>ion | base64 | tr -d '\n')&amp;k8s-service-type=NodePort"</span><br>namespace/weave created<br>serviceaccount/weave-<span class="hljs-keyword">scope</span> created<br>clusterrole.rbac.authorization.k8s.io/weave-<span class="hljs-keyword">scope</span> created<br>clusterrolebinding.rbac.authorization.k8s.io/weave-<span class="hljs-keyword">scope</span> created<br>deployment.apps/weave-<span class="hljs-keyword">scope</span>-app created<br>service/weave-<span class="hljs-keyword">scope</span>-app created<br>deployment.apps/weave-<span class="hljs-keyword">scope</span>-cluster-agent created<br>daemonset.extensions/weave-<span class="hljs-keyword">scope</span>-agent created<br></code></pre></td></tr></table></figure><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@k8s-<span class="hljs-keyword">master</span> <span class="hljs-title">yyhmmwan</span>]<span class="hljs-comment"># kubectl get pod -n weave</span><br>NAME                                         READY   STATUS    RESTARTS   AGE<br>weave-scope-agent-<span class="hljs-number">5</span>g92b                      <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">5m</span>50s<br>weave-scope-agent-z55pn                      <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">5m</span>50s<br>weave-scope-app-<span class="hljs-number">6</span>cbf5dbc45-hw7xp             <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">5m</span>49s<br>weave-scope-cluster-agent-<span class="hljs-number">5</span>d7f64677b-<span class="hljs-number">6</span>xr2l   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running   <span class="hljs-number">0</span>          <span class="hljs-number">5m</span>50s<br>[root@k8s-<span class="hljs-keyword">master</span> <span class="hljs-title">yyhmmwan</span>]<span class="hljs-comment"># kubectl get svc -n weave</span><br>NAME              <span class="hljs-keyword">TYPE</span>       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE<br>weave-scope-app   NodePort   <span class="hljs-number">10.103</span>.<span class="hljs-number">101.115</span>   <span class="hljs-tag">&lt;none&gt;</span>        <span class="hljs-number">80</span>:<span class="hljs-number">31795</span>/TCP   <span class="hljs-number">6m</span>22s<br>[root@k8s-<span class="hljs-keyword">master</span> <span class="hljs-title">yyhmmwan</span>]<span class="hljs-comment"># kubectl get deploy -n weave</span><br>NAME                        READY   UP-TO-<span class="hljs-keyword">DATE</span>   AVAILABLE   AGE<br>weave-scope-app             <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     <span class="hljs-number">1</span>            <span class="hljs-number">1</span>           <span class="hljs-number">7m</span>5s<br>weave-scope-cluster-agent   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     <span class="hljs-number">1</span>            <span class="hljs-number">1</span>           <span class="hljs-number">7m</span>5s<br>[root@k8s-<span class="hljs-keyword">master</span> <span class="hljs-title">yyhmmwan</span>]<span class="hljs-comment"># kubetcl get deploy -n weave</span><br>bash: kubetcl: command not found<br>[root@k8s-<span class="hljs-keyword">master</span> <span class="hljs-title">yyhmmwan</span>]<span class="hljs-comment"># kubectl get deploy -n weave</span><br>NAME                        READY   UP-TO-<span class="hljs-keyword">DATE</span>   AVAILABLE   AGE<br>weave-scope-app             <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     <span class="hljs-number">1</span>            <span class="hljs-number">1</span>           <span class="hljs-number">8m</span>15s<br>weave-scope-cluster-agent   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     <span class="hljs-number">1</span>            <span class="hljs-number">1</span>           <span class="hljs-number">8m</span>15s<br></code></pre></td></tr></table></figure><h1 id="本地模式安装Kubernetes-local-clone"><a href="#本地模式安装Kubernetes-local-clone" class="headerlink" title="本地模式安装Kubernetes (local clone)"></a>本地模式安装Kubernetes (local clone)</h1><p>参考：<br><a href="https://www.weave.works/docs/scope/latest/installing/#k8s" target="_blank" rel="noopener">scope 安装</a><br><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-keyword">cd</span> scope<br>kubectl <span class="hljs-keyword">apply</span> -f examples/k8s<br></code></pre></td></tr></table></figure></p><p>但这个还是从远程image拉取的模式</p><p>Port-forward to access weave-scope-app<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">kubectl port-forward svc/weave-scope-app -n weave 4040:80<br></code></pre></td></tr></table></figure></p><h1 id="本地启动scope"><a href="#本地启动scope" class="headerlink" title="本地启动scope"></a>本地启动scope</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs language">sudo ./scope -mode app -weave=false -debug -app.http.address 127.0.0.1:4041 <br><br>sudo ./scope -mode probe -weave=false -debug -probe-only -probe.docker=true  http://127.0.0.1:4041<br></code></pre></td></tr></table></figure><p>在docker启动scope<br>启动容器需要挂几个目录：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs language">docker run --name scope-probe -v /var/run/docker.sock:/var/run/docker.sock -v /sys/kernel/debug:/sys/kernel/debug --rm -ti --privileged=true  topsec-dsec:v1.0 /bin/bash<br></code></pre></td></tr></table></figure></p><figure class="highlight d"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">./<span class="hljs-keyword">scope</span> -mode app -weave=<span class="hljs-literal">false</span> -<span class="hljs-keyword">debug</span> -app.http.address <span class="hljs-number">0.0</span>.0.0:<span class="hljs-number">4040</span><br>./<span class="hljs-keyword">scope</span> -mode probe -weave=<span class="hljs-literal">false</span> -<span class="hljs-keyword">debug</span> -probe-only -probe.docker=<span class="hljs-literal">true</span>  http:<span class="hljs-comment">//192.168.32.68:4040</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>scope docker k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k8s-API扩展</title>
    <link href="/2019/05/20/k8s-API%E6%89%A9%E5%B1%95/"/>
    <url>/2019/05/20/k8s-API%E6%89%A9%E5%B1%95/</url>
    
    <content type="html"><![CDATA[<h1 id="API-扩展"><a href="#API-扩展" class="headerlink" title="API 扩展"></a>API 扩展</h1><p>Kubernetes 的架构非常灵活，提供了从 API、认证授权、准入控制、网络、存储、运行时以及云平台等一系列的<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/extend-cluster/" target="_blank" rel="noopener">扩展机制</a>，方便用户无侵入的扩展集群的功能。</p><p>从 API 的角度来说，可以通过 Aggregation 和 CustomResourceDefinition（CRD） 等扩展 Kubernetes API。</p><ul><li>API Aggregation 允许在不修改 Kubernetes 核心代码的同时将第三方服务注册到 Kubernetes API 中，这样就可以通过 Kubernetes API 来访问外部服务。</li><li>CustomResourceDefinition 则可以在集群中新增资源对象，并可以与已有资源对象（如 Pod、Deployment 等）相同的方式来管理它们。</li></ul><p>CRD 相比 Aggregation 更易用，两者对比如下</p><table><thead><tr><th>CRDs</th><th>Aggregated API</th></tr></thead><tbody><tr><td>无需编程即可使用 CRD 管理资源</td><td>需要使用 Go 来构建 Aggregated APIserver</td></tr><tr><td>不需要运行额外服务，但一般需要一个 CRD 控制器同步和管理这些资源</td><td>需要独立的第三方服务</td></tr><tr><td>任何缺陷都会在 Kubernetes 核心中修复</td><td>可能需要定期从 Kubernetes 社区同步缺陷修复方法并重新构建 Aggregated APIserver.</td></tr><tr><td>无需额外管理版本</td><td>需要第三方服务来管理版本</td></tr></tbody></table><h2 id="CRD"><a href="#CRD" class="headerlink" title="CRD"></a>CRD</h2><p>CustomResourceDefinition（CRD）是 v1.7 新增的无需改变代码就可以扩展 Kubernetes API 的机制，用来管理自定义对象。它实际上是 <a href="thirdpartyresources.md">ThirdPartyResources（TPR）</a> 的升级版本，而 TPR 已经在 v1.8 中删除。</p><h2 id="API-版本对照表"><a href="#API-版本对照表" class="headerlink" title="API 版本对照表"></a>API 版本对照表</h2><table><thead><tr><th>Kubernetes 版本</th><th>CRD API 版本</th></tr></thead><tbody><tr><td>v1.8+</td><td>apiextensions.k8s.io/v1beta1</td></tr></tbody></table><h2 id="CRD-示例"><a href="#CRD-示例" class="headerlink" title="CRD 示例"></a>CRD 示例</h2><p>下面的例子会创建一个 <code>/apis/stable.example.com/v1/namespaces/&lt;namespace&gt;/crontabs/…</code> 的自定义 API：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs sh">apiVersion: apiextensions.k8s.io/v1beta1<br>kind: CustomResourceDefinition<br>metadata:<br>  <span class="hljs-comment"># name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;</span><br>  name: crontabs.stable.example.com<br>spec:<br>  <span class="hljs-comment"># group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><br>  group: stable.example.com<br>  <span class="hljs-comment"># versions to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;</span><br>  versions:<br>  - name: v1beta1<br>    <span class="hljs-comment"># Each version can be enabled/disabled by Served flag.</span><br>    served: <span class="hljs-literal">true</span><br>    <span class="hljs-comment"># One and only one version must be marked as the storage version.</span><br>    storage: <span class="hljs-literal">true</span><br>  - name: v1<br>    served: <span class="hljs-literal">true</span><br>    storage: <span class="hljs-literal">false</span><br>  <span class="hljs-comment"># either Namespaced or Cluster</span><br>  scope: Namespaced<br>  names:<br>    <span class="hljs-comment"># plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;</span><br>    plural: crontabs<br>    <span class="hljs-comment"># singular name to be used as an alias on the CLI and for display</span><br>    singular: crontab<br>    <span class="hljs-comment"># kind is normally the CamelCased singular type. Your resource manifests use this.</span><br>    kind: CronTab<br>    <span class="hljs-comment"># shortNames allow shorter string to match your resource on the CLI</span><br>    shortNames:<br>    - ct<br></code></pre></td></tr></table></figure><p>API 创建好后，就可以创建具体的 CronTab 对象了</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ cat my-cronjob.yaml<br>apiVersion: <span class="hljs-string">"stable.example.com/v1"</span><br>kind: CronTab<br>metadata:<br>  name: my-new-cron-object<br>spec:<br>  cronSpec: <span class="hljs-string">"* * * * /5"</span><br>  image: my-awesome-cron-image<br><br>$ kubectl create -f my-crontab.yaml<br>crontab <span class="hljs-string">"my-new-cron-object"</span> created<br><br>$ kubectl get crontab<br>NAME                 KIND<br>my-new-cron-object   CronTab.v1.stable.example.com<br>$ kubectl get crontab my-new-cron-object -o yaml<br>apiVersion: stable.example.com/v1<br>kind: CronTab<br>metadata:<br>  creationTimestamp: 2017-07-03T19:00:56Z<br>  name: my-new-cron-object<br>  namespace: default<br>  resourceVersion: <span class="hljs-string">"20630"</span><br>  selfLink: /apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-object<br>  uid: 5c82083e-5fbd-11e7-a204-42010a8c0002<br>spec:<br>  cronSpec: <span class="hljs-string">'* * * * /5'</span><br>  image: my-awesome-cron-image<br></code></pre></td></tr></table></figure><h2 id="Finalizer"><a href="#Finalizer" class="headerlink" title="Finalizer"></a>Finalizer</h2><p>Finalizer 用于实现控制器的异步预删除钩子，可以通过 <code>metadata.finalizers</code> 来指定 Finalizer。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"stable.example.com/v1"</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">CronTab</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  finalizers:</span><br><span class="hljs-bullet">  -</span> <span class="hljs-string">finalizer.stable.example.com</span><br></code></pre></td></tr></table></figure><p>Finalizer 指定后，客户端删除对象的操作只会设置 <code>metadata.deletionTimestamp</code> 而不是直接删除。这会触发正在监听 CRD 的控制器，控制器执行一些删除前的清理操作，从列表中删除自己的 finalizer，然后再重新发起一个删除操作。此时，被删除的对象才会真正删除。</p><h2 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a>Validation</h2><p>v1.8 开始新增了实验性的基于 <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md#schemaObject" target="_blank" rel="noopener">OpenAPI v3 schema</a> 的验证（Validation）机制，可以用来提前验证用户提交的资源是否符合规范。使用该功能需要配置 kube-apiserver 的 <code>--feature-gates=CustomResourceValidation=true</code>。</p><p>比如下面的 CRD 要求</p><ul><li><code>spec.cronSpec</code> 必须是匹配正则表达式的字符串</li><li><code>spec.replicas</code> 必须是从 1 到 10 的整数</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apiextensions.k8s.io/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">CustomResourceDefinition</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">crontabs.stable.example.com</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  group:</span> <span class="hljs-string">stable.example.com</span><br><span class="hljs-attr">  version:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">  scope:</span> <span class="hljs-string">Namespaced</span><br><span class="hljs-attr">  names:</span><br><span class="hljs-attr">    plural:</span> <span class="hljs-string">crontabs</span><br><span class="hljs-attr">    singular:</span> <span class="hljs-string">crontab</span><br><span class="hljs-attr">    kind:</span> <span class="hljs-string">CronTab</span><br><span class="hljs-attr">    shortNames:</span><br><span class="hljs-bullet">    -</span> <span class="hljs-string">ct</span><br><span class="hljs-attr">  validation:</span><br>   <span class="hljs-comment"># openAPIV3Schema is the schema for validating custom objects.</span><br><span class="hljs-attr">    openAPIV3Schema:</span><br><span class="hljs-attr">      properties:</span><br><span class="hljs-attr">        spec:</span><br><span class="hljs-attr">          properties:</span><br><span class="hljs-attr">            cronSpec:</span><br><span class="hljs-attr">              type:</span> <span class="hljs-string">string</span><br><span class="hljs-attr">              pattern:</span> <span class="hljs-string">'^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?)&#123;4&#125;$'</span><br><span class="hljs-attr">            replicas:</span><br><span class="hljs-attr">              type:</span> <span class="hljs-string">integer</span><br><span class="hljs-attr">              minimum:</span> <span class="hljs-number">1</span><br><span class="hljs-attr">              maximum:</span> <span class="hljs-number">10</span><br></code></pre></td></tr></table></figure><p>这样，在创建下面的 CronTab 时</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"stable.example.com/v1"</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">CronTab</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">my-new-cron-object</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  cronSpec:</span> <span class="hljs-string">"* * * *"</span><br><span class="hljs-attr">  image:</span> <span class="hljs-string">my-awesome-cron-image</span><br><span class="hljs-attr">  replicas:</span> <span class="hljs-number">15</span><br></code></pre></td></tr></table></figure><p>会报验证失败的错误：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh">The CronTab <span class="hljs-string">"my-new-cron-object"</span> is invalid: []: Invalid value: map[string]interface &#123;&#125;&#123;<span class="hljs-string">"apiVersion"</span>:<span class="hljs-string">"stable.example.com/v1"</span>, <span class="hljs-string">"kind"</span>:<span class="hljs-string">"CronTab"</span>, <span class="hljs-string">"metadata"</span>:map[string]interface &#123;&#125;&#123;<span class="hljs-string">"name"</span>:<span class="hljs-string">"my-new-cron-object"</span>, <span class="hljs-string">"namespace"</span>:<span class="hljs-string">"default"</span>, <span class="hljs-string">"deletionTimestamp"</span>:interface &#123;&#125;(nil), <span class="hljs-string">"deletionGracePeriodSeconds"</span>:(*int64)(nil), <span class="hljs-string">"creationTimestamp"</span>:<span class="hljs-string">"2017-09-05T05:20:07Z"</span>, <span class="hljs-string">"uid"</span>:<span class="hljs-string">"e14d79e7-91f9-11e7-a598-f0761cb232d1"</span>, <span class="hljs-string">"selfLink"</span>:<span class="hljs-string">""</span>,<span class="hljs-string">"clusterName"</span>:<span class="hljs-string">""</span>&#125;, <span class="hljs-string">"spec"</span>:map[string]interface &#123;&#125;&#123;<span class="hljs-string">"cronSpec"</span>:<span class="hljs-string">"* * * *"</span>, <span class="hljs-string">"image"</span>:<span class="hljs-string">"my-awesome-cron-image"</span>, <span class="hljs-string">"replicas"</span>:15&#125;&#125;:<br>validation failure list:<br>spec.cronSpec <span class="hljs-keyword">in</span> body should match <span class="hljs-string">'^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?)&#123;4&#125;$'</span><br>spec.replicas <span class="hljs-keyword">in</span> body should be less than or equal to 10<br></code></pre></td></tr></table></figure><h2 id="Subresources"><a href="#Subresources" class="headerlink" title="Subresources"></a>Subresources</h2><p>v1.10 开始 CRD 还支持 <code>/status</code> 和 <code>/scale</code> 等两个子资源（Beta），并且从 v1.11 开始默认开启。</p><blockquote><p>v1.10 版本使用前需要在 <code>kube-apiserver</code> 开启 <code>--feature-gates=CustomResourceSubresources=true</code>。</p></blockquote><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># resourcedefinition.yaml</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apiextensions.k8s.io/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">CustomResourceDefinition</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">crontabs.stable.example.com</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  group:</span> <span class="hljs-string">stable.example.com</span><br><span class="hljs-attr">  version:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">  scope:</span> <span class="hljs-string">Namespaced</span><br><span class="hljs-attr">  names:</span><br><span class="hljs-attr">    plural:</span> <span class="hljs-string">crontabs</span><br><span class="hljs-attr">    singular:</span> <span class="hljs-string">crontab</span><br><span class="hljs-attr">    kind:</span> <span class="hljs-string">CronTab</span><br><span class="hljs-attr">    shortNames:</span><br><span class="hljs-bullet">    -</span> <span class="hljs-string">ct</span><br>  <span class="hljs-comment"># subresources describes the subresources for custom resources.</span><br><span class="hljs-attr">  subresources:</span><br>    <span class="hljs-comment"># status enables the status subresource.</span><br><span class="hljs-attr">    status:</span> <span class="hljs-string">&#123;&#125;</span><br>    <span class="hljs-comment"># scale enables the scale subresource.</span><br><span class="hljs-attr">    scale:</span><br>      <span class="hljs-comment"># specReplicasPath defines the JSONPath inside of a custom resource that corresponds to Scale.Spec.Replicas.</span><br><span class="hljs-attr">      specReplicasPath:</span> <span class="hljs-string">.spec.replicas</span><br>      <span class="hljs-comment"># statusReplicasPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Replicas.</span><br><span class="hljs-attr">      statusReplicasPath:</span> <span class="hljs-string">.status.replicas</span><br>      <span class="hljs-comment"># labelSelectorPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Selector.</span><br><span class="hljs-attr">      labelSelectorPath:</span> <span class="hljs-string">.status.labelSelector</span><br></code></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ kubectl create -f resourcedefinition.yaml<br>$ kubectl create -f- &lt;&lt;EOF<br>apiVersion: <span class="hljs-string">"stable.example.com/v1"</span><br>kind: CronTab<br>metadata:<br>  name: my-new-cron-object<br>spec:<br>  cronSpec: <span class="hljs-string">"* * * * */5"</span><br>  image: my-awesome-cron-image<br>  replicas: 3<br>EOF<br><br>$ kubectl scale --replicas=5 crontabs/my-new-cron-object<br>crontabs <span class="hljs-string">"my-new-cron-object"</span> scaled<br><br>$ kubectl get crontabs my-new-cron-object -o jsonpath=<span class="hljs-string">'&#123;.spec.replicas&#125;'</span><br>5<br></code></pre></td></tr></table></figure><h2 id="Categories"><a href="#Categories" class="headerlink" title="Categories"></a>Categories</h2><p>Categories 用来将 CRD 对象分组，这样就可以使用 <code>kubectl get &lt;category-name&gt;</code> 来查询属于该组的所有对象。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># resourcedefinition.yaml</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apiextensions.k8s.io/v1beta1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">CustomResourceDefinition</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">crontabs.stable.example.com</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  group:</span> <span class="hljs-string">stable.example.com</span><br><span class="hljs-attr">  version:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">  scope:</span> <span class="hljs-string">Namespaced</span><br><span class="hljs-attr">  names:</span><br><span class="hljs-attr">    plural:</span> <span class="hljs-string">crontabs</span><br><span class="hljs-attr">    singular:</span> <span class="hljs-string">crontab</span><br><span class="hljs-attr">    kind:</span> <span class="hljs-string">CronTab</span><br><span class="hljs-attr">    shortNames:</span><br><span class="hljs-bullet">    -</span> <span class="hljs-string">ct</span><br>    <span class="hljs-comment"># categories is a list of grouped resources the custom resource belongs to.</span><br><span class="hljs-attr">    categories:</span><br><span class="hljs-bullet">    -</span> <span class="hljs-string">all</span><br></code></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># my-crontab.yaml</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"stable.example.com/v1"</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">CronTab</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">my-new-cron-object</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  cronSpec:</span> <span class="hljs-string">"* * * * */5"</span><br><span class="hljs-attr">  image:</span> <span class="hljs-string">my-awesome-cron-image</span><br></code></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ kubectl create -f resourcedefinition.yaml<br>$ kubectl create -f my-crontab.yaml<br>$ kubectl get all<br>NAME                          AGE<br>crontabs/my-new-cron-object   3s<br></code></pre></td></tr></table></figure><h2 id="CRD-控制器"><a href="#CRD-控制器" class="headerlink" title="CRD 控制器"></a>CRD 控制器</h2><p>在使用 CRD 扩展 Kubernetes API 时，通常还需要实现一个新建资源的控制器，监听新资源的变化情况，并作进一步的处理。</p><p><a href="https://github.com/kubernetes/sample-controller" target="_blank" rel="noopener">https://github.com/kubernetes/sample-controller</a> 提供了一个 CRD 控制器的示例，包括</p><ul><li>如何注册资源 <code>Foo</code></li><li>如何创建、删除和查询 <code>Foo</code> 对象</li><li>如何监听 <code>Foo</code> 资源对象的变化情况</li></ul><h2 id="Kubebuilder"><a href="#Kubebuilder" class="headerlink" title="Kubebuilder"></a>Kubebuilder</h2><p>从上面的实例中可以看到从头构建一个 CRD 控制器并不容易，需要对 Kubernetes 的 API 有深入了解，并且RBAC 集成、镜像构建、持续集成和部署等都需要很大工作量。</p><p><a href="https://github.com/kubernetes-sigs/kubebuilder" target="_blank" rel="noopener">kubebuilder</a> 正是为解决这个问题而生，为 CRD 控制器提供了一个简单易用的框架，并可直接生成镜像构建、持续集成、持续部署等所需的资源文件。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># Install kubebuilder</span><br>VERSION=1.0.1<br>wget https://github.com/kubernetes-sigs/kubebuilder/releases/download/v<span class="hljs-variable">$&#123;VERSION&#125;</span>/kubebuilder_<span class="hljs-variable">$&#123;VERSION&#125;</span>_linux_amd64.tar.gz<br>tar zxvf kubebuilder_<span class="hljs-variable">$&#123;VERSION&#125;</span>_linux_amd64.tar.gz<br>sudo mv kubebuilder_<span class="hljs-variable">$&#123;VERSION&#125;</span>_linux_amd64 /usr/<span class="hljs-built_in">local</span>/kubebuilder<br><span class="hljs-built_in">export</span> PATH=<span class="hljs-variable">$PATH</span>:/usr/<span class="hljs-built_in">local</span>/kubebuilder/bin<br><br><span class="hljs-comment"># Install dep kustomize</span><br>go get -u github.com/golang/dep/cmd/dep<br>go get github.com/kubernetes-sigs/kustomize<br></code></pre></td></tr></table></figure><h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><h4 id="初始化项目"><a href="#初始化项目" class="headerlink" title="初始化项目"></a>初始化项目</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh">mkdir -p <span class="hljs-variable">$GOPATH</span>/src/demo<br><span class="hljs-built_in">cd</span> <span class="hljs-variable">$GOPATH</span>/src/demo<br>kubebuilder init --domain k8s.io --license apache2 --owner <span class="hljs-string">"The Kubernetes Authors"</span><br></code></pre></td></tr></table></figure><h4 id="创建-API"><a href="#创建-API" class="headerlink" title="创建 API"></a>创建 API</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubebuilder create api --group ships --version v1beta1 --kind Sloop<br></code></pre></td></tr></table></figure><p>然后按照实际需要修改 <code>pkg/apis/ship/v1beta1/sloop_types.go</code> 和 <code>pkg/controller/sloop/sloop_controller.go</code> 增加业务逻辑。</p><h4 id="本地运行测试"><a href="#本地运行测试" class="headerlink" title="本地运行测试"></a>本地运行测试</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">make install<br>make run<br></code></pre></td></tr></table></figure><blockquote><p>如果碰到错误 <code>ValidationError(CustomResourceDefinition.status): missing required field &quot;storedVersions&quot; in io.k8s.apiextensions-apiserver.pkg.apis.apiextensions.v1beta1.CustomResourceDefinitionStatus]</code>，可以手动修改 <code>config/crds/ships_v1beta1_sloop.yaml</code>:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">&gt; status:<br>&gt;   acceptedNames:<br>&gt;     kind: ""<br>&gt;     plural: ""<br>&gt;   conditions: []<br>&gt;   storedVersions: []<br>&gt;<br>&gt; 然后运行 `kubectl apply -f config/crds` 创建 CRD。<br><br>然后就可以用 `ships.k8s.io/v1beta1` 来创建 Kind 为 `Sloop` 的资源了，比如 <br><br>```sh<br>kubectl apply -f config/samples/ships_v1beta1_sloop.yaml</span><br></code></pre></td></tr></table></figure></p></blockquote><h4 id="构建镜像并部署控制器"><a href="#构建镜像并部署控制器" class="headerlink" title="构建镜像并部署控制器"></a>构建镜像并部署控制器</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 替换 IMG 为你自己的</span><br><span class="hljs-built_in">export</span> IMG=feisky/demo-crd:v1<br>make docker-build<br>make docker-push<br>make deploy<br></code></pre></td></tr></table></figure><blockquote><p>kustomize 已经不再支持通配符，因而上述 <code>make deploy</code> 可能会碰到 <code>Load from path ../rbac/*.yaml failed</code> 错误，解决方法是手动修改 <code>config/default/kustomization.yaml</code>:</p><p>resources:</p><ul><li>../rbac/rbac_role.yaml</li><li>../rbac/rbac_role_binding.yaml</li><li>../manager/manager.yaml</li></ul><p>然后执行 <code>kustomize build config/default | kubectl apply -f -</code> 部署，默认部署到 <code>demo-system</code> namespace 中。</p></blockquote><h4 id="文档和测试"><a href="#文档和测试" class="headerlink" title="文档和测试"></a>文档和测试</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># run unit tests</span><br>make <span class="hljs-built_in">test</span><br><br><span class="hljs-comment"># generate docs</span><br>kubebuilder docs<br></code></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/#validation" target="_blank" rel="noopener">Extend the Kubernetes API with CustomResourceDefinitions</a></li><li><a href="https://kubernetes.io/docs/api-reference/v1.8/#customresourcedefinition-v1beta1-apiextensions" target="_blank" rel="noopener">CustomResourceDefinition API</a></li></ul><p>详细的使用方法请参考</p><ul><li><a href="aggregation.md">Aggregation</a></li><li><a href="../concepts/customresourcedefinition.md">CustomResourceDefinition</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k8s基本元素概念</title>
    <link href="/2019/05/20/k8s%E5%9F%BA%E6%9C%AC%E5%85%83%E7%B4%A0%E6%A6%82%E5%BF%B5/"/>
    <url>/2019/05/20/k8s%E5%9F%BA%E6%9C%AC%E5%85%83%E7%B4%A0%E6%A6%82%E5%BF%B5/</url>
    
    <content type="html"><![CDATA[<h2 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h2><p>Kubernetes 使用 Pod 来管理容器，每个 Pod 可以包含一个或多个紧密关联的容器。</p><p>Pod 是一组紧密关联的容器集合，它们共享 PID、IPC、Network 和 UTS namespace，是 Kubernetes 调度的基本单位。Pod 内的多个容器共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。</p><p><img src="../../images/pod.png" srcset="/img/loading.gif" lazyload alt="pod"></p><p>在 Kubernetes 中，所有对象都使用 manifest（yaml 或 json）来定义，比如一个简单的 nginx 服务可以定义为 nginx.yaml，它包含一个镜像为 nginx 的容器：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">nginx</span><br><span class="hljs-attr">  labels:</span><br><span class="hljs-attr">    app:</span> <span class="hljs-string">nginx</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  containers:</span><br><span class="hljs-attr">  - name:</span> <span class="hljs-string">nginx</span><br><span class="hljs-attr">    image:</span> <span class="hljs-string">nginx</span><br><span class="hljs-attr">    ports:</span><br><span class="hljs-attr">    - containerPort:</span> <span class="hljs-number">80</span><br></code></pre></td></tr></table></figure><h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><p>Node 是 Pod 真正运行的主机，可以是物理机，也可以是虚拟机。为了管理 Pod，每个 Node 节点上至少要运行 container runtime（比如 docker 或者 rkt）、<code>kubelet</code> 和 <code>kube-proxy</code> 服务。</p><p><img src="../../images/node.png" srcset="/img/loading.gif" lazyload alt="node"></p><h2 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h2><p>Namespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pods, services, replication controllers 和 deployments 等都是属于某一个 namespace 的（默认是 default），而 node, persistentVolumes 等则不属于任何 namespace。</p><h2 id="Controllers"><a href="#Controllers" class="headerlink" title="Controllers"></a>Controllers</h2><p>A Controller can create and manage multiple Pods for you, handling replication and rollout and providing self-healing capabilities at cluster scope.<br>• Deployment<br>• StatefulSet<br>• DaemonSet<br>In general, Controllers use a Pod Template that you provide to create the Pods for which it is responsible.</p><p>pod只是最小单元，如果失败了，也不会自动启动，也不会弹性扩展，这些动态性管理类工作交给controller来完成。</p><p>Deployment同样为Kubernetes的一个核心内容，主要职责同样是为了保证pod的数量和健康，90%的功能与Replication Controller完全一样，可以看做新一代的Replication Controller。</p><p>但是，它又具备了Replication Controller之外的新特性：<br>• Replication Controller全部功能：Deployment继承了上面描述的Replication Controller全部功能。<br>• 事件和状态查看：可以查看Deployment的升级详细进度和状态。<br>• 回滚：当升级pod镜像或者相关参数的时候发现问题，可以使用回滚操作回滚到上一个稳定的版本或者指定的版本。<br>• 版本记录: 每一次对Deployment的操作，都能保存下来，给予后续可能的回滚使用。<br>• 暂停和启动：对于每一次升级，都能够随时暂停和启动。<br>• 多种升级方案：Recreate：删除所有已存在的pod,重新创建新的; RollingUpdate：滚动升级，逐步替换的策略，同时滚动升级时，支持更多的附加参数，例如设置最大不可用pod数量，最小升级间隔时间等等。</p><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>Service 是应用服务的抽象，通过 labels 为应用提供负载均衡和服务发现。匹配 labels 的 Pod IP 和端口列表组成 endpoints，由 kube-proxy 负责将服务 IP 负载均衡到这些 endpoints 上。</p><p>每个 Service 都会自动分配一个 cluster IP（仅在集群内部可访问的虚拟地址）和 DNS 名，其他容器可以通过该地址或 DNS 来访问服务，而不需要了解后端容器的运行。</p><p><img src="../../images/14731220608865.png" srcset="/img/loading.gif" lazyload alt></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">nginx</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  ports:</span><br><span class="hljs-attr">  - port:</span> <span class="hljs-number">8078</span> <span class="hljs-comment"># the port that this service should serve on</span><br><span class="hljs-attr">    name:</span> <span class="hljs-string">http</span><br>    <span class="hljs-comment"># the container on each pod to connect to, can be a name</span><br>    <span class="hljs-comment"># (e.g. 'www') or a number (e.g. 80)</span><br><span class="hljs-attr">    targetPort:</span> <span class="hljs-number">80</span><br><span class="hljs-attr">    protocol:</span> <span class="hljs-string">TCP</span><br><span class="hljs-attr">  selector:</span><br><span class="hljs-attr">    app:</span> <span class="hljs-string">nginx</span><br></code></pre></td></tr></table></figure><h2 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h2><p>Label 是识别 Kubernetes 对象的标签，以 key/value 的方式附加到对象上（key 最长不能超过 63 字节，value 可以为空，也可以是不超过 253 字节的字符串）。</p><p>Label 不提供唯一性，并且实际上经常是很多对象（如 Pods）都使用相同的 label 来标志具体的应用。</p><p>Label 定义好后其他对象可以使用 Label Selector 来选择一组相同 label 的对象（比如 ReplicaSet 和 Service 用 label 来选择一组 Pod）。Label Selector 支持以下几种方式：</p><ul><li>等式，如 <code>app=nginx</code> 和 <code>env!=production</code></li><li>集合，如 <code>env in (production, qa)</code></li><li>多个 label（它们之间是 AND 关系），如 <code>app=nginx,env=test</code></li></ul><h2 id="Annotations"><a href="#Annotations" class="headerlink" title="Annotations"></a>Annotations</h2><p>Annotations 是 key/value 形式附加于对象的注解。不同于 Labels 用于标志和选择对象，Annotations 则是用来记录一些附加信息，用来辅助应用部署、安全策略以及调度策略等。比如 deployment 使用 annotations 来记录 rolling update 的状态。</p><h2 id="proxy"><a href="#proxy" class="headerlink" title="proxy"></a>proxy</h2><h3 id="Proxy-mode-userspace"><a href="#Proxy-mode-userspace" class="headerlink" title="Proxy-mode: userspace"></a>Proxy-mode: userspace</h3><p>也是用iptables，但感觉特别像负载均衡模式， 基于session，流量轮询给多个pod。<br>之所以叫usersapce，实际上就是kube-proxy实现用户态代理，数据包通过类似ngnix进行转发。</p><h3 id="Proxy-mode-iptables"><a href="#Proxy-mode-iptables" class="headerlink" title="Proxy-mode: iptables"></a>Proxy-mode: iptables</h3><p>通过iptables直接转发。不用上送应用层，直接iptables底层进行代理转发的。<br>差别就是，iptables肯定性能高，但用户态可以负载的很好，可以发现pod失效，自动负载别的pod。</p><h3 id="Proxy-mode-ipvs"><a href="#Proxy-mode-ipvs" class="headerlink" title="Proxy-mode: ipvs"></a>Proxy-mode: ipvs</h3><p>就是用ipvs代替iptables。</p><h2 id="Publishing-services-service-types"><a href="#Publishing-services-service-types" class="headerlink" title="Publishing services - service types"></a>Publishing services - service types</h2><p>For some parts of your application (e.g. frontends) you may want to expose a Service onto an external (outside of your cluster) IP address.<br>Kubernetes ServiceTypes allow you to specify what kind of service you want. The default is ClusterIP.<br>如果想expose服务到外部，就需要这个服务了。</p><h3 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP:"></a>ClusterIP:</h3><p>Exposes the service on a cluster-internal IP. Choosing this value makes the service only reachable from within the cluster. This is the default ServiceType.<br>这个ip只能集群内部访问的到。</p><h3 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h3><p> Exposes the service on each Node’s IP at a static port (the NodePort). A ClusterIP service, to which the NodePort service will route, is automatically created. You’ll be able to contact the NodePort service, from outside the cluster, by requesting <nodeip>:<nodeport>.<br>使用node的ip加端口访问。</nodeport></nodeip></p><h3 id="LoadBalancer"><a href="#LoadBalancer" class="headerlink" title="LoadBalancer"></a>LoadBalancer</h3><p>Exposes the service externally using a cloud provider’s load balancer. NodePort and ClusterIPservices, to which the external load balancer will route, are automatically created.</p><h3 id="ExternalName"><a href="#ExternalName" class="headerlink" title="ExternalName"></a>ExternalName</h3><p>Maps the service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value. No proxying of any kind is set up. This requires version 1.7 or higher of kube-dns.</p><h2 id="资源限制"><a href="#资源限制" class="headerlink" title="资源限制"></a>资源限制</h2><p>Kubernetes 通过 cgroups 提供容器资源管理的功能，可以限制每个容器的 CPU 和内存使用，比如对于刚才创建的 deployment，可以通过下面的命令限制 nginx 容器最多只用 50% 的 CPU 和 128MB 的内存：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">$ kubectl <span class="hljs-builtin-name">set</span> resources deployment nginx-app <span class="hljs-attribute">-c</span>=nginx <span class="hljs-attribute">--limits</span>=cpu=500m,memory=128Mi<br>deployment <span class="hljs-string">"nginx"</span><span class="hljs-built_in"> resource </span>requirements updated<br></code></pre></td></tr></table></figure><p>这等同于在每个 Pod 中设置 resources limits：</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">apiVersion</span>: v1<br><span class="hljs-attribute">kind</span>: Pod<br><span class="hljs-attribute">metadata</span>:<br>  <span class="hljs-attribute">labels</span>:<br>    <span class="hljs-attribute">app</span>: nginx<br>  <span class="hljs-attribute">name</span>: nginx<br><span class="hljs-attribute">spec</span>:<br>  <span class="hljs-attribute">containers</span>:<br>    - <span class="hljs-attribute">image</span>: nginx<br>      <span class="hljs-attribute">name</span>: nginx<br>      <span class="hljs-attribute">resources</span>:<br>        <span class="hljs-attribute">limits</span>:<br>          <span class="hljs-attribute">cpu</span>: <span class="hljs-string">"500m"</span><br>          <span class="hljs-attribute">memory</span>: <span class="hljs-string">"128Mi"</span><br></code></pre></td></tr></table></figure><h2 id="健康检查"><a href="#健康检查" class="headerlink" title="健康检查"></a>健康检查</h2><p>Kubernetes 作为一个面向应用的集群管理工具，需要确保容器在部署后确实处在正常的运行状态。Kubernetes 提供了两种探针（Probe，支持 exec、tcpSocket 和 http 方式）来探测容器的状态：</p><ul><li>LivenessProbe：探测应用是否处于健康状态，如果不健康则删除并重新创建容器</li><li>ReadinessProbe：探测应用是否启动完成并且处于正常服务状态，如果不正常则不会接收来自 Kubernetes Service 的流量</li><li>对于已经部署的 deployment，可以通过 kubectl edit deployment/nginx-app 来更新 manifest，增加健康检查部分：</li></ul><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-symbol">apiVersion:</span> extensions/v1beta1<br><span class="hljs-symbol">kind:</span> Deployment<br><span class="hljs-symbol">metadata:</span><br><span class="hljs-symbol">  labels:</span><br><span class="hljs-symbol">    app:</span> nginx<br><span class="hljs-symbol">  name:</span> nginx-default<br><span class="hljs-symbol">spec:</span><br><span class="hljs-symbol">  replicas:</span> <span class="hljs-number">3</span><br><span class="hljs-symbol">  selector:</span><br><span class="hljs-symbol">    matchLabels:</span><br><span class="hljs-symbol">      app:</span> nginx<br><span class="hljs-symbol">  template:</span><br><span class="hljs-symbol">    metadata:</span><br><span class="hljs-symbol">      labels:</span><br><span class="hljs-symbol">        app:</span> nginx<br><span class="hljs-symbol">    spec:</span><br><span class="hljs-symbol">      containers:</span><br>      - image: nginx<br><span class="hljs-symbol">        imagePullPolicy:</span> Always<br><span class="hljs-symbol">        name:</span> http<br><span class="hljs-symbol">        resources:</span> &#123;&#125;<br><span class="hljs-symbol">        terminationMessagePath:</span> <span class="hljs-meta-keyword">/dev/</span>termination-log<br><span class="hljs-symbol">        terminationMessagePolicy:</span> File<br><span class="hljs-symbol">        resources:</span><br><span class="hljs-symbol">          limits:</span><br><span class="hljs-symbol">            cpu:</span> <span class="hljs-string">"500m"</span><br><span class="hljs-symbol">            memory:</span> <span class="hljs-string">"128Mi"</span><br><span class="hljs-symbol">        livenessProbe:</span><br><span class="hljs-symbol">          httpGet:</span><br><span class="hljs-symbol">            path:</span> /<br><span class="hljs-symbol">            port:</span> <span class="hljs-number">80</span><br><span class="hljs-symbol">          initialDelaySeconds:</span> <span class="hljs-number">15</span><br><span class="hljs-symbol">          timeoutSeconds:</span> <span class="hljs-number">1</span><br><span class="hljs-symbol">        readinessProbe:</span><br><span class="hljs-symbol">          httpGet:</span><br><span class="hljs-symbol">            path:</span> /<br><span class="hljs-symbol">            port:</span> <span class="hljs-number">80</span><br><span class="hljs-symbol">          initialDelaySeconds:</span> <span class="hljs-number">5</span><br><span class="hljs-symbol">          timeoutSeconds:</span> <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>k8s pod svc rc</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes的设计理念</title>
    <link href="/2019/05/20/Kubernetes%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5/"/>
    <url>/2019/05/20/Kubernetes%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5/</url>
    
    <content type="html"><![CDATA[<h3 id="设计理念与分布式系统"><a href="#设计理念与分布式系统" class="headerlink" title="设计理念与分布式系统"></a>设计理念与分布式系统</h3><p>分析和理解Kubernetes的设计理念可以使我们更深入地了解Kubernetes系统，更好地利用它管理分布式部署的云原生应用，另一方面也可以让我们借鉴其在分布式系统设计方面的经验。</p><h3 id="API设计原则"><a href="#API设计原则" class="headerlink" title="API设计原则"></a>API设计原则</h3><p>对于云计算系统，系统API实际上处于系统设计的统领地位。Kubernetes集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。理解掌握的API，就好比抓住了K8s系统的牛鼻子。Kubernetes系统API的设计有以下几条原则：</p><ol><li><strong>所有API应该是声明式的</strong>。声明式的操作，相对于命令式操作，对于重复操作的效果是稳定的，这对于容易出现数据丢失或重复的分布式环境来说是很重要的。另外，声明式操作更容易被用户使用，可以使系统向用户隐藏实现的细节，同时也保留了系统未来持续优化的可能性。此外，声明式的API还隐含了所有的API对象都是名词性质的，例如Service、Volume这些API都是名词，这些名词描述了用户所期望得到的一个目标对象。 </li><li><strong>API对象是彼此互补而且可组合的</strong>。这实际上鼓励API对象尽量实现面向对象设计时的要求，即“高内聚，松耦合”，对业务相关的概念有一个合适的分解，提高分解出来的对象的可重用性。 </li><li><strong>高层API以操作意图为基础设计</strong>。如何能够设计好API，跟如何能用面向对象的方法设计好应用系统有相通的地方，高层设计一定是从业务出发，而不是过早的从技术实现出发。因此，针对Kubernetes的高层API设计，一定是以K8s的业务为基础出发，也就是以系统调度管理容器的操作意图为基础设计。 </li><li><strong>低层API根据高层API的控制需要设计</strong>。设计实现低层API的目的，是为了被高层API使用，考虑减少冗余、提高重用性的目的，低层API的设计也要以需求为基础，要尽量抵抗受技术实现影响的诱惑。 </li><li><strong>尽量避免简单封装，不要有在外部API无法显式知道的内部隐藏的机制</strong>。简单的封装，实际没有提供新的功能，反而增加了对所封装API的依赖性。内部隐藏的机制也是非常不利于系统维护的设计方式，例如StatefulSet和ReplicaSet，本来就是两种Pod集合，那么Kubernetes就用不同API对象来定义它们，而不会说只用同一个ReplicaSet，内部通过特殊的算法再来区分这个ReplicaSet是有状态的还是无状态。 </li><li><strong>API操作复杂度与对象数量成正比</strong>。这一条主要是从系统性能角度考虑，要保证整个系统随着系统规模的扩大，性能不会迅速变慢到无法使用，那么最低的限定就是API的操作复杂度不能超过O(N)，N是对象的数量，否则系统就不具备水平伸缩性了。 </li><li><strong>API对象状态不能依赖于网络连接状态</strong>。由于众所周知，在分布式环境下，网络连接断开是经常发生的事情，因此要保证API对象状态能应对网络的不稳定，API对象的状态就不能依赖于网络连接状态。 </li><li><strong>尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难的</strong>。</li></ol><h3 id="控制机制设计原则"><a href="#控制机制设计原则" class="headerlink" title="控制机制设计原则"></a>控制机制设计原则</h3><ul><li><strong>控制逻辑应该只依赖于当前状态</strong>。这是为了保证分布式系统的稳定可靠，对于经常出现局部错误的分布式系统，如果控制逻辑只依赖当前状态，那么就非常容易将一个暂时出现故障的系统恢复到正常状态，因为你只要将该系统重置到某个稳定状态，就可以自信的知道系统的所有控制逻辑会开始按照正常方式运行。 </li><li><strong>假设任何错误的可能，并做容错处理</strong>。在一个分布式系统中出现局部和临时错误是大概率事件。错误可能来自于物理系统故障，外部系统故障也可能来自于系统自身的代码错误，依靠自己实现的代码不会出错来保证系统稳定其实也是难以实现的，因此要设计对任何可能错误的容错处理。 </li><li><strong>尽量避免复杂状态机，控制逻辑不要依赖无法监控的内部状态</strong>。因为分布式系统各个子系统都是不能严格通过程序内部保持同步的，所以如果两个子系统的控制逻辑如果互相有影响，那么子系统就一定要能互相访问到影响控制逻辑的状态，否则，就等同于系统里存在不确定的控制逻辑。 </li><li><strong>假设任何操作都可能被任何操作对象拒绝，甚至被错误解析</strong>。由于分布式系统的复杂性以及各子系统的相对独立性，不同子系统经常来自不同的开发团队，所以不能奢望任何操作被另一个子系统以正确的方式处理，要保证出现错误的时候，操作级别的错误不会影响到系统稳定性。 </li><li><strong>每个模块都可以在出错后自动恢复</strong>。由于分布式系统中无法保证系统各个模块是始终连接的，因此每个模块要有自我修复的能力，保证不会因为连接不到其他模块而自我崩溃。 </li><li><strong>每个模块都可以在必要时优雅地降级服务</strong>。所谓优雅地降级服务，是对系统鲁棒性的要求，即要求在设计实现模块时划分清楚基本功能和高级功能，保证基本功能不会依赖高级功能，这样同时就保证了不会因为高级功能出现故障而导致整个模块崩溃。根据这种理念实现的系统，也更容易快速地增加新的高级功能，因为不必担心引入高级功能影响原有的基本功能。</li></ul><h3 id="架构设计原则"><a href="#架构设计原则" class="headerlink" title="架构设计原则"></a>架构设计原则</h3><ul><li>只有apiserver可以直接访问etcd存储，其他服务必须通过Kubernetes API来访问集群状态</li><li>单节点故障不应该影响集群的状态</li><li>在没有新请求的情况下，所有组件应该在故障恢复后继续执行上次最后收到的请求（比如网络分区或服务重启等）</li><li>所有组件都应该在内存中保持所需要的状态，apiserver将状态写入etcd存储，而其他组件则通过apiserver更新并监听所有的变化</li><li>优先使用事件监听而不是轮询</li></ul><h3 id="引导（Bootstrapping）原则"><a href="#引导（Bootstrapping）原则" class="headerlink" title="引导（Bootstrapping）原则"></a>引导（Bootstrapping）原则</h3><ul><li><a href="http://issue.k8s.io/246" target="_blank" rel="noopener">Self-hosting</a> 是目标</li><li>减少依赖，特别是稳态运行的依赖</li><li>通过分层的原则管理依赖</li><li>循环依赖问题的原则<ul><li>同时还接受其他方式的数据输入（比如本地文件等），这样在其他服务不可用时还可以手动配置引导服务</li><li>状态应该是可恢复或可重新发现的</li><li>支持简单的启动临时实例来创建稳态运行所需要的状态；使用分布式锁或文件锁等来协调不同状态的切换（通常称为<code>pivoting</code>技术）</li><li>自动重启异常退出的服务，比如副本或者进程管理器等</li></ul></li></ul><h2 id="核心技术概念和API对象"><a href="#核心技术概念和API对象" class="headerlink" title="核心技术概念和API对象"></a>核心技术概念和API对象</h2><p>API对象是K8s集群中的管理操作单元。K8s集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。例如副本集Replica Set对应的API对象是RS。</p><p>每个API对象都有3大类属性：元数据metadata、规范spec和状态status。</p><p>元数据是用来标识API对象的，每个对象都至少有3个元数据：namespace，name和uid；除此以外还有各种各样的标签labels用来标识和匹配不同的对象，例如用户可以用标签env来标识区分不同的服务部署环境，分别用env=dev、env=testing、env=production来标识开发、测试、生产的不同服务。</p><p>规范描述了用户期望K8s集群中的分布式系统达到的理想状态（Desired State），例如用户可以通过复制控制器Replication Controller设置期望的Pod副本数为3；</p><p>status描述了系统实际当前达到的状态（Status），例如系统当前实际的Pod副本数为2；那么复本控制器当前的程序逻辑就是自动启动新的Pod，争取达到副本数为3。</p><p>K8s中所有的配置都是通过API对象的spec去设置的，也就是用户通过配置系统的理想状态来改变系统，这是k8s重要设计理念之一，即所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为3的操作运行多次也还是一个结果，而给副本数加1的操作就不是声明式的，运行多次结果就错了。</p><h3 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h3><p>K8s有很多技术概念，同时对应很多API对象，最重要的也是最基础的是微服务Pod。Pod是在K8s集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。Pod对多容器的支持是K8s最基础的设计理念。比如你运行一个操作系统发行版的软件仓库，一个Nginx容器用来发布软件，另一个容器专门用来从源仓库做同步，这两个容器的镜像不太可能是一个团队开发的，但是他们一块儿工作才能提供一个微服务；这种情况下，不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。</p><p>Pod是K8s集群中所有业务类型的基础，可以看作运行在K8s集群中的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前K8s中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为Deployment、Job、DaemonSet和StatefulSet，本文后面会一一介绍。</p><h3 id="复制控制器（Replication-Controller，RC）"><a href="#复制控制器（Replication-Controller，RC）" class="headerlink" title="复制控制器（Replication Controller，RC）"></a>复制控制器（Replication Controller，RC）</h3><p>RC是K8s集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就会启动运行新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有1个Pod在运行。RC是K8s较早期的技术概念，只适用于长期伺服型的业务类型，比如控制小机器人提供高可用的Web服务。</p><h3 id="副本集（Replica-Set，RS）"><a href="#副本集（Replica-Set，RS）" class="headerlink" title="副本集（Replica Set，RS）"></a>副本集（Replica Set，RS）</h3><p>RS是新一代RC，提供同样的高可用能力，区别主要在于RS后来居上，能支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数使用。</p><h3 id="部署-Deployment"><a href="#部署-Deployment" class="headerlink" title="部署(Deployment)"></a>部署(Deployment)</h3><p>部署表示用户对K8s集群的一次更新操作。部署是一个比RS应用模式更广的API对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的RS，然后逐渐将新RS中副本数增加到理想状态，将旧RS中的副本数减小到0的复合操作；这样一个复合操作用一个RS是不太好描述的，所以用一个更通用的Deployment来描述。以K8s的发展方向，未来对所有长期伺服型的的业务的管理，都会通过Deployment来管理。</p><h3 id="服务（Service）"><a href="#服务（Service）" class="headerlink" title="服务（Service）"></a>服务（Service）</h3><p>RC、RS和Deployment只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服务。要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。在K8s集群中，客户端需要访问的服务就是Service对象。每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务。在K8s集群中微服务的负载均衡是由Kube-proxy实现的。Kube-proxy是K8s集群内部的负载均衡器。它是一个分布式代理服务器，在K8s的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。与之相比，我们平时在服务器端使用反向代理作负载均衡，还要进一步解决反向代理的高可用问题。</p><h3 id="任务（Job）"><a href="#任务（Job）" class="headerlink" title="任务（Job）"></a>任务（Job）</h3><p>Job是K8s用来控制批处理型任务的API对象。批处理业务与长期伺服业务的主要区别是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job管理的Pod根据用户的设置把任务成功完成就自动退出了。成功完成的标志根据不同的spec.completions策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功型任务保证有N个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。</p><h3 id="后台支撑服务集（DaemonSet）"><a href="#后台支撑服务集（DaemonSet）" class="headerlink" title="后台支撑服务集（DaemonSet）"></a>后台支撑服务集（DaemonSet）</h3><p>长期伺服型和批处理型服务的核心在业务应用，可能有些节点运行多个同类业务的Pod，有些节点上又没有这类Pod运行；而后台支撑型服务的核心关注点在K8s集群中的节点（物理机或虚拟机），要保证每个节点上都有一个此类Pod运行。节点可能是所有集群节点也可能是通过nodeSelector选定的一些特定节点。典型的后台支撑型服务包括，存储，日志和监控等在每个节点上支撑K8s集群运行的服务。</p><h3 id="有状态服务集（StatefulSet）"><a href="#有状态服务集（StatefulSet）" class="headerlink" title="有状态服务集（StatefulSet）"></a>有状态服务集（StatefulSet）</h3><p>K8s在1.3版本里发布了Alpha版的PetSet以支持有状态服务，并从1.5版本开始重命名为StatefulSet。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。RC和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数；而StatefulSet是用来控制有状态服务，StatefulSet中的每个Pod的名字都是事先确定的，不能更改。StatefulSet中Pod的名字的作用，并不是《千与千寻》的人性原因，而是关联与该Pod对应的状态。</p><p>对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod像牲畜一样没有分别（这似乎也确实意味着失去了人性特征）；对于StatefulSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来Pod的存储继续以它的状态提供服务。</p><p>适合于StatefulSet的业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务。StatefulSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用StatefulSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，StatefulSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。StatefulSet还只在Alpha阶段，后面的设计如何演变，我们还要继续观察。</p><h3 id="集群联邦（Federation）"><a href="#集群联邦（Federation）" class="headerlink" title="集群联邦（Federation）"></a>集群联邦（Federation）</h3><p>K8s在1.3版本里发布了beta版的Federation功能。在云计算环境中，服务的作用距离范围从近到远一般可以有：同主机（Host，Node）、跨主机同可用区（Available Zone）、跨可用区同地区（Region）、跨地区同服务商（Cloud Service Provider）、跨云平台。K8s的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足K8s的调度和计算存储连接要求。而联合集群服务就是为提供跨Region跨服务商K8s集群服务而设计的。</p><p>每个K8s Federation有自己的分布式存储、API Server和Controller Manager。用户可以通过Federation的API Server注册该Federation的成员K8s Cluster。当用户通过Federation的API Server创建、更改API对象时，Federation API Server会在自己所有注册的子K8s Cluster都创建一份对应的API对象。在提供业务请求服务时，K8s Federation会先在自己的各个子Cluster之间做负载均衡，而对于发送到某个具体K8s Cluster的业务请求，会依照这个K8s Cluster独立提供服务时一样的调度模式去做K8s Cluster内部的负载均衡。而Cluster之间的负载均衡是通过域名服务的负载均衡来实现的。</p><p>所有的设计都尽量不影响K8s Cluster现有的工作机制，这样对于每个子K8s集群来说，并不需要更外层的有一个K8s Federation，也就是意味着所有现有的K8s代码和机制不需要因为Federation功能有任何变化。</p><h3 id="存储卷（Volume）"><a href="#存储卷（Volume）" class="headerlink" title="存储卷（Volume）"></a>存储卷（Volume）</h3><p>K8s集群中的存储卷跟Docker的存储卷有些类似，只不过Docker的存储卷作用范围为一个容器，而K8s的存储卷的生命周期和作用范围是一个Pod。每个Pod中声明的存储卷由Pod中的所有容器共享。K8s支持非常多的存储卷类型，特别的，支持多种公有云平台的存储，包括AWS，Google和Azure云；支持多种分布式存储包括GlusterFS和Ceph；也支持较容易使用的主机本地目录hostPath和NFS。K8s还支持使用Persistent Volume Claim即PVC这种逻辑存储，使用这种存储，使得存储的使用者可以忽略后台的实际存储技术（例如AWS，Google或GlusterFS和Ceph），而将有关存储实际技术的配置交给存储管理员通过Persistent Volume来配置。</p><h3 id="持久存储卷（Persistent-Volume，PV）和持久存储卷声明（Persistent-Volume-Claim，PVC）"><a href="#持久存储卷（Persistent-Volume，PV）和持久存储卷声明（Persistent-Volume-Claim，PVC）" class="headerlink" title="持久存储卷（Persistent Volume，PV）和持久存储卷声明（Persistent Volume Claim，PVC）"></a>持久存储卷（Persistent Volume，PV）和持久存储卷声明（Persistent Volume Claim，PVC）</h3><p>PV和PVC使得K8s集群具备了存储的逻辑抽象能力，使得在配置Pod的逻辑里可以忽略对实际后台存储技术的配置，而把这项配置的工作交给PV的配置者，即集群的管理者。存储的PV和PVC的这种关系，跟计算的Node和Pod的关系是非常类似的；PV和Node是资源的提供者，根据集群的基础设施变化而变化，由K8s集群管理员配置；而PVC和Pod是资源的使用者，根据业务服务的需求变化而变化，由K8s集群的使用者即服务的管理员来配置。</p><h3 id="节点（Node）"><a href="#节点（Node）" class="headerlink" title="节点（Node）"></a>节点（Node）</h3><p>K8s集群中的计算能力由Node提供，最初Node称为服务节点Minion，后来改名为Node。K8s集群中的Node也就等同于Mesos集群中的Slave节点，是所有Pod运行所在的工作主机，可以是物理机也可以是虚拟机。不论是物理机还是虚拟机，工作主机的统一特征是上面要运行kubelet管理节点上运行的容器。</p><h3 id="密钥对象（Secret）"><a href="#密钥对象（Secret）" class="headerlink" title="密钥对象（Secret）"></a>密钥对象（Secret）</h3><p>Secret是用来保存和传递密码、密钥、认证凭证这些敏感信息的对象。使用Secret的好处是可以避免把敏感信息明文写在配置文件里。在K8s集群中配置和使用服务不可避免的要用到各种敏感信息实现登录、认证等功能，例如访问AWS存储的用户名密码。为了避免将类似的敏感信息明文写在所有需要使用的配置文件中，可以将这些信息存入一个Secret对象，而在配置文件中通过Secret对象引用这些敏感信息。这种方式的好处包括：意图明确，避免重复，减少暴露机会。</p><h3 id="用户帐户（User-Account）和服务帐户（Service-Account）"><a href="#用户帐户（User-Account）和服务帐户（Service-Account）" class="headerlink" title="用户帐户（User Account）和服务帐户（Service Account）"></a>用户帐户（User Account）和服务帐户（Service Account）</h3><p>顾名思义，用户帐户为人提供账户标识，而服务账户为计算机进程和K8s集群中运行的Pod提供账户标识。用户帐户和服务帐户的一个区别是作用范围；用户帐户对应的是人的身份，人的身份与服务的namespace无关，所以用户账户是跨namespace的；而服务帐户对应的是一个运行中程序的身份，与特定namespace是相关的。</p><h3 id="名字空间（Namespace）"><a href="#名字空间（Namespace）" class="headerlink" title="名字空间（Namespace）"></a>名字空间（Namespace）</h3><p>名字空间为K8s集群提供虚拟的隔离作用，K8s集群初始有两个名字空间，分别是默认名字空间default和系统名字空间kube-system，除此以外，管理员可以创建新的名字空间满足需要。</p><h3 id="RBAC访问授权"><a href="#RBAC访问授权" class="headerlink" title="RBAC访问授权"></a>RBAC访问授权</h3><p>K8s在1.3版本中发布了alpha版的基于角色的访问控制（Role-based Access Control，RBAC）的授权模式。相对于基于属性的访问控制（Attribute-based Access Control，ABAC），RBAC主要是引入了角色（Role）和角色绑定（RoleBinding）的抽象概念。在ABAC中，K8s集群中的访问策略只能跟用户直接关联；而在RBAC中，访问策略可以跟某个角色关联，具体的用户在跟一个或多个角色相关联。显然，RBAC像其他新功能一样，每次引入新功能，都会引入新的API对象，从而引入新的概念抽象，而这一新的概念抽象一定会使集群服务管理和使用更容易扩展和重用。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从K8s的系统架构、技术概念和设计理念，我们可以看到K8s系统最核心的两个设计理念：一个是<strong>容错性</strong>，一个是<strong>易扩展性</strong>。容错性实际是保证K8s系统稳定性和安全性的基础，易扩展性是保证K8s对变更友好，可以快速迭代增加新功能的基础。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/principles.md" target="_blank" rel="noopener">Kubernetes Design Principles</a></li><li><a href="http://www.infoq.com/cn/articles/kubernetes-and-cloud-native-applications-part01" target="_blank" rel="noopener">Kubernetes与云原生应用</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>k8s Api</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>network-policy-分析</title>
    <link href="/2019/05/20/network-policy-%E5%88%86%E6%9E%90/"/>
    <url>/2019/05/20/network-policy-%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h2 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h2><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">kind</span>: NetworkPolicy （资源类型）<br><span class="hljs-attribute">apiVersion</span>: networking.k8s.io/v1 （api版本号）<br><span class="hljs-attribute">metadata</span>:<br>  <span class="hljs-attribute">name</span>: allow-same-namespace  （这条策略的名字）<br>  <span class="hljs-attribute">namespace</span>: default  （对应到的名字空间）<br><span class="hljs-attribute">spec</span>:<br>  <span class="hljs-attribute">podSelector</span>:    （下发的范围选择器）<br>    <span class="hljs-attribute">matchLabels</span>:   （匹配具有如下Lable的） <br>      <span class="hljs-attribute">color</span>: blue<br>  <span class="hljs-attribute">ingress</span>:          （入站策略）<br>  - <span class="hljs-attribute">from</span>:            （允许哪些src访问)<br>    - <span class="hljs-attribute">podSelector</span>:   （又是选择器）<br>        <span class="hljs-attribute">color</span>: red<br>    <span class="hljs-attribute">to</span>:              （目的端口）<br>      <span class="hljs-attribute">ports</span>:<br>      - <span class="hljs-attribute">port</span>: <span class="hljs-number">80</span><br></code></pre></td></tr></table></figure><p>这条就是，允许red访问blue的80端口</p><h2 id="还可以定义允许别的名字空间的选择器"><a href="#还可以定义允许别的名字空间的选择器" class="headerlink" title="还可以定义允许别的名字空间的选择器"></a>还可以定义允许别的名字空间的选择器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs language">ingress:<br>- from:<br>  - podSelector:<br>      color: red<br>    namespaceSelector: <br>      shape: square<br></code></pre></td></tr></table></figure><h2 id="出站策略"><a href="#出站策略" class="headerlink" title="出站策略"></a>出站策略</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs language">egress:<br>- to:<br>  - podSelector: <br>      color: red<br>    ports:<br>    - port: 80<br></code></pre></td></tr></table></figure><h2 id="配置ip地址模式"><a href="#配置ip地址模式" class="headerlink" title="配置ip地址模式"></a>配置ip地址模式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs language">egress:<br>- to:<br>  - ipBlock:<br>      cidr: 172.18.0.0/24<br></code></pre></td></tr></table></figure><h2 id="deny-all"><a href="#deny-all" class="headerlink" title="deny all"></a>deny all</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs language">kind: NetworkPolicy<br>apiVersion: networking.k8s.io/v1<br>metadata:<br>  name: default-deny<br>  namespace: policy-demo<br>spec:<br>  podSelector:<br>    matchLabels: &#123;&#125;<br>  types:<br>  - Ingress<br>  - Egress<br></code></pre></td></tr></table></figure><p>以下配置成功了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs language">apiVersion: networking.k8s.io/v1<br>kind: NetworkPolicy<br>metadata:<br>  name: test-network-policy<br>  namespace: default<br>spec:<br>  podSelector:<br>    matchLabels:<br>      role: db<br>  policyTypes:<br>  - Ingress<br>  - Egress<br>  ingress:<br>  - from:<br>    - ipBlock:<br>        cidr: 172.17.0.0/16<br>        except:<br>        - 172.17.1.0/24<br>    - namespaceSelector:<br>        matchLabels:<br>          project: myproject<br>    - podSelector:<br>        matchLabels:<br>          role: frontend<br>    ports:<br>    - protocol: TCP<br>      port: 6379<br>  egress:<br>  - to:<br>    - ipBlock:<br>        cidr: 10.0.0.0/24<br>    ports:<br>    - protocol: TCP<br>      port: 5978<br></code></pre></td></tr></table></figure></p><p>查看具体描述<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">kubectl describe networkpolicy</span><br></code></pre></td></tr></table></figure></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/services-networking/network-policies/</a><br><a href="https://docs.projectcalico.org/master/security/kubernetes-network-policy#create-deny-all-default-ingress-and-egress-network-policy" target="_blank" rel="noopener">https://docs.projectcalico.org/master/security/kubernetes-network-policy#create-deny-all-default-ingress-and-egress-network-policy</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>k8s calico networkpolicy ingress engress</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于kubernetes的网络策略</title>
    <link href="/2019/05/20/%E5%9F%BA%E4%BA%8Ekubernetes%E7%9A%84%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/"/>
    <url>/2019/05/20/%E5%9F%BA%E4%BA%8Ekubernetes%E7%9A%84%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/</url>
    
    <content type="html"><![CDATA[<p><a href="https://kubernetes.feisky.xyz/he-xin-yuan-li/index-2/network-policy" target="_blank" rel="noopener">network-policy</a></p><p>Network Policy 提供了基于策略的网络控制，用于隔离应用并减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量。Network Policy 需要网络插件来监测这些策略和 Pod 的变更，并为 Pod 配置流量控制。</p><h2 id="策略特性"><a href="#策略特性" class="headerlink" title="策略特性"></a>策略特性</h2><p>The Kubernetes Network Policy API supports the following features:</p><ul><li>Policies are namespace scoped</li><li>Policies are applied to pods using label selectors</li><li>Policy rules can specify the connections that are allowed to/from pods, namespaces, or CIDRs</li><li>Policy rules can specify protocols (TCP, UDP, SCTP), named ports or port numbers</li></ul><p>在使用 Network Policy 时，需要注意</p><ul><li>v1.6 以及以前的版本需要在 kube-apiserver 中开启 <code>extensions/v1beta1/networkpolicies</code></li><li>v1.7 版本 Network Policy 已经 GA，API 版本为 <code>networking.k8s.io/v1</code></li><li>v1.8 版本新增 <strong>Egress</strong> 和 <strong>IPBlock</strong> 的支持</li><li>网络插件要支持 Network Policy，如 Calico、Romana、Weave Net 和 trireme 等</li></ul><h2 id="API-版本对照表"><a href="#API-版本对照表" class="headerlink" title="API 版本对照表"></a>API 版本对照表</h2><table><thead><tr><th>Kubernetes 版本</th><th>Networking API 版本</th></tr></thead><tbody><tr><td>v1.5-v1.6</td><td>extensions/v1beta1</td></tr><tr><td>v1.7+</td><td>networking.k8s.io/v1</td></tr></tbody></table><h2 id="网络策略"><a href="#网络策略" class="headerlink" title="网络策略"></a>网络策略</h2><h3 id="Namespace-隔离"><a href="#Namespace-隔离" class="headerlink" title="Namespace 隔离"></a>Namespace 隔离</h3><p>==默认情况下，所有 Pod 之间是全通的==。</p><p>每个 Namespace 可以配置独立的网络策略，来隔离 Pod 之间的流量。</p><p>默认 Pod 是未隔离的，它们可以从任何的源接收请求。 具有一个可以选择 Pod 的网络策略后，Pod 就会变成隔离的。</p><p>v1.7 + 版本通过创建匹配所有 Pod 的 Network Policy 来作为默认的网络策略，比如默认拒绝所有 Pod 之间 Ingress 通信</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">default-deny</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span> <span class="hljs-string">&#123;&#125;</span><br><span class="hljs-attr">  policyTypes:</span><br><span class="hljs-bullet">  -</span> <span class="hljs-string">Ingress</span><br></code></pre></td></tr></table></figure><p>默认拒绝所有 Pod 之间 Egress 通信的策略为</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">default-deny</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span> <span class="hljs-string">&#123;&#125;</span><br><span class="hljs-attr">  policyTypes:</span><br><span class="hljs-bullet">  -</span> <span class="hljs-string">Egress</span><br></code></pre></td></tr></table></figure><p>甚至是默认拒绝所有 Pod 之间 Ingress 和 Egress 通信的策略为</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">default-deny</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span> <span class="hljs-string">&#123;&#125;</span><br><span class="hljs-attr">  policyTypes:</span><br><span class="hljs-bullet">  -</span> <span class="hljs-string">Ingress</span><br><span class="hljs-bullet">  -</span> <span class="hljs-string">Egress</span><br></code></pre></td></tr></table></figure><p>而默认允许所有 Pod 之间 Ingress 通信的策略为</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">allow-all</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span> <span class="hljs-string">&#123;&#125;</span><br><span class="hljs-attr">  ingress:</span><br><span class="hljs-bullet">  -</span> <span class="hljs-string">&#123;&#125;</span><br></code></pre></td></tr></table></figure><p>默认允许所有 Pod 之间 Egress 通信的策略为</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">allow-all</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span> <span class="hljs-string">&#123;&#125;</span><br><span class="hljs-attr">  egress:</span><br><span class="hljs-bullet">  -</span> <span class="hljs-string">&#123;&#125;</span><br></code></pre></td></tr></table></figure><p>而 v1.6 版本则通过 Annotation 来隔离 namespace 的所有 Pod 之间的流量，包括从外部到该 namespace 中所有 Pod 的流量以及 namespace 内部 Pod 相互之间的流量：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubectl annotate ns &lt;namespace&gt; <span class="hljs-string">"net.beta.kubernetes.io/network-policy=&#123;\"ingress\": &#123;\"isolation\": \"DefaultDeny\"&#125;&#125;"</span><br></code></pre></td></tr></table></figure><h3 id="Pod-隔离"><a href="#Pod-隔离" class="headerlink" title="Pod 隔离"></a>Pod 隔离</h3><p>通过使用标签选择器（包括 namespaceSelector 和 podSelector）来控制 Pod 之间的流量。比如下面的 Network Policy</p><ul><li>允许 default namespace 中带有 <code>role=frontend</code> 标签的 Pod 访问 default namespace 中带有 <code>role=db</code> 标签 Pod 的 6379 端口</li><li>允许带有 <code>project=myprojects</code> 标签的 namespace 中所有 Pod 访问 default namespace 中带有 <code>role=db</code> 标签 Pod 的 6379 端口</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># v1.6 以及更老的版本应该使用 extensions/v1beta1</span><br><span class="hljs-comment"># apiVersion: extensions/v1beta1</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">test-network-policy</span><br><span class="hljs-attr">  namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span><br><span class="hljs-attr">    matchLabels:</span><br><span class="hljs-attr">      role:</span> <span class="hljs-string">db</span><br><span class="hljs-attr">  ingress:</span><br><span class="hljs-attr">  - from:</span><br><span class="hljs-attr">    - namespaceSelector:</span><br><span class="hljs-attr">        matchLabels:</span><br><span class="hljs-attr">          project:</span> <span class="hljs-string">myproject</span><br><span class="hljs-attr">    - podSelector:</span><br><span class="hljs-attr">        matchLabels:</span><br><span class="hljs-attr">          role:</span> <span class="hljs-string">frontend</span><br><span class="hljs-attr">    ports:</span><br><span class="hljs-attr">    - protocol:</span> <span class="hljs-string">tcp</span><br><span class="hljs-attr">      port:</span> <span class="hljs-number">6379</span><br></code></pre></td></tr></table></figure><p>另外一个同时开启 Ingress 和 Egress 通信的策略为</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">test-network-policy</span><br><span class="hljs-attr">  namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span><br><span class="hljs-attr">    matchLabels:</span><br><span class="hljs-attr">      role:</span> <span class="hljs-string">db</span><br><span class="hljs-attr">  policyTypes:</span><br><span class="hljs-bullet">  -</span> <span class="hljs-string">Ingress</span><br><span class="hljs-bullet">  -</span> <span class="hljs-string">Egress</span><br><span class="hljs-attr">  ingress:</span><br><span class="hljs-attr">  - from:</span><br><span class="hljs-attr">    - ipBlock:</span><br><span class="hljs-attr">        cidr:</span> <span class="hljs-number">172.17</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/16</span><br><span class="hljs-attr">        except:</span><br><span class="hljs-bullet">        -</span> <span class="hljs-number">172.17</span><span class="hljs-number">.1</span><span class="hljs-number">.0</span><span class="hljs-string">/24</span><br><span class="hljs-attr">    - namespaceSelector:</span><br><span class="hljs-attr">        matchLabels:</span><br><span class="hljs-attr">          project:</span> <span class="hljs-string">myproject</span><br><span class="hljs-attr">    - podSelector:</span><br><span class="hljs-attr">        matchLabels:</span><br><span class="hljs-attr">          role:</span> <span class="hljs-string">frontend</span><br><span class="hljs-attr">    ports:</span><br><span class="hljs-attr">    - protocol:</span> <span class="hljs-string">TCP</span><br><span class="hljs-attr">      port:</span> <span class="hljs-number">6379</span><br><span class="hljs-attr">  egress:</span><br><span class="hljs-attr">  - to:</span><br><span class="hljs-attr">    - ipBlock:</span><br><span class="hljs-attr">        cidr:</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/24</span><br><span class="hljs-attr">    ports:</span><br><span class="hljs-attr">    - protocol:</span> <span class="hljs-string">TCP</span><br><span class="hljs-attr">      port:</span> <span class="hljs-number">5978</span><br></code></pre></td></tr></table></figure><p>它用来隔离 default namespace 中带有 <code>role=db</code> 标签的 Pod：</p><ul><li>允许 default namespace 中带有 <code>role=frontend</code> 标签的 Pod 访问 default namespace 中带有 <code>role=db</code> 标签 Pod 的 6379 端口</li><li>允许带有 <code>project=myprojects</code> 标签的 namespace 中所有 Pod 访问 default namespace 中带有 <code>role=db</code> 标签 Pod 的 6379 端口</li><li>允许 default namespace 中带有 <code>role=db</code> 标签的 Pod 访问 <code>10.0.0.0/24</code> 网段的 TCP 5987 端口</li></ul><h2 id="简单示例"><a href="#简单示例" class="headerlink" title="简单示例"></a>简单示例</h2><p>以 calico 为例看一下 Network Policy 的具体用法。</p><p>首先配置 kubelet 使用 CNI 网络插件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubelet --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin ...<br></code></pre></td></tr></table></figure><p>安装 calio 网络插件</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 注意修改 CIDR，需要跟 k8s pod-network-cidr 一致，默认为 192.168.0.0/16</span><br>kubectl apply -f https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml<br></code></pre></td></tr></table></figure><p>首先部署一个 nginx 服务</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ kubectl run nginx --image=nginx --replicas=2<br>deployment <span class="hljs-string">"nginx"</span> created<br>$ kubectl expose deployment nginx --port=80<br>service <span class="hljs-string">"nginx"</span> exposed<br></code></pre></td></tr></table></figure><p>此时，通过其他 Pod 是可以访问 nginx 服务的</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ kubectl get svc,pod<br>NAME                        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE<br>svc/kubernetes              10.100.0.1    &lt;none&gt;        443/TCP    46m<br>svc/nginx                   10.100.0.16   &lt;none&gt;        80/TCP     33s<br><br>NAME                        READY         STATUS        RESTARTS   AGE<br>po/nginx-701339712-e0qfq    1/1           Running       0          35s<br>po/nginx-701339712-o00ef    1/1           Running       0<br><br>$ kubectl run busybox --rm -ti --image=busybox /bin/sh<br>Waiting <span class="hljs-keyword">for</span> pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: <span class="hljs-literal">false</span><br><br>Hit enter <span class="hljs-keyword">for</span> <span class="hljs-built_in">command</span> prompt<br><br>/ <span class="hljs-comment"># wget --spider --timeout=1 nginx</span><br>Connecting to nginx (10.100.0.16:80)<br>/ <span class="hljs-comment">#</span><br></code></pre></td></tr></table></figure><p>开启 default namespace 的 DefaultDeny Network Policy 后，其他 Pod（包括 namespace 外部）不能访问 nginx 了：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ cat default-deny.yaml<br>apiVersion: networking.k8s.io/v1<br>kind: NetworkPolicy<br>metadata:<br>  name: default-deny<br>spec:<br>  podSelector: &#123;&#125;<br>  policyTypes:<br>  - Ingress<br><br>$ kubectl create -f default-deny.yaml<br><br>$ kubectl run busybox --rm -ti --image=busybox /bin/sh<br>Waiting <span class="hljs-keyword">for</span> pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: <span class="hljs-literal">false</span><br><br>Hit enter <span class="hljs-keyword">for</span> <span class="hljs-built_in">command</span> prompt<br><br>/ <span class="hljs-comment"># wget --spider --timeout=1 nginx</span><br>Connecting to nginx (10.100.0.16:80)<br>wget: download timed out<br>/ <span class="hljs-comment">#</span><br></code></pre></td></tr></table></figure><p>最后再创建一个运行带有 <code>access=true</code> 的 Pod 访问的网络策略</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ cat nginx-policy.yaml<br>kind: NetworkPolicy<br>apiVersion: networking.k8s.io/v1<br>metadata:<br>  name: access-nginx<br>spec:<br>  podSelector:<br>    matchLabels:<br>      run: nginx<br>  ingress:<br>  - from:<br>    - podSelector:<br>        matchLabels:<br>          access: <span class="hljs-string">"true"</span><br><br>$ kubectl create -f nginx-policy.yaml<br>networkpolicy <span class="hljs-string">"access-nginx"</span> created<br><br><span class="hljs-comment"># 不带 access=true 标签的 Pod 还是无法访问 nginx 服务</span><br>$ kubectl run busybox --rm -ti --image=busybox /bin/sh<br>Waiting <span class="hljs-keyword">for</span> pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: <span class="hljs-literal">false</span><br><br>Hit enter <span class="hljs-keyword">for</span> <span class="hljs-built_in">command</span> prompt<br><br>/ <span class="hljs-comment"># wget --spider --timeout=1 nginx</span><br>Connecting to nginx (10.100.0.16:80)<br>wget: download timed out<br>/ <span class="hljs-comment">#</span><br><br><br><span class="hljs-comment"># 而带有 access=true 标签的 Pod 可以访问 nginx 服务</span><br>$ kubectl run busybox --rm -ti --labels=<span class="hljs-string">"access=true"</span> --image=busybox /bin/sh<br>Waiting <span class="hljs-keyword">for</span> pod default/busybox-472357175-y0m47 to be running, status is Pending, pod ready: <span class="hljs-literal">false</span><br><br>Hit enter <span class="hljs-keyword">for</span> <span class="hljs-built_in">command</span> prompt<br><br>/ <span class="hljs-comment"># wget --spider --timeout=1 nginx</span><br>Connecting to nginx (10.100.0.16:80)<br>/ <span class="hljs-comment">#</span><br></code></pre></td></tr></table></figure><p>最后开启 nginx 服务的外部访问：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs sh">$ cat nginx-external-policy.yaml<br>apiVersion: networking.k8s.io/v1<br>kind: NetworkPolicy<br>metadata:<br>  name: front-end-access<br>  namespace: sock-shop<br>spec:<br>  podSelector:<br>    matchLabels:<br>      run: nginx<br>  ingress:<br>    - ports:<br>        - protocol: TCP<br>          port: 80<br><br>$ kubectl create -f nginx-external-policy.yaml<br></code></pre></td></tr></table></figure><h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><h3 id="禁止访问指定服务"><a href="#禁止访问指定服务" class="headerlink" title="禁止访问指定服务"></a>禁止访问指定服务</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubectl run web --image=nginx --labels app=web,env=prod --expose --port 80<br></code></pre></td></tr></table></figure><p><img src="../../images/15022447799137.jpg" srcset="/img/loading.gif" lazyload alt></p><p>网络策略</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">web-deny-all</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span><br><span class="hljs-attr">    matchLabels:</span><br><span class="hljs-attr">      app:</span> <span class="hljs-string">web</span><br><span class="hljs-attr">      env:</span> <span class="hljs-string">prod</span><br></code></pre></td></tr></table></figure><h3 id="只允许指定-Pod-访问服务"><a href="#只允许指定-Pod-访问服务" class="headerlink" title="只允许指定 Pod 访问服务"></a>只允许指定 Pod 访问服务</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubectl run apiserver --image=nginx --labels app=bookstore,role=api --expose --port 80<br></code></pre></td></tr></table></figure><p><img src="../../images/15022448622429.jpg" srcset="/img/loading.gif" lazyload alt></p><p>网络策略</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">api-allow</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span><br><span class="hljs-attr">    matchLabels:</span><br><span class="hljs-attr">      app:</span> <span class="hljs-string">bookstore</span><br><span class="hljs-attr">      role:</span> <span class="hljs-string">api</span><br><span class="hljs-attr">  ingress:</span><br><span class="hljs-attr">  - from:</span><br><span class="hljs-attr">      - podSelector:</span><br><span class="hljs-attr">          matchLabels:</span><br><span class="hljs-attr">            app:</span> <span class="hljs-string">bookstore</span><br></code></pre></td></tr></table></figure><h3 id="禁止-namespace-中所有-Pod-之间的相互访问"><a href="#禁止-namespace-中所有-Pod-之间的相互访问" class="headerlink" title="禁止 namespace 中所有 Pod 之间的相互访问"></a>禁止 namespace 中所有 Pod 之间的相互访问</h3><p><img src="../../images/15022451724392.gif" srcset="/img/loading.gif" lazyload alt></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">default-deny</span><br><span class="hljs-attr">  namespace:</span> <span class="hljs-string">default</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span> <span class="hljs-string">&#123;&#125;</span><br></code></pre></td></tr></table></figure><h3 id="禁止其他-namespace-访问服务"><a href="#禁止其他-namespace-访问服务" class="headerlink" title="禁止其他 namespace 访问服务"></a>禁止其他 namespace 访问服务</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubectl create namespace secondary<br>kubectl run web --namespace secondary --image=nginx \<br>    --labels=app=web --expose --port 80<br></code></pre></td></tr></table></figure><p><img src="../../images/15022452203435.gif" srcset="/img/loading.gif" lazyload alt></p><p>网络策略</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  namespace:</span> <span class="hljs-string">secondary</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">web-deny-other-namespaces</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span><br><span class="hljs-attr">    matchLabels:</span><br><span class="hljs-attr">  ingress:</span><br><span class="hljs-attr">  - from:</span><br><span class="hljs-attr">    - podSelector:</span> <span class="hljs-string">&#123;&#125;</span><br></code></pre></td></tr></table></figure><h3 id="只允许指定-namespace-访问服务"><a href="#只允许指定-namespace-访问服务" class="headerlink" title="只允许指定 namespace 访问服务"></a>只允许指定 namespace 访问服务</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubectl run web --image=nginx \<br>    --labels=app=web --expose --port 80<br></code></pre></td></tr></table></figure><p><img src="../../images/15022453441751.gif" srcset="/img/loading.gif" lazyload alt></p><p>网络策略</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">web-allow-prod</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span><br><span class="hljs-attr">    matchLabels:</span><br><span class="hljs-attr">      app:</span> <span class="hljs-string">web</span><br><span class="hljs-attr">  ingress:</span><br><span class="hljs-attr">  - from:</span><br><span class="hljs-attr">    - namespaceSelector:</span><br><span class="hljs-attr">        matchLabels:</span><br><span class="hljs-attr">          purpose:</span> <span class="hljs-string">production</span><br></code></pre></td></tr></table></figure><h3 id="允许外网访问服务"><a href="#允许外网访问服务" class="headerlink" title="允许外网访问服务"></a>允许外网访问服务</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">kubectl run web --image=nginx --labels=app=web --port 80<br>kubectl expose deployment/web --<span class="hljs-built_in">type</span>=LoadBalancer<br></code></pre></td></tr></table></figure><p><img src="../../images/15022454444461.gif" srcset="/img/loading.gif" lazyload alt></p><p>网络策略</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">NetworkPolicy</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span><br><span class="hljs-attr">metadata:</span><br><span class="hljs-attr">  name:</span> <span class="hljs-string">web-allow-external</span><br><span class="hljs-attr">spec:</span><br><span class="hljs-attr">  podSelector:</span><br><span class="hljs-attr">    matchLabels:</span><br><span class="hljs-attr">      app:</span> <span class="hljs-string">web</span><br><span class="hljs-attr">  ingress:</span><br><span class="hljs-attr">  - ports:</span><br><span class="hljs-attr">    - port:</span> <span class="hljs-number">80</span><br><span class="hljs-attr">    from:</span> <span class="hljs-string">[]</span><br></code></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/" target="_blank" rel="noopener">Kubernetes network policies</a></li><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/" target="_blank" rel="noopener">Declare Network Policy</a></li><li><a href="https://ahmet.im/blog/kubernetes-network-policy/" target="_blank" rel="noopener">Securing Kubernetes Cluster Networking</a></li><li><a href="https://github.com/ahmetb/kubernetes-networkpolicy-tutorial" target="_blank" rel="noopener">Kubernetes Network Policy Recipes</a></li><li><a href="http://docs.kubernetes.org.cn/694.html" target="_blank" rel="noopener">http://docs.kubernetes.org.cn/694.html</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>k8s networkpolicy calico</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在kubernetes上部署Calico网络插件</title>
    <link href="/2019/05/20/%E5%9C%A8kubernetes%E4%B8%8A%E9%83%A8%E7%BD%B2Calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6/"/>
    <url>/2019/05/20/%E5%9C%A8kubernetes%E4%B8%8A%E9%83%A8%E7%BD%B2Calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="1-创建单主机Kubernetes集群"><a href="#1-创建单主机Kubernetes集群" class="headerlink" title="1.创建单主机Kubernetes集群"></a>1.创建单主机Kubernetes集群</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">参考【使用kubeadm安装Kubernetes】<br></code></pre></td></tr></table></figure><ol><li><p>作为具有sudo权限的普通用户，请在安装了kubeadm的主机上打开终端。</p></li><li><p>使用以下命令初始化主服务器。</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">sudo kubeadm init --pod-network-cidr=<span class="hljs-number">192.168</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">16</span><br></code></pre></td></tr></table></figure></li></ol><p>注意：如果您的网络中已在使用192.168.0.0/16，则必须选择不同的pod网络CIDR，在上述命令中替换192.168.0.0/16以及下面应用的任何清单中。</p><ol start="3"><li><p>执行以下命令配置kubectl（也返回 kubeadm init）。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs undefined">mkdir -p <span class="hljs-variable">$HOME</span>/.kube<br>sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube<span class="hljs-built_in">/config<br></span>sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube<span class="hljs-built_in">/config<br></span>`<br></code></pre></td></tr></table></figure></li><li><p>使用以下命令安装Calico。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl apply -f \<br>https:<span class="hljs-regexp">//</span>docs.projectcalico.org<span class="hljs-regexp">/v3.6/g</span>etting-started<span class="hljs-regexp">/kubernetes/i</span>nstallation<span class="hljs-regexp">/hosted/</span>kubernetes-datastore<span class="hljs-regexp">/calico-networking/</span><span class="hljs-number">1.7</span><span class="hljs-regexp">/calico.yaml</span><br></code></pre></td></tr></table></figure></li></ol><p>注意：您还可以 在新选项卡中查看YAML。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs undefined">configmap/calico-config created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/felixconfigurations<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/ipamblocks<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/blockaffinities<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/ipamhandles<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/bgppeers<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/bgpconfigurations<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/ippools<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/hostendpoints<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/clusterinformations<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/globalnetworkpolicies<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/globalnetworksets<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>customresourcedefinition<span class="hljs-selector-class">.apiextensions</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/networkpolicies<span class="hljs-selector-class">.crd</span><span class="hljs-selector-class">.projectcalico</span><span class="hljs-selector-class">.org</span> created<br>clusterrole<span class="hljs-selector-class">.rbac</span><span class="hljs-selector-class">.authorization</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/calico-kube-controllers created<br>clusterrolebinding<span class="hljs-selector-class">.rbac</span><span class="hljs-selector-class">.authorization</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/calico-kube-controllers created<br>clusterrole<span class="hljs-selector-class">.rbac</span><span class="hljs-selector-class">.authorization</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/calico-node created<br>clusterrolebinding<span class="hljs-selector-class">.rbac</span><span class="hljs-selector-class">.authorization</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/calico-node created<br>daemonset.extensions/calico-node created<br>serviceaccount/calico-node created<br>deployment.extensions/calico-kube-controllers created<br>serviceaccount/calico-kube-controllers created<br></code></pre></td></tr></table></figure><ol start="5"><li>使用以下命令确认所有窗格正在运行。<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">watch kubectl <span class="hljs-keyword">get</span> pods <span class="hljs-comment">--all-namespaces</span><br></code></pre></td></tr></table></figure></li></ol><p>等到每个吊舱具有STATUS的Running。</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs undefined">NAMESPACE     NAME                                       READY   STATUS             RESTARTS   AGE<br><span class="hljs-section">default</span>       curl<span class="hljs-number">-66959</span>f6557-wrctx                      <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">0</span>          <span class="hljs-number">34</span>m<br>kube-system   calico-kube-controllers<span class="hljs-number">-644</span>fcf8fbf<span class="hljs-number">-8</span>w7m5   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">0</span>          <span class="hljs-number">15</span>m<br>kube-system   calico-node-c7fkl                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">0</span>          <span class="hljs-number">15</span>m<br>kube-system   calico-node-fkvv7                          <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">0</span>          <span class="hljs-number">15</span>m<br>kube-system   coredns<span class="hljs-number">-86</span>c58d9df4-k5gtz                   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">0</span>          <span class="hljs-number">86</span>m<br>kube-system   coredns<span class="hljs-number">-86</span>c58d9df4-sgnk6                   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">0</span>          <span class="hljs-number">86</span>m<br>kube-system   etcd-k8s-master                            <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">1</span>          <span class="hljs-number">85</span>m<br>kube-system   kube-apiserver-k8s-master                  <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">1</span>          <span class="hljs-number">85</span>m<br>kube-system   kube-controller-manager-k8s-master         <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">1</span>          <span class="hljs-number">85</span>m<br>kube-system   kube-flannel-ds-amd64-gngrk                <span class="hljs-number">0</span>/<span class="hljs-number">1</span>     CrashLoopBackOff   <span class="hljs-number">17</span>         <span class="hljs-number">44</span>m<br>kube-system   kube-flannel-ds-amd64-hgftp                <span class="hljs-number">0</span>/<span class="hljs-number">1</span>     CrashLoopBackOff   <span class="hljs-number">13</span>         <span class="hljs-number">44</span>m<br>kube-system   kube-proxy<span class="hljs-number">-78</span>sj6                           <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">1</span>          <span class="hljs-number">86</span>m<br>kube-system   kube-proxy-xwbxv                           <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">2</span>          <span class="hljs-number">86</span>m<br>kube-system   kube-scheduler-k8s-master                  <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running            <span class="hljs-number">2</span>          <span class="hljs-number">85</span>m<br></code></pre></td></tr></table></figure><ol start="6"><li><p>按CTRL + C退出watch。</p></li><li><p>删除主服务器上的污点，以便您可以在其上安排pod。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl taint nodes --all <span class="hljs-keyword">node</span><span class="hljs-title">-role</span>.kubernetes.io/<span class="hljs-literal">master</span>-<br></code></pre></td></tr></table></figure></li></ol><p>它应该返回以下内容。<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-keyword">node</span><span class="hljs-title">/&lt;your-hostname</span>&gt; untainted<br></code></pre></td></tr></table></figure></p><ol start="8"><li>使用以下命令确认您现在在群集中有一个节点。<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-builtin-name">get</span> nodes -o wide<br></code></pre></td></tr></table></figure></li></ol><p>它应该返回类似下面的内容。</p><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@k8s-master ~]# kubectl get nodes -o wide<br>NAME         STATUS   ROLES    AGE   VERSION   INTERNAL-<span class="hljs-built_in">IP</span>   EXTERNAL-<span class="hljs-built_in">IP</span>   OS-IMAGE                KERNEL-VERSION              CONTAINER-RUNTIME<br>k8s-master   Ready    master   92m   v1<span class="hljs-meta">.13</span><span class="hljs-meta">.4</span>   <span class="hljs-number">10.166</span><span class="hljs-meta">.0</span><span class="hljs-meta">.4</span>    &lt;none&gt;        CentOS Linux <span class="hljs-number">7</span> (Core)   <span class="hljs-number">3.10</span><span class="hljs-meta">.0</span>-<span class="hljs-number">957.5</span><span class="hljs-meta">.1</span>.el7.x86_64   docker://<span class="hljs-number">18.9</span><span class="hljs-meta">.3</span><br>k8s-node     Ready    &lt;none&gt;   92m   v1<span class="hljs-meta">.13</span><span class="hljs-meta">.4</span>   <span class="hljs-number">10.166</span><span class="hljs-meta">.0</span><span class="hljs-meta">.5</span>    &lt;none&gt;        CentOS Linux <span class="hljs-number">7</span> (Core)   <span class="hljs-number">3.10</span><span class="hljs-meta">.0</span>-<span class="hljs-number">957.5</span><span class="hljs-meta">.1</span>.el7.x86_64   docker://<span class="hljs-number">18.6</span><span class="hljs-meta">.1</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>docker centos7 k8s calico</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Calico</title>
    <link href="/2019/05/20/Calico/"/>
    <url>/2019/05/20/Calico/</url>
    
    <content type="html"><![CDATA[<h1 id="Calico"><a href="#Calico" class="headerlink" title="Calico"></a>Calico</h1><p><a href="https://www.projectcalico.org/" target="_blank" rel="noopener">Calico</a>是一个纯三层的数据中心网络方案（不需要 Overlay），并且与 OpenStack、Kubernetes、AWS、GCE 等 IaaS 和容器平台都有良好的集成。</p><p>Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的 vRouter 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息像整个 Calico 网络内传播——小规模部署可以直接互联，大规模下可通过指定的 BGP route reflector 来完成。 这样保证最终所有的 workload 之间的数据流量都是通过 IP 路由的方式完成互联的。Calico 节点组网可以直接利用数据中心的网络结构（无论是 L2 或者 L3），不需要额外的 NAT，隧道或者 Overlay Network。</p><p>此外，Calico 基于 iptables 还提供了丰富而灵活的网络 Policy，保证通过各个节点上的 ACLs 来提供 Workload 的多租户隔离、安全组以及其他可达性限制等功能。</p><hr><h2 id="Calico-架构"><a href="#Calico-架构" class="headerlink" title="Calico 架构"></a>Calico 架构</h2><p><img src="https://raw.githubusercontent.com/Willie-lin/Hexo-Blog-Picture/master/calico/Calico1.png" srcset="/img/loading.gif" lazyload alt="Calico架构"></p><h4 id="Calico-主要由-Felix、etcd、BGP-client-以及-BGP-Route-Reflector-组成"><a href="#Calico-主要由-Felix、etcd、BGP-client-以及-BGP-Route-Reflector-组成" class="headerlink" title="Calico 主要由 Felix、etcd、BGP client 以及 BGP Route Reflector 组成"></a>Calico 主要由 Felix、etcd、BGP client 以及 BGP Route Reflector 组成</h4><ol><li>Felix，Calico Agent，跑在每台需要运行 Workload 的节点上，主要负责配置路由及 ACLs 等信息来确保 Endpoint 的连通状态；</li><li>etcd，分布式键值存储，主要负责网络元数据一致性，确保 Calico 网络状态的准确性；</li><li>BGP Client（BIRD）, 主要负责把 Felix 写入 Kernel 的路由信息分发到当前 Calico 网络，确保 Workload 间的通信的有效性；</li><li>BGP Route Reflector（BIRD），大规模部署时使用，摒弃所有节点互联的 mesh 模式，通过一个或者多个 BGP Route Reflector 来完成集中式的路由分发。</li><li><p>calico/calico-ipam，主要用作 Kubernetes 的 CNI 插件</p><p> <img src="https://raw.githubusercontent.com/Willie-lin/Hexo-Blog-Picture/master/calico/calico2.png" srcset="/img/loading.gif" lazyload alt="Calico"></p></li></ol><h2 id="IP-in-IP"><a href="#IP-in-IP" class="headerlink" title="IP-in-IP"></a>IP-in-IP</h2><p>Calico 控制平面的设计要求物理网络得是 L2 Fabric，这样 vRouter 间都是直接可达的，路由不需要把物理设备当做下一跳。为了支持 L3 Fabric，Calico 推出了 IPinIP 的选项。</p><h2 id="Calico-CNI"><a href="#Calico-CNI" class="headerlink" title="Calico CNI"></a>Calico CNI</h2><p> 参见 <a href="https://github.com/projectcalico/cni-plugin" target="_blank" rel="noopener">https://github.com/projectcalico/cni-plugin</a>。</p><h2 id="Calico-CNM"><a href="#Calico-CNM" class="headerlink" title="Calico CNM"></a>Calico CNM</h2><p>Calico 通过 Pool 和 Profile 的方式实现了 docker CNM 网络:</p><ol><li>Pool，定义可用于 Docker Network 的 IP 资源范围，比如：10.0.0.0/8 或者 192.168.0.0/16;</li><li>Profile，定义 Docker Network Policy 的集合，由 tags 和 rules 组成；每个 Profile 默认拥有一个和 Profile 名字相同的 Tag，每个 Profile 可以有多个 Tag，以 List 形式保存。</li></ol><p>具体实现见 <a href="https://github.com/projectcalico/libnetwork-plugin" target="_blank" rel="noopener">https://github.com/projectcalico/libnetwork-plugin</a>，而使用方法可以参考 <a href="https://docs.projectcalico.org/master/getting-started/docker/" target="_blank" rel="noopener">https://docs.projectcalico.org/master/getting-started/docker/</a>。</p><h2 id="Calico-Kubernetes"><a href="#Calico-Kubernetes" class="headerlink" title="Calico Kubernetes"></a>Calico Kubernetes</h2><p>对于使用 kubeadm 创建的 Kubernetes 集群，使用以下配置安装 calico 时需要配置<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">--pod-network-cidr=<span class="hljs-number">192.168</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">16</span><br>--service-cidr=<span class="hljs-number">10.96</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">12</span> （不能与 Calico 网络重叠）<br></code></pre></td></tr></table></figure></p><p>然后运行<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl apply -f https:<span class="hljs-regexp">//</span>docs.projectcalico.org<span class="hljs-regexp">/v3.1/g</span>etting-started<span class="hljs-regexp">/kubernetes/i</span>nstallation<span class="hljs-regexp">/hosted/</span>rbac-kdd.yaml<br>kubectl apply -f https:<span class="hljs-regexp">//</span>docs.projectcalico.org<span class="hljs-regexp">/v3.1/g</span>etting-started<span class="hljs-regexp">/kubernetes/i</span>nstallation<span class="hljs-regexp">/hosted/</span>kubernetes-datastore<span class="hljs-regexp">/calico-networking/</span><span class="hljs-number">1.7</span><span class="hljs-regexp">/calico.yaml</span><br></code></pre></td></tr></table></figure></p><p>更详细的自定义配置方法见 <a href="https://docs.projectcalico.org/v3.0/getting-started/kubernetes" target="_blank" rel="noopener">https://docs.projectcalico.org/v3.0/getting-started/kubernetes</a>。</p><p>这会在 Pod 中启动 Calico-etcd，在所有 Node 上启动 bird6、felix 以及 confd，并配置 CNI 网络为 calico 插件：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment"># Calico 相关进程</span><br>$ ps -ef | grep calico | grep -v grep<br>root      9012  8995  0 14:51 ?        00:00:00 /bin/sh -c /usr/local/bin/etcd <span class="hljs-attribute">--name</span>=calico <span class="hljs-attribute">--data-dir</span>=/var/etcd/calico-data <span class="hljs-attribute">--advertise-client-urls</span>=http://$CALICO_ETCD_IP:6666 <span class="hljs-attribute">--listen-client-urls</span>=http://0.0.0.0:6666 <span class="hljs-attribute">--listen-peer-urls</span>=http://0.0.0.0:6667<br>root      9038  9012  0 14:51 ?        00:00:01 /usr/local/bin/etcd <span class="hljs-attribute">--name</span>=calico <span class="hljs-attribute">--data-dir</span>=/var/etcd/calico-data <span class="hljs-attribute">--advertise-client-urls</span>=http://10.146.0.2:6666 <span class="hljs-attribute">--listen-client-urls</span>=http://0.0.0.0:6666 <span class="hljs-attribute">--listen-peer-urls</span>=http://0.0.0.0:6667<br>root      9326  9325  0 14:51 ?        00:00:00 bird6 -R -s /var/run/calico/bird6.ctl -d -c /etc/calico/confd/config/bird6.cfg<br>root      9327  9322  0 14:51 ?        00:00:00 confd <span class="hljs-attribute">-confdir</span>=/etc/calico/confd <span class="hljs-attribute">-interval</span>=5 -watch <span class="hljs-attribute">--log-level</span>=debug <span class="hljs-attribute">-node</span>=http://10.96.232.136:6666 -client-key= -client-cert= -client-ca-keys=<br>root      9328  9324  0 14:51 ?        00:00:00 bird -R -s /var/run/calico/bird.ctl -d -c /etc/calico/confd/config/bird.cfg<br>root      9329  9323  1 14:51 ?        00:00:04 calico-felix<br></code></pre></td></tr></table></figure><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment"># CNI 网络插件配置</span><br>$ cat <span class="hljs-regexp">/etc/</span>cni<span class="hljs-regexp">/net.d/</span><span class="hljs-number">10</span>-calico.conf<br>&#123;<br><span class="hljs-string">"name"</span>: <span class="hljs-string">"k8s-pod-network"</span>,<br>   <span class="hljs-string">"cniVersion"</span>: <span class="hljs-string">"0.1.0"</span>,<br>   <span class="hljs-string">"type"</span>: <span class="hljs-string">"calico"</span>,<br>   <span class="hljs-string">"etcd_endpoints"</span>: <span class="hljs-string">"http://10.96.232.136:6666"</span>,<br>   <span class="hljs-string">"log_level"</span>: <span class="hljs-string">"info"</span>,<br>   <span class="hljs-string">"ipam"</span>: &#123;<br>       <span class="hljs-string">"type"</span>: <span class="hljs-string">"calico-ipam"</span><br>   &#125;,<br><span class="hljs-string">"policy"</span>: &#123;<br>       <span class="hljs-string">"type"</span>: <span class="hljs-string">"k8s"</span>,<br>        <span class="hljs-string">"k8s_api_root"</span>: <span class="hljs-string">"https://10.96.0.1:443"</span>,<br>        <span class="hljs-string">"k8s_auth_token"</span>: <span class="hljs-string">"&lt;token&gt;"</span><br>   &#125;,<br>   <span class="hljs-string">"kubernetes"</span>: &#123;<br>       <span class="hljs-string">"kubeconfig"</span>: <span class="hljs-string">"/etc/cni/net.d/calico-kubeconfig"</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-string">$</span> <span class="hljs-string">cat</span> <span class="hljs-string">/etc/cni/net.d/calico-kubeconfig</span><br><span class="hljs-comment"># Kubeconfig file for Calico CNI plugin.</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Config</span><br><span class="hljs-attr">clusters:</span><br><span class="hljs-attr">- name:</span> <span class="hljs-string">local</span><br>       <span class="hljs-attr">cluster:</span><br>    <span class="hljs-attr">insecure-skip-tls-verify:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">users:</span><br><span class="hljs-attr">   - name:</span> <span class="hljs-string">calico</span><br><span class="hljs-attr">contexts:</span><br><span class="hljs-attr">- name:</span> <span class="hljs-string">calico-context</span><br>     <span class="hljs-attr">context:</span><br>     <span class="hljs-attr">cluster:</span> <span class="hljs-string">local</span><br>     <span class="hljs-attr">user:</span> <span class="hljs-string">calico</span><br><span class="hljs-attr"> current-context:</span> <span class="hljs-string">calico-context</span><br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/Willie-lin/Hexo-Blog-Picture/master/calico/calico-flow3.png" srcset="/img/loading.gif" lazyload alt="calico-flow"></p><h2 id="Calico-的不足"><a href="#Calico-的不足" class="headerlink" title="Calico 的不足"></a>Calico 的不足</h2><ol><li>既然是三层实现，当然不支持 VRF</li><li>不支持多租户网络的隔离功能，在多租户场景下会有网络安全问题</li><li>Calico 控制平面的设计要求物理网络得是 L2 Fabric，这样 vRouter 间都是直接可达的<h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3></li><li><a href="https://xuxinkun.github.io/2016/07/22/cni-cnm/" target="_blank" rel="noopener">https://xuxinkun.github.io/2016/07/22/cni-cnm/</a></li><li><a href="https://www.projectcalico.org/" target="_blank" rel="noopener">https://www.projectcalico.org/</a></li><li><a href="http://blog.dataman-inc.com/shurenyun-docker-133/" target="_blank" rel="noopener">http://blog.dataman-inc.com/shurenyun-docker-133/</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>k8s calico dns</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用kubeadm部署kubernetes集群</title>
    <link href="/2019/05/20/%E4%BD%BF%E7%94%A8kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/"/>
    <url>/2019/05/20/%E4%BD%BF%E7%94%A8kubeadm%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h1 id="1-准备"><a href="#1-准备" class="headerlink" title="1.准备"></a>1.准备</h1><h3 id="1-1-系统配置"><a href="#1-1-系统配置" class="headerlink" title="1.1 系统配置"></a>1.1 系统配置</h3><h4 id="在安装之前，需要先做如下准备。两台CentOS-7-6主机如下："><a href="#在安装之前，需要先做如下准备。两台CentOS-7-6主机如下：" class="headerlink" title="在安装之前，需要先做如下准备。两台CentOS 7.6主机如下："></a>在安装之前，需要先做如下准备。两台CentOS 7.6主机如下：</h4><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">cat /etc/hosts<br><span class="hljs-number">192.168</span><span class="hljs-number">.32</span><span class="hljs-number">.87</span> node1<br><span class="hljs-number">192.168</span><span class="hljs-number">.32</span><span class="hljs-number">.88</span> node2<br></code></pre></td></tr></table></figure><p>如果各个主机启用了防火墙，需要开放Kubernetes各个组件所需要的端口，可以查看Installing kubeadm中的”Check required ports”一节。 这里简单起见在各节点禁用防火墙：</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">systemctl <span class="hljs-keyword">stop</span> firewalld<br>systemctl <span class="hljs-keyword">disable</span> firewalld<br></code></pre></td></tr></table></figure><p>禁用SELINUX：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">setenforce 0<br>vi /etc/selinux<span class="hljs-built_in">/config<br></span><span class="hljs-attribute">SELINUX</span>=disabled<br></code></pre></td></tr></table></figure></p><p>创建/etc/sysctl.d/k8s.conf文件，添加如下内容：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">net<span class="hljs-selector-class">.bridge</span><span class="hljs-selector-class">.bridge-nf-call-ip6tables</span> = <span class="hljs-number">1</span><br>net<span class="hljs-selector-class">.bridge</span><span class="hljs-selector-class">.bridge-nf-call-iptables</span> = <span class="hljs-number">1</span><br>net<span class="hljs-selector-class">.ipv4</span><span class="hljs-selector-class">.ip_forward</span> = <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure></p><p>执行命令使修改生效。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">modprobe br_netfilter<br>sysctl -<span class="hljs-selector-tag">p</span> /etc/sysctl.d/k8s.conf<br></code></pre></td></tr></table></figure></p><h3 id="1-2-kube-proxy开启ipvs的前置条件"><a href="#1-2-kube-proxy开启ipvs的前置条件" class="headerlink" title="1.2 kube-proxy开启ipvs的前置条件"></a>1.2 kube-proxy开启ipvs的前置条件</h3><p>由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块：<br><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined">ip_<span class="hljs-attr">vs<br>ip_vs_rr<br>ip_vs_wrr<br>ip_vs_sh<br>nf_conntrack_ipv4</span><br></code></pre></td></tr></table></figure></p><p>在所有的Kubernetes节点node1和node2上执行以下脚本:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF<br><span class="hljs-meta">#!/bin/bash</span><br>modprobe -- ip_vs<br>modprobe -- ip_vs_rr<br>modprobe -- ip_vs_wrr<br>modprobe -- ip_vs_sh<br>modprobe -- nf_conntrack_ipv4 <br> <br>EOF<br>chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4<br></code></pre></td></tr></table></figure><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"># keruse <span class="hljs-symbol">'nf_conntrack</span>' instead <span class="hljs-keyword">of</span> <span class="hljs-symbol">'nf_conntrack_ipv4</span>' <span class="hljs-keyword">for</span> linux kernel &gt;= <span class="hljs-number">4.19</span><br></code></pre></td></tr></table></figure><p>上面脚本创建了的/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。</p><p>接下来还需要确保各个节点上已经安装了ipset软件包yum install ipset。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm yum install ipvsadm。</p><p>如果以上前提条件如果不满足，则即使kube-proxy的配置开启了ipvs模式，也会退回到iptables模式。</p><h3 id="1-3-安装Docker"><a href="#1-3-安装Docker" class="headerlink" title="1.3 安装Docker"></a>1.3 安装Docker</h3><p>Kubernetes从1.6开始使用CRI(Container Runtime Interface)容器运行时接口。默认的容器运行时仍然是Docker，使用的是kubelet中内置dockershim CRI实现。</p><p>内网状态下直接执行：<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum <span class="hljs-keyword">install</span> docker<br></code></pre></td></tr></table></figure></p><p>安装docker的yum源:<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum install -<span class="hljs-keyword">y</span> yum-utils device-mapper-persistent-data lvm2<br>yum-config-manager \<br>    --<span class="hljs-built_in">add</span>-repo \<br>    http<span class="hljs-variable">s:</span>//download.docker.<span class="hljs-keyword">com</span>/linux/centos/docker-<span class="hljs-keyword">ce</span>.repo<br>查看最新的Docker版本：<br><br>yum <span class="hljs-keyword">list</span> docker-<span class="hljs-keyword">ce</span>.x86_64  --showduplicates |<span class="hljs-keyword">sort</span> -r<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">3</span>:<span class="hljs-number">18.09</span>.<span class="hljs-number">0</span>-<span class="hljs-number">3</span>.el7                     docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">18.06</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">3</span>.el7                    docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">18.06</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">3</span>.el7                    docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">18.03</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">18.03</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.12</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.12</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.09</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.09</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.06</span>.<span class="hljs-number">2</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.06</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.06</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.03</span>.<span class="hljs-number">3</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7                    docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.03</span>.<span class="hljs-number">2</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.03</span>.<span class="hljs-number">1</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br>docker-<span class="hljs-keyword">ce</span>.x86_64            <span class="hljs-number">17.03</span>.<span class="hljs-number">0</span>.<span class="hljs-keyword">ce</span>-<span class="hljs-number">1</span>.el7.centos             docker-<span class="hljs-keyword">ce</span>-stable<br></code></pre></td></tr></table></figure></p><p>Kubernetes 1.12已经针对Docker的1.11.1, 1.12.1, 1.13.1, 17.03, 17.06, 17.09, 18.06等版本做了验证，需要注意Kubernetes 1.12最低支持的Docker版本是1.11.1。Kubernetes 1.13对Docker的版本依赖方面没有变化。 我们这里在各节点安装docker的18.06.1版本。</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs undefined">yum makecache fast<br><br>yum install -y --setopt=obsoletes=<span class="hljs-number">0</span> \<br>  docker-ce<span class="hljs-number">-18.06</span><span class="hljs-number">.1</span>.ce<span class="hljs-number">-3.</span>el7<br><br>systemctl start docker<br>systemctl enable docker<br></code></pre></td></tr></table></figure><p>确认一下iptables filter表中FOWARD链的默认策略(pllicy)为ACCEPT。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs undefined">iptables -nvL<br>Chain INPUT (policy ACCEPT <span class="hljs-number">263</span> packets, <span class="hljs-number">19209</span> bytes)<br> pkts bytes target     prot opt in     out     source               destination<br><br>Chain FORWARD (policy ACCEPT <span class="hljs-number">0</span> packets, <span class="hljs-number">0</span> bytes)<br> pkts bytes target     prot opt in     out     source               destination<br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> DOCKER-USER  all  --  *      *       <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span><br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> DOCKER-ISOLATION-STAGE<span class="hljs-number">-1</span>  all  --  *      *       <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span><br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> ACCEPT     all  --  *      docker0  <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            ctstate RELATED,ESTABLISHED<br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> DOCKER     all  --  *      docker0  <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span><br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> ACCEPT     all  --  docker0 !docker0  <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span><br>    <span class="hljs-number">0</span>     <span class="hljs-number">0</span> ACCEPT     all  --  docker0 docker0  <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span>            <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure></p><blockquote><p>Docker从1.13版本开始调整了默认的防火墙规则，禁用了iptables filter表中FOWARD链，这样会引起Kubernetes集群中跨Node的Pod无法通信。但这里通过安装docker 1806，发现默认策略又改回了ACCEPT，这个不知道是从哪个版本改回的，因为我们线上版本使用的1706还是需要手动调整这个策略的。</p></blockquote><h1 id="2-使用kubeadm部署Kubernetes"><a href="#2-使用kubeadm部署Kubernetes" class="headerlink" title="2. 使用kubeadm部署Kubernetes"></a>2. 使用kubeadm部署Kubernetes</h1><h3 id="2-1-安装kubeadm和kubelet"><a href="#2-1-安装kubeadm和kubelet" class="headerlink" title="2.1 安装kubeadm和kubelet"></a>2.1 安装kubeadm和kubelet</h3><p> 下面在各节点安装kubeadm和kubelet：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo<br>[kubernetes]<br>name=Kubernetes<br>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64<br>enabled=1<br>gpgcheck=1<br>repo_gpgcheck=1<br>gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg<br>        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg<br>EOF<br></code></pre></td></tr></table></figure></p><p>测试地址<a href="https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64是否可用，如果不可用需要科学上网。" target="_blank" rel="noopener">https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64是否可用，如果不可用需要科学上网。</a><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs undefined">curl https:<span class="hljs-comment">//packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</span><br>yum makecache fast<br>yum install -y kubelet kubeadm kubectl<br><br><br>Installed:<br>  kubeadm<span class="hljs-selector-class">.x86_64</span> <span class="hljs-number">0</span>:<span class="hljs-number">1.13</span>.<span class="hljs-number">0</span>-<span class="hljs-number">0</span>                                    kubectl<span class="hljs-selector-class">.x86_64</span> <span class="hljs-number">0</span>:<span class="hljs-number">1.13</span>.<span class="hljs-number">0</span>-<span class="hljs-number">0</span>                                                           kubelet<span class="hljs-selector-class">.x86_64</span> <span class="hljs-number">0</span>:<span class="hljs-number">1.13</span>.<span class="hljs-number">0</span>-<span class="hljs-number">0</span><br><br>Dependency Installed:<br>  cri-tools<span class="hljs-selector-class">.x86_64</span> <span class="hljs-number">0</span>:<span class="hljs-number">1.12</span>.<span class="hljs-number">0</span>-<span class="hljs-number">0</span>                                  kubernetes-cni<span class="hljs-selector-class">.x86_64</span> <span class="hljs-number">0</span>:<span class="hljs-number">0.6</span>.<span class="hljs-number">0</span>-<span class="hljs-number">0</span>                                                       socat<span class="hljs-selector-class">.x86_64</span> <span class="hljs-number">0</span>:<span class="hljs-number">1.7</span>.<span class="hljs-number">3.2</span>-<span class="hljs-number">2</span>.el7<br></code></pre></td></tr></table></figure></p><p>从安装结果可以看出还安装了cri-tools, kubernetes-cni, socat三个依赖：<br>官方从Kubernetes 1.9开始就将cni依赖升级到了0.6.0版本，在当前1.12中仍然是这个版本<br>socat是kubelet的依赖<br>cri-tools是CRI(Container Runtime Interface)容器运行时接口的命令行工具<br>运行kubelet –help可以看到原来kubelet的绝大多数命令行flag参数都被DEPRECATED了，如：<br><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">--address <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>   The IP address <span class="hljs-keyword">for</span> the Kubelet <span class="hljs-keyword">to</span> serve <span class="hljs-keyword">on</span> (<span class="hljs-keyword">set</span> <span class="hljs-keyword">to</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span> <span class="hljs-keyword">for</span> all IPv4 interfaces <span class="hljs-keyword">and</span> `::` <span class="hljs-keyword">for</span> all IPv6 interfaces) (<span class="hljs-keyword">default</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>) (DEPRECATED: This parameter should be <span class="hljs-keyword">set</span> via the config file specified <span class="hljs-keyword">by</span> the Kubelet<span class="hljs-comment">'s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.)</span><br></code></pre></td></tr></table></figure></p><p>而官方推荐我们使用–config指定配置文件，并在配置文件中指定原来这些flag所配置的内容。具体内容可以查看这里Set Kubelet parameters via a config file。这也是Kubernetes为了支持动态Kubelet配置（Dynamic Kubelet Configuration）才这么做的，参考Reconfigure a Node’s Kubelet in a Live Cluster。</p><p>kubelet的配置文件必须是json或yaml格式，具体可查看这里。</p><p>Kubernetes 1.8开始要求关闭系统的Swap，如果不关闭，默认配置下kubelet将无法启动。</p><p>关闭系统的Swap方法如下:<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attribute">swapoff -a</span><br></code></pre></td></tr></table></figure></p><p>修改 /etc/fstab 文件，注释掉 SWAP 的自动挂载，使用free -m确认swap已经关闭。 swappiness参数调整，修改/etc/sysctl.d/k8s.conf添加下面一行：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attr">vm.swappiness</span>=<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure></p><p>执行<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs undefined">sysctl -<span class="hljs-selector-tag">p</span> /etc/sysctl.d/k8s<span class="hljs-selector-class">.conf</span><br><br>使修改生效。<br></code></pre></td></tr></table></figure></p><hr><p>因为这里本次用于测试两台主机上还运行其他服务，关闭swap可能会对其他服务产生影响，所以这里修改kubelet的配置去掉这个限制。 之前的Kubernetes版本我们都是通过kubelet的启动参数–fail-swap-on=false去掉这个限制的。前面已经分析了Kubernetes不再推荐使用启动参数，而推荐使用配置文件。 所以这里我们改成配置文件配置的形式。</p><p>查看/etc/systemd/system/kubelet.service.d/10-kubeadm.conf，看到了下面的内容：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-comment"># <span class="hljs-doctag">Note:</span> This dropin only works with kubeadm and kubelet v1.11+</span><br><span class="hljs-section">[Service]</span><br><span class="hljs-attr">Environment</span>=<span class="hljs-string">"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"</span><br><span class="hljs-attr">Environment</span>=<span class="hljs-string">"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"</span><br><span class="hljs-comment"># This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically</span><br><span class="hljs-attr">EnvironmentFile</span>=-/var/lib/kubelet/kubeadm-flags.env<br><span class="hljs-comment"># This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use</span><br><span class="hljs-comment"># the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.</span><br><span class="hljs-attr">EnvironmentFile</span>=-/etc/sysconfig/kubelet<br><span class="hljs-attr">ExecStart</span>=<br><span class="hljs-attr">ExecStart</span>=/usr/bin/kubelet <span class="hljs-variable">$KUBELET_KUBECONFIG_ARGS</span> <span class="hljs-variable">$KUBELET_CONFIG_ARGS</span> <span class="hljs-variable">$KUBELET_KUBEADM_ARGS</span> <span class="hljs-variable">$KUBELET_EXTRA_ARGS</span><br></code></pre></td></tr></table></figure><p>上面显示kubeadm部署的kubelet的配置文件–config=/var/lib/kubelet/config.yaml，实际去查看/var/lib/kubelet和这个config.yaml的配置文件都没有被创建。 可以猜想肯定是运行kubeadm初始化集群时会自动生成这个配置文件，而如果我们不关闭Swap的话，第一次初始化集群肯定会失败的。</p><p>所以还是老老实实的回到使用kubelet的启动参数–fail-swap-on=false去掉必须关闭Swap的限制。 修改/etc/sysconfig/kubelet，加入：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-attr">KUBELET_EXTRA_ARGS</span>=--fail-swap-<span class="hljs-literal">on</span>=<span class="hljs-literal">false</span><br></code></pre></td></tr></table></figure></p><h3 id="2-2-使用kubeadm-init初始化集群"><a href="#2-2-使用kubeadm-init初始化集群" class="headerlink" title="2.2 使用kubeadm init初始化集群"></a>2.2 使用kubeadm init初始化集群</h3><p>在各节点开机启动kubelet服务：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs undefined">systemctl <span class="hljs-builtin-name">enable</span> kubelet.service<br></code></pre></td></tr></table></figure></p><p>接下来使用kubeadm初始化集群，选择node1作为Master Node，在node1上执行下面的命令：<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubeadm init \<br>  --kubernetes-version=v1<span class="hljs-number">.13</span><span class="hljs-number">.0</span> \<br>  --pod-network-cidr=<span class="hljs-number">10.244</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span>/<span class="hljs-number">16</span> \<br>  --apiserver-advertise-address=<span class="hljs-number">192.168</span><span class="hljs-number">.61</span><span class="hljs-number">.11</span><br></code></pre></td></tr></table></figure></p><p>因为我们选择flannel作为Pod网络插件，所以上面的命令指定–pod-network-cidr=10.244.0.0/16。</p><p>执行时报了下面的错误：<br><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[init] using Kubernetes version: v1.13.0<br>[preflight] running pre-flight checks<br>[preflight] Some fatal errors occurred:<br>        [ERROR Swap]: running with swap on is<span class="hljs-built_in"> not </span>supported. Please disable swap<br>[preflight] If you know what you are doing, you can make a<span class="hljs-built_in"> check </span>non-fatal with `--ignore-preflight-errors=...`<br></code></pre></td></tr></table></figure></p><p>有一个错误信息是running with swap on is not supported. Please disable swap。因为我们决定配置failSwapOn: false，所以重新添加–ignore-preflight-errors=Swap参数忽略这个错误，重新运行。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@k8s-master yyhmmwan]<span class="hljs-comment"># kubeadm init \ --kubernetes-version=v1.13.4 \ --pod-network-cidr=10.244.0.0/16 \ --apiserver-a</span><br>dvertise-address=10.166.0.5 <br>[init] Using Kubernetes version: v1.13.4<br>[preflight] Running pre-flight checks<br>[preflight] Pulling images required for setting up a Kubernetes cluster<br>[preflight] This might take a minute or two, depending on the speed of your internet connection<br>[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'<br>[kubelet-<span class="hljs-keyword">start</span>] Writing kubelet environment <span class="hljs-keyword">file</span> <span class="hljs-keyword">with</span> flags <span class="hljs-keyword">to</span> <span class="hljs-keyword">file</span> <span class="hljs-string">"/var/lib/kubelet/kubeadm-flags.env"</span><br>[kubelet-<span class="hljs-keyword">start</span>] Writing kubelet configuration <span class="hljs-keyword">to</span> <span class="hljs-keyword">file</span> <span class="hljs-string">"/var/lib/kubelet/config.yaml"</span><br>[kubelet-<span class="hljs-keyword">start</span>] Activating the kubelet service<br>[certs] <span class="hljs-keyword">Using</span> certificateDir folder <span class="hljs-string">"/etc/kubernetes/pki"</span><br>[certs] Generating <span class="hljs-string">"etcd/ca"</span> certificate <span class="hljs-keyword">and</span> <span class="hljs-keyword">key</span><br>[certs] Generating <span class="hljs-string">"etcd/server"</span> certificate <span class="hljs-keyword">and</span> <span class="hljs-keyword">key</span><br>[certs] etcd/<span class="hljs-keyword">server</span> serving cert <span class="hljs-keyword">is</span> signed <span class="hljs-keyword">for</span> DNS <span class="hljs-keyword">names</span> [k8s-<span class="hljs-keyword">master</span> localhost] <span class="hljs-keyword">and</span> IPs [<span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.4</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> ::<span class="hljs-number">1</span>]<br>[certs] Generating <span class="hljs-string">"etcd/peer"</span> certificate <span class="hljs-keyword">and</span> <span class="hljs-keyword">key</span><br>[certs] etcd/peer serving cert <span class="hljs-keyword">is</span> signed <span class="hljs-keyword">for</span> DNS <span class="hljs-keyword">names</span> [k8s-<span class="hljs-keyword">master</span> localhost] <span class="hljs-keyword">and</span> IPs [<span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.4</span> <span class="hljs-number">127.0</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> ::<span class="hljs-number">1</span>]<br>[certs] Generating <span class="hljs-string">"apiserver-etcd-client"</span> certificate <span class="hljs-keyword">and</span> <span class="hljs-keyword">key</span><br>[certs] Generating <span class="hljs-string">"etcd/healthcheck-client"</span> certificate <span class="hljs-keyword">and</span> <span class="hljs-keyword">key</span><br>[certs] Generating <span class="hljs-string">"ca"</span> certificate <span class="hljs-keyword">and</span> <span class="hljs-keyword">key</span><br>[certs] Generating <span class="hljs-string">"apiserver-kubelet-client"</span> certificate <span class="hljs-keyword">and</span> <span class="hljs-keyword">key</span><br>[certs] Generating <span class="hljs-string">"apiserver"</span> certificate <span class="hljs-keyword">and</span> <span class="hljs-keyword">key</span><br>[certs] apiserver serving cert <span class="hljs-keyword">is</span> signed <span class="hljs-keyword">for</span> DNS <span class="hljs-keyword">names</span> [k8s-<span class="hljs-keyword">master</span> kubernetes kubernetes.default kubernetes.default.svc ku<br>bernetes.default.svc.cluster.local] <span class="hljs-keyword">and</span> IPs [<span class="hljs-number">10.96</span><span class="hljs-number">.0</span><span class="hljs-number">.1</span> <span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.4</span>]<br>[certs] Generating <span class="hljs-string">"front-proxy-ca"</span> certificate <span class="hljs-keyword">and</span> <span class="hljs-keyword">key</span><br>[certs] Generating <span class="hljs-string">"front-proxy-client"</span> certificate <span class="hljs-keyword">and</span> <span class="hljs-keyword">key</span><br>[certs] Generating <span class="hljs-string">"sa"</span> <span class="hljs-keyword">key</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">public</span> <span class="hljs-keyword">key</span><br>[kubeconfig] <span class="hljs-keyword">Using</span> kubeconfig folder <span class="hljs-string">"/etc/kubernetes"</span><br>[kubeconfig] Writing <span class="hljs-string">"admin.conf"</span> kubeconfig <span class="hljs-keyword">file</span><br>[kubeconfig] Writing <span class="hljs-string">"kubelet.conf"</span> kubeconfig <span class="hljs-keyword">file</span><br>[kubeconfig] Writing <span class="hljs-string">"controller-manager.conf"</span> kubeconfig <span class="hljs-keyword">file</span><br>[kubeconfig] Writing <span class="hljs-string">"scheduler.conf"</span> kubeconfig <span class="hljs-keyword">file</span><br>[control-plane] <span class="hljs-keyword">Using</span> manifest folder <span class="hljs-string">"/etc/kubernetes/manifests"</span><br>[control-plane] Creating <span class="hljs-keyword">static</span> Pod manifest <span class="hljs-keyword">for</span> <span class="hljs-string">"kube-apiserver"</span><br>[control-plane] Creating <span class="hljs-keyword">static</span> Pod manifest <span class="hljs-keyword">for</span> <span class="hljs-string">"kube-controller-manager"</span><br>[control-plane] Creating <span class="hljs-keyword">static</span> Pod manifest <span class="hljs-keyword">for</span> <span class="hljs-string">"kube-scheduler"</span><br>[etcd] Creating <span class="hljs-keyword">static</span> Pod manifest <span class="hljs-keyword">for</span> <span class="hljs-keyword">local</span> etcd <span class="hljs-keyword">in</span> <span class="hljs-string">"/etc/kubernetes/manifests"</span><br>[<span class="hljs-keyword">wait</span>-control-plane] Waiting <span class="hljs-keyword">for</span> the kubelet <span class="hljs-keyword">to</span> boot up the control plane <span class="hljs-keyword">as</span> <span class="hljs-keyword">static</span> Pods <span class="hljs-keyword">from</span> <span class="hljs-keyword">directory</span> <span class="hljs-string">"/etc/kubernetes/m<br>anifests"</span>. This can take up <span class="hljs-keyword">to</span> <span class="hljs-number">4</span>m0s<br>[apiclient] <span class="hljs-keyword">All</span> control plane components <span class="hljs-keyword">are</span> healthy <span class="hljs-keyword">after</span> <span class="hljs-number">22.502906</span> <span class="hljs-keyword">seconds</span><br>[uploadconfig] storing the configuration used <span class="hljs-keyword">in</span> ConfigMap <span class="hljs-string">"kubeadm-config"</span> <span class="hljs-keyword">in</span> the <span class="hljs-string">"kube-system"</span> Namespace<br>[kubelet] Creating a ConfigMap <span class="hljs-string">"kubelet-config-1.13"</span> <span class="hljs-keyword">in</span> namespace kube-<span class="hljs-keyword">system</span> <span class="hljs-keyword">with</span> the configuration <span class="hljs-keyword">for</span> the kubelets <span class="hljs-keyword">in</span> t<br>he cluster<br>[patchnode] Uploading the CRI Socket information <span class="hljs-string">"/var/run/dockershim.sock"</span> <span class="hljs-keyword">to</span> the Node API <span class="hljs-keyword">object</span> <span class="hljs-string">"k8s-master"</span> <span class="hljs-keyword">as</span> an anno<br>tation<br>[mark-control-plane] Marking the node k8s-<span class="hljs-keyword">master</span> <span class="hljs-keyword">as</span> control-plane <span class="hljs-keyword">by</span> adding the label <span class="hljs-string">"node-role.kubernetes.io/master=''"</span><br>[mark-control-plane] Marking the node k8s-<span class="hljs-keyword">master</span> <span class="hljs-keyword">as</span> control-plane <span class="hljs-keyword">by</span> adding the taints [node-role.kubernetes.io/<span class="hljs-keyword">master</span>:NoS<br>chedule]<br>[bootstrap-token] <span class="hljs-keyword">Using</span> token: <span class="hljs-number">29</span>h44u.gqmsznz07dl313l5<br>[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC <span class="hljs-keyword">Roles</span><br>[bootstraptoken] configured RBAC <span class="hljs-keyword">rules</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">allow</span> Node Bootstrap tokens <span class="hljs-keyword">to</span> post CSRs <span class="hljs-keyword">in</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">for</span> nodes <span class="hljs-keyword">to</span> <span class="hljs-keyword">get</span> <span class="hljs-keyword">long</span> term cer<br>tificate credentials<br>[bootstraptoken] configured RBAC <span class="hljs-keyword">rules</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">allow</span> the csrapprover controller automatically approve CSRs <span class="hljs-keyword">from</span> a Node Bootstra<br>p Token<br>[bootstraptoken] configured RBAC <span class="hljs-keyword">rules</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">allow</span> certificate rotation <span class="hljs-keyword">for</span> <span class="hljs-keyword">all</span> node <span class="hljs-keyword">client</span> certificates <span class="hljs-keyword">in</span> the cluster<br>[bootstraptoken] creating the <span class="hljs-string">"cluster-info"</span> ConfigMap <span class="hljs-keyword">in</span> the <span class="hljs-string">"kube-public"</span> namespace<br>[addons] Applied essential addon: CoreDNS<br>[addons] Applied essential addon: kube-proxy<br><br>Your Kubernetes <span class="hljs-keyword">master</span> has <span class="hljs-keyword">initialized</span> successfully!<br><span class="hljs-keyword">To</span> <span class="hljs-keyword">start</span> <span class="hljs-keyword">using</span> your cluster, you need <span class="hljs-keyword">to</span> run the <span class="hljs-keyword">following</span> <span class="hljs-keyword">as</span> a regular <span class="hljs-keyword">user</span>:<br>  mkdir -p $HOME/.kube<br>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br>  sudo chown $(<span class="hljs-keyword">id</span> -u):$(<span class="hljs-keyword">id</span> -g) $HOME/.kube/config<br>You should <span class="hljs-keyword">now</span> deploy a pod network <span class="hljs-keyword">to</span> the cluster.<br>Run <span class="hljs-string">"kubectl apply -f [podnetwork].yaml"</span> <span class="hljs-keyword">with</span> one <span class="hljs-keyword">of</span> the options listed <span class="hljs-keyword">at</span>:<br>  https://kubernetes.io/docs/concepts/cluster-administration/addons/<br>You can <span class="hljs-keyword">now</span> <span class="hljs-keyword">join</span> <span class="hljs-keyword">any</span> <span class="hljs-built_in">number</span> <span class="hljs-keyword">of</span> machines <span class="hljs-keyword">by</span> running the <span class="hljs-keyword">following</span> <span class="hljs-keyword">on</span> <span class="hljs-keyword">each</span> node<br><span class="hljs-keyword">as</span> root:<br>  kubeadm <span class="hljs-keyword">join</span> <span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.4</span>:<span class="hljs-number">6443</span> <br><span class="hljs-comment">--token 29h44u.gqmsznz07dl313l5 </span><br><span class="hljs-comment">--discovery-token-ca-cert-hash </span><br>sha256:ec7d11d9cdf220310f3aac7fbacceea41ddb7e86be9cf790c247c0c6802a8a7e<br><span class="hljs-string">``</span><span class="hljs-string">` <br><br>上面记录了完成的初始化输出的内容，根据输出的内容基本上可以看出手动初始化安装一个Kubernetes集群所需要的关键步骤。<br><br>其中有以下关键内容：</span><br></code></pre></td></tr></table></figure><p>[kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml”<br>[certificates]生成相关的各种证书<br>[kubeconfig]生成相关的kubeconfig文件<br>[bootstraptoken]生成token记录下来，后边使用kubeadm join往集群中添加节点时会用<br>到下面的命令是配置常规用户如何使用kubectl访问集群：<br> <figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs undefined">```<br>mkdir -p <span class="hljs-variable">$HOME</span>/.kube<br>  sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube<span class="hljs-built_in">/config<br></span>  sudo chown $(id -u):$(id -g) <span class="hljs-variable">$HOME</span>/.kube/config<br></code></pre></td></tr></table></figure></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined">最后给出了将节点加入集群的命令<br>  <span class="hljs-selector-tag">kubeadm</span> <span class="hljs-selector-tag">join</span> 10<span class="hljs-selector-class">.166</span><span class="hljs-selector-class">.0</span><span class="hljs-selector-class">.4</span><span class="hljs-selector-pseudo">:6443</span> <br><span class="hljs-selector-tag">--token</span> 29<span class="hljs-selector-tag">h44u</span><span class="hljs-selector-class">.gqmsznz07dl313l5</span> <br><span class="hljs-selector-tag">--discovery-token-ca-cert-hash</span> <br><span class="hljs-selector-tag">sha256</span><span class="hljs-selector-pseudo">:ec7d11d9cdf220310f3aac7fbacceea41ddb7e86be9cf790c247c0c6802a8a7e</span><br></code></pre></td></tr></table></figure><p>查看一下集群状态：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@k8s-master yyhmmwan]# kubectl <span class="hljs-builtin-name">get</span> cs<br>NAME                 STATUS    MESSAGE              <span class="hljs-builtin-name">ERROR</span><br>controller-manager   Healthy   ok                   <span class="hljs-built_in"><br>scheduler </span>           Healthy   ok                   <br>etcd-0               Healthy   &#123;<span class="hljs-string">"health"</span>: <span class="hljs-string">"true"</span>&#125;<br></code></pre></td></tr></table></figure></p><p>确认个组件都处于healthy状态。</p><p>集群初始化如果遇到问题，可以使用下面的命令进行清理：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubeadm reset<br>ifconfig cni0 down<span class="hljs-built_in"><br>ip </span>link delete cni0<br>ifconfig flannel.1 down<span class="hljs-built_in"><br>ip </span>link delete flannel.1<br>rm -rf /var/lib/cni/<br></code></pre></td></tr></table></figure></p><h3 id="2-3-安装Pod-Network"><a href="#2-3-安装Pod-Network" class="headerlink" title="2.3 安装Pod Network"></a>2.3 安装Pod Network</h3><p>接下来安装flannel network add-on：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs undefined">mkdir -<span class="hljs-selector-tag">p</span> ~/k8s/<br>cd ~/k8s<br>wget https:<span class="hljs-comment">//raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br>kubectl apply -f  kube-flannel<span class="hljs-selector-class">.yml</span><br><br>clusterrole<span class="hljs-selector-class">.rbac</span><span class="hljs-selector-class">.authorization</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/flannel created<br>clusterrolebinding<span class="hljs-selector-class">.rbac</span><span class="hljs-selector-class">.authorization</span><span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span>/flannel created<br>serviceaccount/flannel created<br>configmap/kube-flannel-cfg created<br>daemonset.extensions/kube-flannel-ds-amd64 created<br>daemonset.extensions/kube-flannel-ds-arm64 created<br>daemonset.extensions/kube-flannel-ds-arm created<br>daemonset.extensions/kube-flannel-ds-ppc64le created<br>daemonset.extensions/kube-flannel-ds-s390x created<br></code></pre></td></tr></table></figure></p><p>如果Node有多个网卡的话，参考flannel issues 39701，目前需要在kube-flannel.yml中使用–iface参数指定集群主机内网网卡的名称，否则可能会出现dns无法解析。需要将kube-flannel.yml下载到本地，flanneld启动参数加上–iface=<iface-name><br><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs undefined"><span class="hljs-code">......<br>containers:<br>      - name: kube-flannel<br>        image: quay.io/coreos/flannel:v0.10.0-amd64<br>        command:<br>        - /opt/bin/flanneld<br>        args:<br>        - --ip-masq<br>        - --kube-subnet-mgr<br>        - --iface=eth1<br>......</span><br></code></pre></td></tr></table></figure></iface-name></p><p>使用kubectl get pod –all-namespaces -o wide确保所有的Pod都处于Running状态。</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs undefined">[root@k8s-master k8s]# kubectl get pod --all-namespaces -o wide<br>NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE    IP           NODE         NOMINATED NODE   READINESS GATES<br>kube-system   coredns<span class="hljs-number">-86</span>c58d9df4-k5gtz             <span class="hljs-number">0</span>/<span class="hljs-number">1</span>     ContainerCreating   <span class="hljs-number">0</span>          <span class="hljs-number">45</span>m    &lt;none&gt;       k8s-node     &lt;none&gt;           &lt;none&gt;<br>kube-system   coredns<span class="hljs-number">-86</span>c58d9df4-sgnk6             <span class="hljs-number">0</span>/<span class="hljs-number">1</span>     ContainerCreating   <span class="hljs-number">0</span>          <span class="hljs-number">45</span>m    &lt;none&gt;       k8s-node     &lt;none&gt;           &lt;none&gt;<br>kube-system   etcd-k8s-master                      <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running             <span class="hljs-number">1</span>          <span class="hljs-number">44</span>m    <span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.4</span>   k8s-master   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-apiserver-k8s-master            <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running             <span class="hljs-number">1</span>          <span class="hljs-number">44</span>m    <span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.4</span>   k8s-master   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-controller-manager-k8s-master   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running             <span class="hljs-number">1</span>          <span class="hljs-number">44</span>m    <span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.4</span>   k8s-master   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-flannel-ds-amd64-gngrk          <span class="hljs-number">0</span>/<span class="hljs-number">1</span>     CrashLoopBackOff    <span class="hljs-number">4</span>          <span class="hljs-number">3</span>m7s   <span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.5</span>   k8s-node     &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-flannel-ds-amd64-hgftp          <span class="hljs-number">0</span>/<span class="hljs-number">1</span>     CrashLoopBackOff    <span class="hljs-number">4</span>          <span class="hljs-number">3</span>m7s   <span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.4</span>   k8s-master   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy<span class="hljs-number">-78</span>sj6                     <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running             <span class="hljs-number">0</span>          <span class="hljs-number">45</span>m    <span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.5</span>   k8s-node     &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-xwbxv                     <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running             <span class="hljs-number">2</span>          <span class="hljs-number">45</span>m    <span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.4</span>   k8s-master   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-scheduler-k8s-master            <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     Running             <span class="hljs-number">2</span>          <span class="hljs-number">44</span>m    <span class="hljs-number">10.166</span><span class="hljs-number">.0</span><span class="hljs-number">.4</span>   k8s-master   &lt;none&gt;           &lt;none&gt;<br></code></pre></td></tr></table></figure><h3 id="2-4-master-node参与工作负载"><a href="#2-4-master-node参与工作负载" class="headerlink" title="2.4 master node参与工作负载"></a>2.4 master node参与工作负载</h3><p>使用kubeadm初始化的集群，出于安全考虑Pod不会被调度到Master Node上，也就是说Master Node不参与工作负载。这是因为当前的master节点node1被打上了node-role.kubernetes.io/master:NoSchedule的污点：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl describe <span class="hljs-keyword">node</span> <span class="hljs-title">node1</span> | grep Taint<br>Taints:             <span class="hljs-keyword">node</span><span class="hljs-title">-role</span>.kubernetes.io/<span class="hljs-literal">master</span>:NoSchedule<br></code></pre></td></tr></table></figure></p><p>因为这里搭建的是测试环境，去掉这个污点使node1参与工作负载：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl taint nodes node1 <span class="hljs-keyword">node</span><span class="hljs-title">-role</span>.kubernetes.io/<span class="hljs-literal">master</span>-<br><span class="hljs-keyword">node</span> <span class="hljs-title">"node1</span><span class="hljs-string">" untainted</span><br></code></pre></td></tr></table></figure></p><h3 id="2-5-测试DNS"><a href="#2-5-测试DNS" class="headerlink" title="2.5 测试DNS"></a>2.5 测试DNS</h3><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs undefined">kubectl <span class="hljs-built_in">run</span> curl <span class="hljs-comment">--image=radial/busyboxplus:curl -it</span><br>kubectl <span class="hljs-built_in">run</span> <span class="hljs-comment">--generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.</span><br>If you don't see a command prompt, <span class="hljs-keyword">try</span> pressing enter.<br>[ root@curl<span class="hljs-number">-5</span>cc7b478b6-r997p:/ ]$<br></code></pre></td></tr></table></figure><p>进入后执行nslookup kubernetes.default确认解析正常:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs undefined">nslookup kubernetes.default<br>Server:    10.96.0.10<span class="hljs-built_in"><br>Address </span>1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br><br>Name:      kubernetes.default<span class="hljs-built_in"><br>Address </span>1: 10.96.0.1 kubernetes.default.svc.cluster.local<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    
    <tags>
      
      <tag>kubeadm kubernetes, centos7 linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2019/05/20/hello-world/"/>
    <url>/2019/05/20/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
